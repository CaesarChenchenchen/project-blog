[
  {
    "objectID": "posts/my_first_post/index.html",
    "href": "posts/my_first_post/index.html",
    "title": "My First post",
    "section": "",
    "text": "This is an example first post. Whatever you write here will appear inside your post.\nIt is possible to write lists:\nAnd make headers"
  },
  {
    "objectID": "posts/my_first_post/index.html#this-is-a-second-level-heading",
    "href": "posts/my_first_post/index.html#this-is-a-second-level-heading",
    "title": "My First post",
    "section": "This is a second level heading",
    "text": "This is a second level heading\nYou can include images, and many other kinds of content.\nIf you are using the visual editor in Rstudio you can drag images onto the editor to insert them into the document. Otherwise, you need to place the image inside of the folder for this post, and then you can insert it to your post directly, like this:\n\nThe first image in a blog post will also be used on the listings page."
  },
  {
    "objectID": "posts/Example_assignment/index.html",
    "href": "posts/Example_assignment/index.html",
    "title": "Example assignment",
    "section": "",
    "text": "This assignment engages you in the process of introspection. Your task is to use introspection to evaluate and describe your own mental imagery abilities. Attempt to answer the following kinds of questions. What is your mental imagery like? Do you have mental imagery for different kinds of senses? Is your mental imagery vivid and life-like or very different from normal perception? How would you describe your mental imagery?\nYou should write a minimum of 250 words, but feel free to write more. Submit your document on blackboard by the due date."
  },
  {
    "objectID": "posts/Example_assignment/index.html#my-mental-imagery",
    "href": "posts/Example_assignment/index.html#my-mental-imagery",
    "title": "Example assignment",
    "section": "My mental imagery",
    "text": "My mental imagery\nMy mental imagery is like…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explore the World in Data!",
    "section": "",
    "text": "How to Find My Bar in New York?\n\n\n\n\n\n\n\nspatial\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nTianxiao Chen\n\n\n\n\n\n\n  \n\n\n\n\nCamden Gateway District Plan\n\n\n\n\n\n\n\nplanning\n\n\nbook\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nAlexander Nelms, Tianxiao Chen, Jayden Schultz, Kaye LI, chenglin Hu, Avery Weiss, Jackson LaSarso, Yuanhao Zhai\n\n\n\n\n\n\n  \n\n\n\n\nNJ Transit Delay Time Prediction\n\n\n\n\n\n\n\ntransit\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nTianxiao & Ling\n\n\n\n\n\n\n  \n\n\n\n\nGermantown Equity Strategic Planning\n\n\n\n\n\n\n\nplanning\n\n\nbook\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nTre Ambroise, Tianxiao Chen, Shuting Li, Sophie Maes, Brenna Schmidt, Mimi Tran, Evan Zhao\n\n\n\n\n\n\n  \n\n\n\n\nSan Francisco Crime Analysis in Physical and Social Environment\n\n\n\n\n\n\n\nspatial\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nTianxiao Chen\n\n\n\n\n\n\n  \n\n\n\n\nLandscape Architecture & Urban Design Work\n\n\n\n\n\n\n\nurban design\n\n\nlandscape architecture\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2022\n\n\nTianxiao Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "This is a template for using quarto to create a course blog. A course blog can be used in many ways to engage with course content. For example, you could use the blog to post assignments, or to dive more deeply into course material that interests you. By the end of the course, your blog can serve as a portfolio of ways that you engaged in the material. By learning how to use quarto for your blog, you will also be learning new skills for creating and sharing reproducible documents that could be useful to you in the future.\nThe purpose of this page is to provide tips and pointers about blogging with quarto. Quarto is simple enough for creating a basic course blog. However, it is also very deep and can be used to create all sorts of documents, from website, to slide decks, and books.\n\n\nMany questions about quarto can be answered from the quarto documentation located at: https://quarto.org.\nIf you are looking for something specific and don’t know where to find it on the website, use the search tool in the top right corner.\n\n\n\n\nTo use this template you will need a Github.com account, and access to R and Rstudio.\nYou can get access by creating a Github.com account, and downloading the necessary open-source software to your machine.\nIt is also possible to use Rstudio in your web-browser, which does not require downloading any software. There are two sets of instructions, one for the downloading approach, and the other for using Rstudio cloud. Scroll to the bottom for instructions on using Rstudio cloud.\n\n\n\nIn order to use this template you will need to install some free open-source software on your computer.\n\nSign up for a free account at https://github.com. This a website for sharing open-source software, but it can also be used to serve your blog as a website for free.\nDownload Github Desktop and install it on your machine. This should install the version control software git on your system, and you can use Github Desktop to easily push your blog from your local computer so that it can be viewed on Github.com.\nDownload R and install it on your machine. R is a programming language capable of many things, and it needs to be on your machine before you can run R Studio.\nDownload R Studio Desktop and install it on your machine. R Studio is called an “IDE” or integrated development environment, that you can use to write your blog with quarto.\n\n\n\n\nOnce you have the software installed, the next step is to create a quarto blog project in R studio. The collection of files in this template is a pre-made quarto blog project that you can modify for your own purposes. You can also make one yourself in Rstudio.\nAssuming you have downloaded this template, and you have installed the above software, then you need to open quartoCourseBlog.Rproj.\n\n\n\n\nTo find out if everything is working, try rendering the blog. Go to the “Build” Tab and press “Render Website”.\n\nAfter the rendering is complete, you should be able to view your blog. It might show up in the viewer pane like this:\n\nAnd if you press the ‘window-with-an-arrow’ button, you can view the website in your default browser. Quarto websites automatically adjust for the size of the window, so it may appear differently in the viewer pane versus the browser.\n\n\n\nAll of the blog posts are located in the posts folder.\n\nTo make a new post, copy an existing post and then modify it. For example, my posts folder currently contains one post, and it is inside the my_first_post folder.\n\nI can copy the folder and make a new one with the same contents from the Rstudio gear-box menu:\n\nI made a folder for a second post called Example_assignment.\n\nThese are the two files inside the folder. The .qmd file is a plain text file where you will write the blog post. This folder can also be used to store other assets you might put in the post, such as pictures.\n\n\n\nTo write a new post, open the .qmd file, edit the text, and then re-render the website. This is what the text in the .qmd file looked like when I copied it.\n\nThe text at the top between the “---” is called YAML, and provides meta-data for your document. This is where you can change the title, date, name, and add keywords if you want.\nThe rest of the document is for the main body of the post. For example, I changed the text to read:\n\n\n\n\n\nRender the website from the build tab again to see your new post.\n\n\n\nTo share your blog online you will have to publish it on a server that can be accessed by other people on the internet. There are multiple ways to do this step, and I recommend using Github pages. You can view more in-depth instructions from quarto here https://quarto.org/docs/publishing/github-pages.html.\nHere are the steps:\n\nOpen Github Desktop\nGo to preferences and sign in to your Github.com account\n“Add” your blog project folder to Github Desktop\nThere should be an option for a commit message, write a note in there like “first commit”.\nPublish to github.com and uncheck private repository so that other people will be able to see your repository.\nYou should now be able to see your new repository in your github.com profile, which means you should be able to see a copy of your blog files in the repository.\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder.\nAccess the blog from the url generated by the github pages settings page.\n\n\n\n\nWhenever you make changes to your blog project that you want to share online follow these steps:\n\nMake changes to your blog, like writing a new post, or editing an old one.\nRender the website in R-studio. What you see here should be what you will see later on Github.com\nOpen Github Desktop and Commit your changes, by writing brief commit title, and pressing commit.\nThen, use Github Desktop to Push your changes to github.com.\nWait half a minute or so, and you should see your new content appear on the website.\n\n\n\n\nI am planning to add a video overview of these steps soon. In the meantime, these instructions may be enough to get started with R studio cloud and github.com.\n\nSign up for a free account with posit cloud here https://posit.cloud/plans/free\nSign up for a free https://github.com account.\nLog in to Github, and search for this repository https://github.com/CrumpLab/quartoCourseBlog.\n\n\nClick the green “Use this template” Button\nThis will make a copy of the template in your github account, it will show up as one of your repositories\nGive your new repository a name\n\n\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder. You should now be able to view the blog from the url given by github pages.\nLog into Posit Cloud\nCreate a New Project, choose “New Project from Git Repository”\n\n\nenter the URL to the github repository you just made\n\n\nLoad the project, and edit/modify the files (see above for examples of creating new posts etc.)\nTo send your changed files back to github.com you need to do a few steps\n\n\nFrom the Git tab: stage your changes, commit your changes, and push your changes using the green up arrow.\nYou will also need to authenticate your git credentials, and allow Rstudio cloud to update your github repository\nIn the terminal run these two lines, but replace with your name and email\n\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\nWhen you “push” your changes back to Github using the green up arrow, you will be asked to enter a username and password. You can enter the username for your Github.com account, but your password won’t work. You need to set up a personal access token.\nGo to your github.com profile &gt; settings page. Scroll down, click on “&lt;&gt; Developer Settings”, on the left\nClick on personal access tokens, generate a new token, give it repo access. Save the text somewhere and use it as your github password when pushing from RStudio.\n\n\nAt this point you should be able to work on your blog in Rstudio Cloud, and push your changes to have them updated on github.com, which serves your blog online.\n\n\n\n\nSee this growing list of quarto resources for much, much more:\nhttps://github.com/mcanouil/awesome-quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Future urban analyst and planner\nHi! I am Tianxiao, from University of Pennsylvania Master of City Planning 24’. From my perspective, data-driven is the future of decision-making in the field of regional planning. Therefore, that’s my passionate of the application of data analyst and machine learning in the urban field."
  },
  {
    "objectID": "readme.html#quarto-documentation",
    "href": "readme.html#quarto-documentation",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Many questions about quarto can be answered from the quarto documentation located at: https://quarto.org.\nIf you are looking for something specific and don’t know where to find it on the website, use the search tool in the top right corner."
  },
  {
    "objectID": "readme.html#using-this-template",
    "href": "readme.html#using-this-template",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "To use this template you will need a Github.com account, and access to R and Rstudio.\nYou can get access by creating a Github.com account, and downloading the necessary open-source software to your machine.\nIt is also possible to use Rstudio in your web-browser, which does not require downloading any software. There are two sets of instructions, one for the downloading approach, and the other for using Rstudio cloud. Scroll to the bottom for instructions on using Rstudio cloud."
  },
  {
    "objectID": "readme.html#downloading-the-free-software",
    "href": "readme.html#downloading-the-free-software",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "In order to use this template you will need to install some free open-source software on your computer.\n\nSign up for a free account at https://github.com. This a website for sharing open-source software, but it can also be used to serve your blog as a website for free.\nDownload Github Desktop and install it on your machine. This should install the version control software git on your system, and you can use Github Desktop to easily push your blog from your local computer so that it can be viewed on Github.com.\nDownload R and install it on your machine. R is a programming language capable of many things, and it needs to be on your machine before you can run R Studio.\nDownload R Studio Desktop and install it on your machine. R Studio is called an “IDE” or integrated development environment, that you can use to write your blog with quarto."
  },
  {
    "objectID": "readme.html#make-a-quarto-blog-project",
    "href": "readme.html#make-a-quarto-blog-project",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Once you have the software installed, the next step is to create a quarto blog project in R studio. The collection of files in this template is a pre-made quarto blog project that you can modify for your own purposes. You can also make one yourself in Rstudio.\nAssuming you have downloaded this template, and you have installed the above software, then you need to open quartoCourseBlog.Rproj."
  },
  {
    "objectID": "readme.html#render-the-blog",
    "href": "readme.html#render-the-blog",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "To find out if everything is working, try rendering the blog. Go to the “Build” Tab and press “Render Website”.\n\nAfter the rendering is complete, you should be able to view your blog. It might show up in the viewer pane like this:\n\nAnd if you press the ‘window-with-an-arrow’ button, you can view the website in your default browser. Quarto websites automatically adjust for the size of the window, so it may appear differently in the viewer pane versus the browser."
  },
  {
    "objectID": "readme.html#make-a-new-blog-post",
    "href": "readme.html#make-a-new-blog-post",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "All of the blog posts are located in the posts folder.\n\nTo make a new post, copy an existing post and then modify it. For example, my posts folder currently contains one post, and it is inside the my_first_post folder.\n\nI can copy the folder and make a new one with the same contents from the Rstudio gear-box menu:\n\nI made a folder for a second post called Example_assignment.\n\nThese are the two files inside the folder. The .qmd file is a plain text file where you will write the blog post. This folder can also be used to store other assets you might put in the post, such as pictures.\n\n\n\nTo write a new post, open the .qmd file, edit the text, and then re-render the website. This is what the text in the .qmd file looked like when I copied it.\n\nThe text at the top between the “---” is called YAML, and provides meta-data for your document. This is where you can change the title, date, name, and add keywords if you want.\nThe rest of the document is for the main body of the post. For example, I changed the text to read:"
  },
  {
    "objectID": "readme.html#re-render-to-see-your-changes",
    "href": "readme.html#re-render-to-see-your-changes",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Render the website from the build tab again to see your new post."
  },
  {
    "objectID": "readme.html#share-your-blog-on-github.com",
    "href": "readme.html#share-your-blog-on-github.com",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "To share your blog online you will have to publish it on a server that can be accessed by other people on the internet. There are multiple ways to do this step, and I recommend using Github pages. You can view more in-depth instructions from quarto here https://quarto.org/docs/publishing/github-pages.html.\nHere are the steps:\n\nOpen Github Desktop\nGo to preferences and sign in to your Github.com account\n“Add” your blog project folder to Github Desktop\nThere should be an option for a commit message, write a note in there like “first commit”.\nPublish to github.com and uncheck private repository so that other people will be able to see your repository.\nYou should now be able to see your new repository in your github.com profile, which means you should be able to see a copy of your blog files in the repository.\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder.\nAccess the blog from the url generated by the github pages settings page."
  },
  {
    "objectID": "readme.html#pushing-new-posts-to-github.com",
    "href": "readme.html#pushing-new-posts-to-github.com",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Whenever you make changes to your blog project that you want to share online follow these steps:\n\nMake changes to your blog, like writing a new post, or editing an old one.\nRender the website in R-studio. What you see here should be what you will see later on Github.com\nOpen Github Desktop and Commit your changes, by writing brief commit title, and pressing commit.\nThen, use Github Desktop to Push your changes to github.com.\nWait half a minute or so, and you should see your new content appear on the website."
  },
  {
    "objectID": "readme.html#posit-cloud-formerly-rstudio-cloud",
    "href": "readme.html#posit-cloud-formerly-rstudio-cloud",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "I am planning to add a video overview of these steps soon. In the meantime, these instructions may be enough to get started with R studio cloud and github.com.\n\nSign up for a free account with posit cloud here https://posit.cloud/plans/free\nSign up for a free https://github.com account.\nLog in to Github, and search for this repository https://github.com/CrumpLab/quartoCourseBlog.\n\n\nClick the green “Use this template” Button\nThis will make a copy of the template in your github account, it will show up as one of your repositories\nGive your new repository a name\n\n\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder. You should now be able to view the blog from the url given by github pages.\nLog into Posit Cloud\nCreate a New Project, choose “New Project from Git Repository”\n\n\nenter the URL to the github repository you just made\n\n\nLoad the project, and edit/modify the files (see above for examples of creating new posts etc.)\nTo send your changed files back to github.com you need to do a few steps\n\n\nFrom the Git tab: stage your changes, commit your changes, and push your changes using the green up arrow.\nYou will also need to authenticate your git credentials, and allow Rstudio cloud to update your github repository\nIn the terminal run these two lines, but replace with your name and email\n\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\nWhen you “push” your changes back to Github using the green up arrow, you will be asked to enter a username and password. You can enter the username for your Github.com account, but your password won’t work. You need to set up a personal access token.\nGo to your github.com profile &gt; settings page. Scroll down, click on “&lt;&gt; Developer Settings”, on the left\nClick on personal access tokens, generate a new token, give it repo access. Save the text somewhere and use it as your github password when pushing from RStudio.\n\n\nAt this point you should be able to work on your blog in Rstudio Cloud, and push your changes to have them updated on github.com, which serves your blog online."
  },
  {
    "objectID": "readme.html#more-quarto",
    "href": "readme.html#more-quarto",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "See this growing list of quarto resources for much, much more:\nhttps://github.com/mcanouil/awesome-quarto"
  },
  {
    "objectID": "posts/Camden/index.html",
    "href": "posts/Camden/index.html",
    "title": "Camden Gateway District Plan",
    "section": "",
    "text": "If want to see the complete version of our planning, please check the link and see the full planning book.\n\nIntroduction\nCamden, NJ, a historic and industrial urban center located directly across the Delaware River from Philadelphia, is in many ways a polemic example of the story of the post-industrial city. Once a key player in the most industrious region of the country, Camden was home to shipbuilding, canning, and many other manufacturing uses due to its proximity to markets, ports, heavy rail and related infrastructure. Like many similar Northeast and Midwest cities with an industrial and manufacturing past, the period of de-industrialization and urban renewal had drastically disparate impacts among Camden’s communities of color. Many of the challenges Camden still faces are attributable to the discrimminatory socio-economic doctrines and unjust political power imbalances that existed during these eras of urban development.”\n\n\n\nImplementation\nHighways are not inherently bad, but in the context of our study area, they consume a large percentage of area and are a barrier to local mobility. The I-676 and Admiral Wilson Blvd provide a greater regional mobility to largely non-Camden residents but at the cost of Camden residents. Symbolically, the highways memorialize the large-scale displacement of the 60s and the past half century of City turmoil. Camden residents have a lack of economic and social opportunities that are typically afforded to residents of other cities like those that use Camden’s highways. At the same time, Camden has a bright future. The recent wave of community-led neighborhood plans underline that Camden residents want to define Camden’s legacy. At the same time, the recent economic and transportation investments highlight that Camden is a city worth investing in. The issue is that this planning and investing are site and neighborhood specific. This siloed decision making is not addressing all of the factors at play. Although our plan is technically only for the Gateway District, we believe that this district can be a unifying force for Camden’s neighborhood planning and site investments. So even though we are creating a vision for the Gateway, our vision can affect the City as a whole.\n\n\n\nDevelopment Strategies\nUsing a area-based, scenario-based development and zoning method, our principle is to center residents and residential areas at the heart of the neighborhood, to create a counterforce against the dominant uses of transportation and office in Camden nowadays. Based on the new proposed land use, shown on the map on the right, there will be 110 plus acres of open space along the Cooper river and throughout the neighborhoods, 96 plus acres of developable land, 3373 new residential units, all of which will generate 33 million property and tax revenues."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "NJ Transit Delay Time Prediction",
    "section": "",
    "text": "Serving as the second largest commuter rail network in the United State, the NJ Transit spans New Jersey and the state to New York City. However, their delays are getting worse with more office workers returning, on the other hand, there are no interactive apps for commuters that can predict the immediate delays that may happen by chance or fixedly occur on their daily commute routes during the day. As such, they cannot foresee them instantly and mitigate accordingly.\nIn our project, we further investigate into the delay performance on NJ transit’s commuter rail routes and come up with some interactive & instant predictive strategies targeted at commuters within NJ Transit Commuter Rail Routes.\nAs such, Delay detective is designed with commuters in mind. The functions include the notifications of the estimated arrival time before scheduled arrival time, on-time & average-delay performance, as well as passenger feedback.\n\n\nCode\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(tigris)\nlibrary(viridis)\nlibrary(riem)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(RSocrata)\nlibrary(spdep)\nlibrary(caret)\nlibrary(ckanr)\nlibrary(FNN)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(ggcorrplot) # plot correlation plot\nlibrary(corrr)      # another way to plot correlation plot\nlibrary(kableExtra)\nlibrary(jtools)     # for regression model plots\nlibrary(ggstance) # to support jtools plots\nlibrary(ggpubr)    # plotting R^2 value on ggplot point scatter\nlibrary(broom.mixed) # needed for effects plots\nlibrary(vtable)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(purrr)\nlibrary(geosphere)\nlibrary(googlesheets4)\nlibrary(corrplot)\n\nplotTheme &lt;- theme(\n  plot.title =element_text(size=12),\n  plot.subtitle = element_text(size=8),\n  plot.caption = element_text(size = 6),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title.y = element_text(size = 10),\n  # Set the entire chart region to blank\n  panel.background=element_blank(),\n  plot.background=element_blank(),\n  #panel.border=element_rect(colour=\"#F0F0F0\"),\n  # Format the grid\n  panel.grid.major=element_line(colour=\"#D0D0D0\",size=.2),\n  axis.ticks=element_blank())\n\nmapTheme &lt;- theme(plot.title =element_text(size=10),\n                  plot.subtitle = element_text(size=8),\n                  plot.caption = element_text(size = 6),\n                  axis.line=element_blank(),\n                  axis.text.x=element_blank(),\n                  axis.text.y=element_blank(),\n                  axis.ticks=element_blank(),\n                  axis.title.x=element_blank(),\n                  axis.title.y=element_blank(),\n                  panel.background=element_blank(),\n                  panel.border=element_blank(),\n                  panel.grid.major=element_line(colour = 'transparent'),\n                  panel.grid.minor=element_blank(),\n                  legend.direction = \"vertical\", \n                  legend.position = \"right\",\n                  plot.margin = margin(1, 1, 1, 1, 'cm'),\n                  legend.key.height = unit(1, \"cm\"), legend.key.width = unit(0.2, \"cm\"))\n\nsource(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r\")\n\npalette6 &lt;- c(\"#264653\",\"#2a9d8f\",'#8AB17D',\"#e9c46a\",'#f4a261',\"#e76f51\")\npalette5 &lt;- c(\"#264653\",\"#2a9d8f\",\"#e9c46a\",'#f4a261',\"#e76f51\")\npalette4 &lt;- c(\"#264653\",\"#2a9d8f\",\"#e9c46a\",\"#e76f51\")\npalette2 &lt;- c(\"#264653\",\"#2a9d8f\")\n\n\n\n\nCode\n#geometry data\nline &lt;- st_read('https://services6.arcgis.com/M0t0HPE53pFK525U/arcgis/rest/services/NJTRANSIT_RAIL_LINES_1/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson')%&gt;%\n  mutate(LINE_NAME = ifelse(LINE_NAME == 'Bergen County Line','Bergen Co. Line ',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Montclair-Boonton Line','Montclair-Boonton',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'North Jersey Coast Line','No Jersey Coast',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Northeast Corridor','Northeast Corrdr',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Pascack Valley Line','Pascack Valley',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Princeton Dinky','Princeton Shuttle',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Raritan Valley Line','Raritan Valley',LINE_NAME))%&gt;%\n  dplyr::select(LINE_NAME,geometry)\n\nstop &lt;- st_read(\"https://services6.arcgis.com/M0t0HPE53pFK525U/arcgis/rest/services/NJTransit_Rail_Stations/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\") %&gt;%\n  mutate(STATION_ID = ifelse(STATION_ID == 'Atlantic City', 'Atlantic City Rail Terminal', STATION_ID),\n         STATION_ID = ifelse((STATION_ID == 'Middletown')&(COUNTY == 'Orange, NY'), 'Middletown NY', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Princeton Jct.', 'Princeton Junction', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Secaucus Junction Upper Level', 'Secaucus Upper Lvl', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Anderson Street-Hackensack', 'Anderson Street', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Bay Street-Montclair', 'Bay Street', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Broadway', 'Broadway Fair Lawn', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Essex Street-Hackensack', 'Essex Street', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Glen Rock-Boro Hall', 'Glen Rock Boro Hall', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Glen Rock-Main', 'Glen Rock Main Line', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Hoboken Terminal', 'Hoboken', STATION_ID),\n         STATION_ID = ifelse((STATION_ID == 'Middletown')&(COUNTY == 'Monmouth'), 'Middletown NJ', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Montclair St Univ', 'Montclair State U', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Mountain View-Wayne', 'Mountain View', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Pennsauken Transit Center', 'Pennsauken', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Radburn', 'Radburn Fair Lawn', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Ramsey', 'Ramsey Main St', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Rte 17 Ramsey', 'Ramsey Route 17', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Secaucus Junction Lower Level', 'Secaucus Lower Lvl', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Teterboro-Williams Ave', 'Teterboro', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Watsessing', 'Watsessing Avenue', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Wayne Route 23 Transit Center', 'Wayne-Route 23', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Wood-Ridge', 'Wood Ridge', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == '30th Street Station', 'Philadelphia', STATION_ID),\n         line_intersct = str_count(RAIL_SERVICE, \",\") + 1)%&gt;%\n  dplyr::select(STATION_ID,LATITUDE,LONGITUDE,line_intersct)%&gt;%\n  st_drop_geometry()\n\nid &lt;- '1V_kl3QKOxrTlwA8UG7kdv_VpJdaojKMj'\ndelay_df &lt;- read.csv(sprintf(\"https://docs.google.com/uc?id=%s&export=download\", id))%&gt;%\n  filter(type == 'NJ Transit')%&gt;%\n  na.omit()\n\nmerged_dataset &lt;- merge(delay_df, stop, by.x = \"from\", by.y = \"STATION_ID\",all.x = TRUE)\nmerged_dataset &lt;- merge(merged_dataset, stop, by.x = \"to\", by.y = \"STATION_ID\",all.x = TRUE)\nmerged_dataset &lt;- merged_dataset%&gt;%\n  filter(to != 'Mount Arlington')%&gt;%\n  filter(from != 'Mount Arlington')%&gt;%\n  rename(from_lat = LATITUDE.x,\n         from_lon = LONGITUDE.x,\n         from_inter = line_intersct.x,\n         to_lat = LATITUDE.y,\n         to_lon = LONGITUDE.y,\n         to_inter = line_intersct.y\n         )%&gt;%\n  mutate(distance = distHaversine(cbind(from_lon, from_lat), cbind(to_lon, to_lat)),\n         interval60 = floor_date(ymd_hms(scheduled_time), unit = \"hour\"),\n         week = week(interval60),\n         dotw = wday(interval60, label=TRUE),\n         time_of_day = case_when(hour(interval60) &lt; 7 | hour(interval60) &gt; 19 ~ \"Overnight\",\n                                 hour(interval60) &gt;= 7 & hour(interval60) &lt; 10 ~ \"AM Rush\",\n                                 hour(interval60) &gt;= 10 & hour(interval60) &lt; 15 ~ \"Mid-Day\",\n                                 hour(interval60) &gt;= 15 & hour(interval60) &lt;= 19 ~ \"PM Rush\"),\n         weekend = ifelse(dotw %in% c(\"Sun\", \"Sat\"), \"Weekend\", \"Weekday\"))\n\n#wweather data\nweather.Panel &lt;- \n  riem_measures(station = \"EWR\", date_start = \"2019-10-01\", date_end = \"2019-11-02\") %&gt;%\n  dplyr::select(valid, tmpf, p01i, sknt)%&gt;%\n  replace(is.na(.), 0) %&gt;%\n    mutate(interval60 = ymd_h(substr(valid,1,13))) %&gt;%\n    mutate(week = week(interval60),\n           dotw = wday(interval60, label=TRUE)) %&gt;%\n    group_by(interval60) %&gt;%\n    summarize(Temperature = max(tmpf),\n              Precipitation = sum(p01i),\n              Wind_Speed = max(sknt)) %&gt;%\n    mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))\n\n# census data\nNJCensus &lt;- \n  get_acs(geography = \"county subdivision\", \n          variables = c(\"B01003_001\", \"B19013_001\", \n                        \"B02001_002\", \"B08013_001\",\n                        \"B08012_001\", \"B08301_001\", \n                        \"B08301_010\", \"B01002_001\"), \n          year = 2019, \n          state = \"NJ\", \n          geometry = TRUE, \n          output = \"wide\") %&gt;%\n  rename(Total_Pop =  B01003_001E,\n         Med_Inc = B19013_001E,\n         Med_Age = B01002_001E,\n         White_Pop = B02001_002E,\n         Travel_Time = B08013_001E,\n         Num_Commuters = B08012_001E,\n         Means_of_Transport = B08301_001E,\n         Total_Public_Trans = B08301_010E) %&gt;%\n  select(Total_Pop, Med_Inc, White_Pop, Travel_Time,\n         Means_of_Transport, Total_Public_Trans,\n         Med_Age,\n         GEOID, geometry) %&gt;%\n  mutate(Percent_White = White_Pop / Total_Pop,\n         Mean_Commute_Time = Travel_Time / Total_Public_Trans,\n         Percent_Taking_Public_Trans = Total_Public_Trans / Means_of_Transport)\n\nNJCensus_select &lt;- NJCensus%&gt;%\n  mutate(bigcity = ifelse(Total_Pop &gt;= 100000, 'big city','small city'))%&gt;%\n  select(geometry, Total_Pop,bigcity, GEOID)\n\nNJTracts &lt;- \n  NJCensus %&gt;%\n  as.data.frame() %&gt;%\n  distinct(GEOID, .keep_all = TRUE) %&gt;%\n  select(GEOID, geometry) %&gt;% \n  st_sf\n\ntrain_census &lt;- st_join(merged_dataset %&gt;% \n          filter(is.na(from_lon) == FALSE &\n                   is.na(from_lat) == FALSE &\n                   is.na(to_lat) == FALSE &\n                   is.na(to_lon) == FALSE) %&gt;%\n          st_as_sf(., coords = c(\"from_lon\", \"from_lat\"), crs = 4326),\n        NJTracts %&gt;%\n          st_transform(crs=4326),\n        join=st_intersects,\n              left = TRUE) %&gt;%\n  rename(From.Tract = GEOID) %&gt;%\n  mutate(from_lon = unlist(map(geometry, 1)),\n         from_lat = unlist(map(geometry, 2)))%&gt;%\n  as.data.frame() %&gt;%\n  select(-geometry)\n\ntrain_census &lt;- train_census %&gt;%\n  st_as_sf(., coords = c(\"to_lon\", \"to_lat\"), crs = 4326) %&gt;%\n  st_join(., NJTracts %&gt;%\n            st_transform(crs=4326),\n          join=st_intersects,\n          left = TRUE) %&gt;%\n  rename(To.Tract = GEOID)  %&gt;%\n  mutate(to_lon = unlist(map(geometry, 1)),\n         to_lat = unlist(map(geometry, 2)))%&gt;%\n  as.data.frame() %&gt;%\n  select(-geometry)\n\ntrain_dataset &lt;-train_census  %&gt;%\n  left_join(weather.Panel, by =\"interval60\")\n\nmerged_dataset &lt;- train_dataset  %&gt;%\n  left_join(NJCensus_select, by = c(\"From.Tract\" = \"GEOID\")) %&gt;%\n  left_join(NJCensus_select, by =c(\"To.Tract\"=\"GEOID\")) %&gt;%\n  select(-geometry.x,-geometry.y) %&gt;%\n  rename(From_Total_Pop = Total_Pop.x,\n         To_Total_Pop = Total_Pop.y,\n         From_city = bigcity.x,\n         To_city = bigcity.y)%&gt;%\n  mutate(From_Total_Pop = ifelse(from == \"Philadelphia\", 1579075, From_Total_Pop),\n         To_Total_Pop = ifelse(to == \"Philadelphia\", 1579075, To_Total_Pop),\n         From_Total_Pop = ifelse(from == \"Middletown NY\", 1631993, From_Total_Pop),\n         To_Total_Pop = ifelse(to == \"Middletown NY\", 1631993, To_Total_Pop),\n         From_city = ifelse(from == \"Philadelphia\", 'big city', From_city),\n         To_city = ifelse(to == \"Philadelphia\", 'big city', To_city),\n         From_city = ifelse(from == \"Middletown NY\", 'big city', From_city),\n         To_city = ifelse(to == \"Middletown NY\", 'big city', To_city))\n\nmedian_value_f &lt;- median(merged_dataset$From_Total_Pop, na.rm = TRUE)\nmerged_dataset$From_Total_Pop[is.na(merged_dataset$From_Total_Pop)] &lt;- median_value_f\nmedian_value_t &lt;- median(merged_dataset$To_Total_Pop, na.rm = TRUE)\nmerged_dataset$To_Total_Pop[is.na(merged_dataset$To_Total_Pop)] &lt;- median_value_t\n\nmerged_dataset &lt;- merged_dataset%&gt;%\n  mutate(From_city = ifelse(From_Total_Pop == 24784, 'big city', From_city),\n         To_city = ifelse(To_Total_Pop == 24784, 'big city', To_city))"
  },
  {
    "objectID": "posts/post-with-code/index.html#data-source",
    "href": "posts/post-with-code/index.html#data-source",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Data Source",
    "text": "Data Source\n*NJ Transit Delay Data — The dataset provides delay data for each month between NJ transit 2018-2019, and delay data for October 2019 was used in this project.\n*NJ Rail Station & Line Data — The dataset provides the geometry data of the line and station, and be used for further data visualization.\n*Weather Data — The dataset provides the weather data collected from the weather stations. And the dataset include the precipitation, wind speed and temperature data.\n*Census Data — The dataset is provided by Census Bureau and gives the social-ecnomic situation of city."
  },
  {
    "objectID": "posts/post-with-code/index.html#serial-autocorrelation---fixed-effects",
    "href": "posts/post-with-code/index.html#serial-autocorrelation---fixed-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Serial Autocorrelation - Fixed Effects",
    "text": "Serial Autocorrelation - Fixed Effects\nFrom the Temporal Series Analysis, We can find that Delay in NJ Transit has an obvious regularity in the temporal field. We were able to find that weekend delays had a longer average length than weekdays. And when we look at latency over a 24-hour period, we can see that latency reaches its maximum between 2:00 a.m. and 3:00 a.m., and overall latency stays on an upward trend from 4:00 a.m. onwards. And when we look at the relationship between stop sequence and delay time, we are able to see that the average latency time increases as the station sequence increases. We were able to find the highest latency in the PM Rush phase, followed by the overnight phase. When we wanted to explore the compounding of time, we were able to find that the PM Rush phase and the overnight phase had significantly higher latency times on weekends than on weekdays. And when we look at the pattern of average delay times for 24 hours in a day compared to weekdays and weekends, we find that they both maintain a similar pattern.\n\n\nCode\ndelay_time &lt;- merged_dataset %&gt;%\n  group_by(time_of_day)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_day &lt;- merged_dataset %&gt;%\n  group_by(dotw)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_week &lt;- merged_dataset %&gt;%\n  group_by(weekend)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_hour &lt;- merged_dataset %&gt;%\n  group_by(hour(interval60))%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(hour = 'hour(interval60)')\n\ndelay_sequence &lt;- merged_dataset %&gt;%\n  group_by(stop_sequence)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_time_week &lt;- merged_dataset %&gt;%\n  group_by(time_of_day,weekend)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_week_hour &lt;- merged_dataset %&gt;%\n  group_by(weekend,hour(interval60))%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(hour = 'hour(interval60)')\n\ngrid.arrange(ggplot(data = delay_day, aes(x = dotw, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in a week\", x = \"Day of The Week\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_week, aes(x = weekend, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = palette2) +\n  labs(title = \"Delay comparison in Weekend\", x = \"Weekend or Weekday\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_hour, aes(x = hour, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in 24 hours\", x = \"Hour in a day\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_sequence, aes(x = stop_sequence, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in each sequence\", x = \"Stop Sequence\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_time, aes(x = time_of_day, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in a week\", x = \"Day of The Week\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_week_hour, aes(x = hour, y = mean_delay, color = weekend)) +\n  geom_line() +\n  scale_color_manual(values = palette2) +\n  labs(title = \"The delay under week and time\", x = \"Hour\", y = \"Delay Minutes\") +\n  theme_minimal(),nrow=3)\n\n\n\n\n\nCode\nmerged_dataset%&gt;%\n  dplyr::select(interval60, from, delay_minutes) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value)) + \n    geom_line(size = 0.8,colour=\"#2a9d8f\")+\n      labs(title = \"Delay distribution in A Month\", subtitle = \"NJ, Oct, 2019\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal()"
  },
  {
    "objectID": "posts/post-with-code/index.html#spatial-autocorrelation---fixed-effects",
    "href": "posts/post-with-code/index.html#spatial-autocorrelation---fixed-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Spatial Autocorrelation - Fixed Effects",
    "text": "Spatial Autocorrelation - Fixed Effects\nFrom the charts and maps, we can observe that the delay time is highly associated with station and line. Specifically, the Atlantic city line has the most serious delay situation.Same with the stations along the Atlantic city Line. However, NJ Transit Commuter Routes as a whole have similar trends in delays. Also, in order to compare the difference in delay time, we compared the size of the city where the station is located with delay time based on census tracts under counties and defined cities with populations over 100,000 as big cities. It was found that delay time was an insignificant factor. In the meantime, from the map we can also conclude that the direction doesn’t seem to have a significant influence on delay in general.\n\n\nCode\ndelay_line &lt;- merged_dataset %&gt;%\n  group_by(line)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  arrange(., mean_delay)\n\ndelay_from &lt;- merged_dataset %&gt;%\n  group_by(from)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  arrange(., -mean_delay)%&gt;%\n  head(20)\n\nbig_from &lt;- merged_dataset %&gt;%\n  group_by(From_city)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'from')%&gt;%\n  rename(city_type = From_city)\n\nbig_to &lt;- merged_dataset %&gt;%\n  group_by(To_city)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'to')%&gt;%\n  rename(city_type = To_city)\n\ngrid.arrange(ggplot(data = delay_line, aes(x = line, y = mean_delay, fill = mean_delay)) +\n  geom_col(position = \"dodge\")+\n  labs(title = \"Delay minutes comparison in lines\", x = \"line\", y = \"Mean Delay\") + \n  scale_fill_gradient(low = \"#2a9d8f\", high = \"#264653\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 15, hjust = 1)) ,\n  ggplot(data = rbind(big_from,big_to), aes(x = status , y = mean_delay, fill = city_type)) +\n  geom_col(position = \"dodge\")+\n  labs(title = \"Delay minutes comparison in big and small city\", x = \"City Type\", y = \"Mean Delay\") + \n  scale_fill_manual(values = palette2)+\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 0 , hjust = 1)),\n  ggplot(data = delay_from, aes(x = from, y = mean_delay, fill = mean_delay)) +\n  geom_col(position = \"dodge\")+\n  labs(title = \"Delay minutes comparison in lines\", x = \"line\", y = \"Mean Delay\") + \n  scale_fill_gradient(low = \"#2a9d8f\", high = \"#264653\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)))\n\n\n\n\n\n\n\nCode\nmap_from &lt;- merged_dataset %&gt;%\n  group_by(from)%&gt;%\n  summarize(mean_delay = mean(delay_minutes)) %&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Orientation')%&gt;%\n  rename(station = from)\n\nmap_to &lt;- merged_dataset %&gt;%\n  group_by(to)%&gt;%\n  summarize(mean_delay = mean(delay_minutes)) %&gt;%\n  left_join(stop,by=c('to'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Destination')%&gt;%\n  rename(station = to)\n\nggplot() +\n  geom_sf(data = NJTracts, color = 'grey') + \n  geom_sf(data = rbind(map_from,map_to), aes(size = mean_delay,color = mean_delay), alpha = 0.5) +\n  scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\")+\n  scale_size_continuous(name = \"Delay Minutes\") +\n  coord_sf()+\n  labs(title=\"Delayed Time in Station, October, 2019\")+\n  facet_grid(~status)+\n  mapTheme()+\n  theme_minimal()"
  },
  {
    "objectID": "posts/post-with-code/index.html#operation-effects",
    "href": "posts/post-with-code/index.html#operation-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Operation Effects",
    "text": "Operation Effects\nIn addition to the influence of the temporal and spatial dimensions on the delay time, we also wanted to explore the influence of some external factors as well as the operational factors of the train system on the delay.Looking at the weather conditions in October, the temperatures showed a fluctuating downward trend, while at the precipitation level several large precipitation events were found in the second half of October.\n\n\nCode\ngrid.arrange(\n  ggplot(weather.Panel, aes(interval60,Precipitation)) + geom_line(color=\"#2a9d8f\") + \n  labs(title=\"Percipitation\", x=\"Hour\", y=\"Perecipitation\") + theme_minimal(),\n  ggplot(weather.Panel, aes(interval60,Wind_Speed)) + geom_line(color=\"#2a9d8f\") + \n    labs(title=\"Wind Speed\", x=\"Hour\", y=\"Wind Speed\") + theme_minimal(),\n  ggplot(weather.Panel, aes(interval60,Temperature)) + geom_line(color=\"#2a9d8f\") + \n    labs(title=\"Temperature\", x=\"Hour\", y=\"Temperature\") + theme_minimal(),\n  top=\"Weather Data - NJ EWR - Nov, 2019\")\n\n\n\n\n\nFor the number of intersections, overall the delay time decreases as more lines pass through the station, while the direction of the line has no significant effect on the intersection. When we focus on the effect of weather on the delay, we can find that rainy weather will have higher delay time, and as the weather rises the delay time will show a decreasing trend. This result may be explained by the fact that trains travel at lower speeds in rainy weather and that trains take more time to start up in low temperatures. The distance between stations does not have a significant effect on the delay time.\n\n\nCode\ndelay_distance &lt;- merged_dataset %&gt;%\n  group_by(distance)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_intersct &lt;- rbind(\n  merged_dataset %&gt;%\n  group_by(from_inter)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'from')%&gt;%\n  rename(inter = from_inter),\n  merged_dataset %&gt;%\n  group_by(to_inter)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'to')%&gt;%\n  rename(inter = to_inter))\n\ndelay_rain_week &lt;- merged_dataset %&gt;%\n  mutate(rain = ifelse(Precipitation == 0,'NoRain','Rain'))%&gt;%\n  group_by(rain)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_temp &lt;- merged_dataset %&gt;%\n  group_by(Temperature)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n# just calculate the intersecation delay\ngrid.arrange(\n  ggplot(data = delay_intersct, aes(x = inter, y = mean_delay,fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = palette2) +\n  labs(title = \"Delay minutes in each intersection\", x = \"Num of Intersection\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_rain_week, aes(x = rain, y = mean_delay,fill=rain)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = palette2) +\n  labs(title = \"Delay minutes with Rain\", x = \"Rain\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_distance, aes(x = distance, y = mean_delay)) +\n  geom_line(color = \"#2a9d8f\") +\n  labs(title = \"The relationship between delay and distance\", x = \"Distance\", y = \"Value\") +\n  geom_smooth(method = \"lm\", se = TRUE)+\n  theme_minimal(),\n  ggplot(data = delay_temp, aes(x = Temperature, y = mean_delay)) +\n  geom_line(color = \"#2a9d8f\") +\n  labs(title = \"The relationship between delay and temperature\", x = \"Temperature\", y = \"Value\") +\n  geom_smooth(method = \"lm\", se = TRUE)+\n  theme_minimal())"
  },
  {
    "objectID": "posts/post-with-code/index.html#space-time-autocorrelation",
    "href": "posts/post-with-code/index.html#space-time-autocorrelation",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Space-time Autocorrelation",
    "text": "Space-time Autocorrelation\nAfter the spatial and temporal analysis, We would like to explore more deeply the autocorrelation of space-time with delay time. Looking at the distribution of the average delay time in terms of time and site, we were able to find a delay effect of the average delay time on the site. Therefore, we can draw the inference that the front site on a route will have a lagging effect on the delay time of the back site. And overall, the commuting area around New York has much smaller and more consistent delays, relative to the Atlantic City line.\n\n\nCode\ndelay_stop_time &lt;- merged_dataset %&gt;%\n  group_by(from,to,hour(interval60))%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(hour = 'hour(interval60)')%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = delay_stop_time, aes(size = line_intersct,color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay For One Day in Oct, 2019\",\n         subtitle = \"Hours in a day: {current_frame}\") +\n    transition_manual(hour)+ mapTheme()+theme_minimal()\n\n\n\n\n\nAnd when we want to consider the effect of intersections on delay times, we are able to find that the number of intersections on weekdays does not have a large impact on the degree of delay, except for the New York station which has a smaller delay time. This may be due to the fact that New York station has more passenger throughput resulting in a tighter departure frequency. On weekends, we are able to find that stations that are intersections have lower average delay times.\n\n\nCode\ndelay_intersct_week_f &lt;-merged_dataset %&gt;%\n  group_by(from_inter,weekend)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ninter_stop &lt;- stop%&gt;%\n  right_join(delay_intersct_week_f,by=c('line_intersct' = 'from_inter'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = inter_stop, aes(size = line_intersct,color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay Intersection Comparison, 2019\") +\n    facet_wrap(~weekend)+ mapTheme()+theme_minimal()\n\n\n\n\n\nAnd in addition to exploring the phenomenon of temporal pattern in a day, we also hope to discover temporal patterns over long periods of time. In a weekly dimension, we find that weekday and weekend delays are relatively stable, and the direction of the line does not have a significant spatial effect on the delay. Similarly, when we go to look at the spatial distribution of the average delay time for each week in a month, the variation in delay time is small and very stable.\n\n\nCode\ndelay_to_time &lt;- merged_dataset %&gt;%\n  group_by(to,dotw)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(Day = dotw)%&gt;%\n  left_join(stop,by=c('to'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Orientation')%&gt;%\n  rename(station = to)\n\ndelay_from_time &lt;- merged_dataset %&gt;%\n  group_by(from,dotw)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(Day = dotw)%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Destination')%&gt;%\n  rename(station = from)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = rbind(delay_from_time,delay_to_time), aes(size =mean_delay, color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay For One Week in Oct, 2019\",\n         subtitle = \"Day in a week: {current_frame}\") +\n    facet_wrap(~status)+\n    transition_manual(Day)+ mapTheme()+theme_minimal()\n\n\n\n\n\n\n\nCode\ndelay_to_time &lt;- merged_dataset %&gt;%\n  group_by(to,week)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  left_join(stop,by=c('to'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Orientation')%&gt;%\n  rename(station = to)\n\ndelay_from_time &lt;- merged_dataset %&gt;%\n  group_by(from,week)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Destination')%&gt;%\n  rename(station = from)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = rbind(delay_from_time,delay_to_time), aes(size =mean_delay, color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay For One Month in Oct, 2019\",\n         subtitle = \"Week in a month: {current_frame}\") +\n    facet_wrap(~status)+\n    transition_manual(week)+ mapTheme()+theme_minimal()\n\n\n\n\n\nOverall, the spatial-temporal distribution of delay times is consistent with the findings of the temporal and spatial analyses conducted in the previous At the same time, we found a spatial manifestation of the lag effect of delay times at the site level. This provides us with a choice of new independent variables for the subsequent construction of the predictive model."
  },
  {
    "objectID": "posts/post-with-code/index.html#lag-effects",
    "href": "posts/post-with-code/index.html#lag-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Lag Effects",
    "text": "Lag Effects\nBased on the exploratory analysis described above, we found lag effects on delay times at the spatial and temporal levels, so we created temporal lag and spatial lag variables to make predictions in our real-world model. In a practical sense, the temporal lag can be interpreted as the effect of the amount of delay that occurs before a certain time at a passenger’s stop on the delay of the schedule he is traveling on. The spatial lag can be interpreted as the effect of the delay of the stop of the passenger’s trip before a certain time at his stop on the delay of the stop at which he is traveling.\nBecause for the USE CASE we want to realize the delay prediction of passengers in the period before boarding, we choose a relatively small time lag of 15 minutes as the unit, so that we can provide more time lag variables to be added in the model the closer the boarding time is to the boarding time.\n\nTime Lag\nIn terms of the correlation of the time-lagged variables with respect to the delay time, their correlation with the delay time decreases as the lag time increases. However, in general, the correlation of delay time in the same site is overall low.\n\n\nCode\nmerged_dataset &lt;- merged_dataset %&gt;%\n   mutate(interval15 = floor_date(ymd_hms(scheduled_time), unit = \"15 mins\"))\n\nmerged_dataset &lt;- \n  merged_dataset %&gt;% \n  arrange(from_id, interval15) %&gt;% \n  mutate(lag15min = dplyr::lag(delay_minutes,1),\n         lag30min = dplyr::lag(delay_minutes,2),\n         lag45min = dplyr::lag(delay_minutes,3),\n         lag1h = dplyr::lag(delay_minutes,4),\n         lag1h15min = dplyr::lag(delay_minutes,5),\n         lag1h30min = dplyr::lag(delay_minutes,6),\n         lag1h45min = dplyr::lag(delay_minutes,7),\n         lag2h = dplyr::lag(delay_minutes,8),\n         lag2h15min = dplyr::lag(delay_minutes,9),\n         lag2h30min = dplyr::lag(delay_minutes,10),\n         lag2h45min = dplyr::lag(delay_minutes,11),\n         lag3h = dplyr::lag(delay_minutes,12))\n\nas.data.frame(merged_dataset) %&gt;%\n    group_by(interval15) %&gt;% \n    summarise_at(vars(starts_with(\"lag\"), \"delay_minutes\"), mean, na.rm = TRUE) %&gt;%\n    gather(Variable, Value, -interval15, -delay_minutes) %&gt;%\n    mutate(Variable = factor(Variable, levels=c(\"lag15min\",\"lag30min\",\"lag45min\",\"lag1h\",                       \"lag1h15min\",\"lag1h30min\",\"lag1h45min\",\"lag2h\",\"lag2h15min\",\"lag2h30min\",\"lag2h45min\",\"lag3h\")))%&gt;%\n    group_by(Variable) %&gt;%  \n    summarize(correlation = round(cor(Value, delay_minutes),2))\n\n\n# A tibble: 12 × 2\n   Variable   correlation\n   &lt;fct&gt;            &lt;dbl&gt;\n 1 lag15min          0.59\n 2 lag30min          0.44\n 3 lag45min          0.4 \n 4 lag1h             0.4 \n 5 lag1h15min        0.35\n 6 lag1h30min        0.33\n 7 lag1h45min        0.23\n 8 lag2h             0.15\n 9 lag2h15min        0.18\n10 lag2h30min        0.22\n11 lag2h45min        0.23\n12 lag3h             0.23\n\n\n\n\nCode\nmerged_dataset_station &lt;- \n  merged_dataset %&gt;% \n  arrange(train_id, interval15,stop_sequence) %&gt;% \n  mutate(lagsstation = if_else(stop_sequence == 1, 0, lag(delay_minutes, 1)),\n         lags2station = if_else(stop_sequence == 1 | stop_sequence == 2, 0, lag(delay_minutes, 2)),\n         lags3station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3, 0, lag(delay_minutes, 3)),\n         lags4station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4, 0, lag(delay_minutes, 4)),\n         lags5station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5, 0, lag(delay_minutes, 5)),\n         lags6station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5| stop_sequence == 6, 0, lag(delay_minutes, 6)),\n         lags7station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5 | stop_sequence ==6 | stop_sequence == 7, 0, lag(delay_minutes, 7)),\n         lags8station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5| stop_sequence == 6| stop_sequence == 7| stop_sequence == 8 , 0, lag(delay_minutes, 8))\n         )\n\nselected_columns &lt;- merged_dataset_station[, c(\"delay_minutes\", \"lag15min\",\"lag30min\",\"lag45min\",\"lag1h\",                       \"lag1h15min\",\"lag1h30min\",\"lag1h45min\",\"lag2h\",\"lag2h15min\",\"lag2h30min\",\"lag2h45min\",\"lag3h\",\"week\")]\ncor_delay_all_time &lt;- cor(selected_columns, use = \"complete.obs\")[\"delay_minutes\", -1]%&gt;%\n  as.data.frame()%&gt;%rename(cor_score = '.')\n\nplotData.lag_time &lt;-\n  filter(as.data.frame(merged_dataset_station), week == 43) %&gt;%\n  dplyr::select(lag15min,lag30min,lag45min,lag1h,                       lag1h15min,lag1h30min,lag1h45min,lag2h,lag2h15min,lag2h30min,lag2h45min,lag3h, delay_minutes) %&gt;%\n  gather(Variable, Value, -delay_minutes) %&gt;%\n  mutate(Variable = fct_relevel(Variable,\"lag15min\",\"lag30min\",\"lag45min\",\"lag1h\",                       \"lag1h15min\",\"lag1h30min\",\"lag1h45min\",\"lag2h\",\"lag2h15min\",\"lag2h30min\",\"lag2h45min\",\"lag3h\"))\n\ncorrelation.lag_time &lt;-\n  group_by(plotData.lag_time, Variable) %&gt;%\n    summarize(correlation = round(cor(Value, delay_minutes, use = \"complete.obs\"), 2))\n\nggplot(plotData.lag_time, aes(Value,delay_minutes))+\n  geom_point(size = 0.1) +\n  geom_text(data = correlation.lag_time, aes(label = paste(\"r =\", round(correlation, 2))),\n            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +\n  geom_smooth(method = 'lm', se=FALSE, color =\"#2a9d8f\")+\n  facet_wrap(~Variable, ncol = 4, scales = 'free') +\n  labs(title = \"Delay minute from previous time as a function of spatial lags\",\n       subtitle = \"One week in Oct, 2019\") +\n  mapTheme()+theme_minimal()\n\n\n\n\n\n\n\nStation Lag\nAs for the correlation of spatial lags, we were able to find that the overall correlation of the lagged variables of the sites for the delay time decreases as the number of lagged sites increases. However, overall, the correlation of spatially lagged variables to delay time is high.\n\n\n\n\n\nOverall, at the level of lagged impacts, spatial lagged impacts possess a high correlation for delay times, while temporal lagged impacts do not have a high correlation for delay times."
  },
  {
    "objectID": "posts/post-with-code/index.html#correlation-matrix",
    "href": "posts/post-with-code/index.html#correlation-matrix",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\nIn summary, we found the influence of factors on delay time at the spatial, temporal, operational, and lag effect levels, and we constructed a correlation matrix to visualize the correlation between the factors and the delay time for better screening of effective independent variables for subsequent data modeling. From the matrix, we can find that the spatial lag and time lag variables have more obvious effects on delay time.\n\n\nCode\nmerged_dataset_station &lt;- merged_dataset_station%&gt;%\n  mutate(isbig_from = ifelse(From_city == 'big city',1,0),\n         isbig_to = ifelse(To_city == 'big city',1,0),\n         isweekday = ifelse(weekend == 'Weekday',1,0))\n\ncor_matrix &lt;- merged_dataset_station %&gt;%\n  dplyr::select(delay_minutes,stop_sequence,distance,Temperature,Precipitation,Wind_Speed,from_inter,to_inter,isweekday,lag15min,lag30min,lag45min,lag1h,                       lag1h15min,lag1h30min,lag1h45min,lag2h,lag2h15min,lag2h30min,lag2h45min,lag3h,lagsstation,lags2station,lags3station, lags4station, lags5station, lags6station, lags7station, lags8station)\n\ncor_matrix &lt;- cor(cor_matrix, method = \"pearson\", use = \"complete.obs\")\n\ncorrplot(cor_matrix, method = 'shade', order = 'AOE', diag = FALSE,tl.col = 'black')"
  },
  {
    "objectID": "posts/post-with-code/index.html#model-evaluation",
    "href": "posts/post-with-code/index.html#model-evaluation",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nFrom the comparison of predicted and actual delay times, we can see that the closer the model is to the predicted time has a better model predictive ability. Whereas the model has worse predictive ability for very large and very small values.\n\n\nCode\ndelay.Test_5 &lt;- delay.Test%&gt;%\n  mutate(pre_5 = predict(reg.30, newdata = delay.Test),\n         abosulte_error = abs(pre_5 - delay_minutes),\n         MAE = mean(abosulte_error,na.rm = TRUE),\n         sd_AE = sd(abosulte_error,na.rm = TRUE),\n         per_error = (pre_5 - delay_minutes)/delay_minutes,\n         per_error = ifelse(per_error == Inf,0,per_error),\n         per_error = ifelse(per_error == -Inf,0,per_error))%&gt;%\n  rename(mod_30 = pre_5)\n\ndelay.Test_6 &lt;- delay.Test%&gt;%\n  mutate(pre_6 = predict(reg.60, newdata = delay.Test),\n         abosulte_error = abs(pre_6 - delay_minutes),\n         MAE = mean(abosulte_error,na.rm = TRUE),\n         sd_AE = sd(abosulte_error,na.rm = TRUE),\n         per_error = (pre_6 - delay_minutes)/delay_minutes,\n         per_error = ifelse(per_error == Inf,0,per_error),\n         per_error = ifelse(per_error == -Inf,0,per_error))%&gt;%\n  rename(mod_60 = pre_6)\n\ndelay.Test_7 &lt;- delay.Test%&gt;%\n  mutate(pre_7 = predict(reg.90, newdata = delay.Test),\n         abosulte_error = abs(pre_7 - delay_minutes),\n         MAE = mean(abosulte_error,na.rm = TRUE),\n         sd_AE = sd(abosulte_error,na.rm = TRUE),\n         per_error = (pre_7 - delay_minutes)/delay_minutes,\n         per_error = ifelse(per_error == Inf,0,per_error),\n         per_error = ifelse(per_error == -Inf,0,per_error))%&gt;%\n  rename(mod_90 = pre_7)\n\ngrid.arrange(\n  delay.Test_5%&gt;%\n  dplyr::select(interval60, from, delay_minutes, mod_30) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n    geom_line(size = 0.9)+\n      labs(title = \"Predicted/Observed delay time series\", subtitle = \"30 Minustes-Pre Predict\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal(),\n  delay.Test_6%&gt;%\n  dplyr::select(interval60, from, delay_minutes, mod_60) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n    geom_line(size = 0.9)+\n      labs(title = \"Predicted/Observed delay time series\", subtitle = \"60 Minustes-Pre Predict\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal(),\n  delay.Test_7%&gt;%\n  dplyr::select(interval60, from, delay_minutes, mod_90) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n    geom_line(size = 0.9)+\n      labs(title = \"Predicted/Observed delay time series\", subtitle = \"90 Minustes-Pre Predict\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal(),\n  ncol=1)\n\n\n\n\n\nAnd when we focus on the spatial generalizability of the model’s performance, we are um able to find that all three models show a more even MAE, except for the line from New York to the north. Other than that, we are able to find that the models have larger model errors for stations in and around New York. This phenomenon may stem from the fact that in New York there is a higher frequency of trips and the same station may be affected by the delay time of trips on different lines.\n\n\nCode\ntemp &lt;- delay.Test_5 %&gt;% \n  group_by(from)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(mod = '30m-Predict')\n\ntemp2 &lt;- delay.Test_6 %&gt;% \n  group_by(from)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(mod = '60m-Predict')\n\ntemp3 &lt;- delay.Test_7 %&gt;% \n  group_by(from)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(mod = '90m-Predict')\n\ntemp4 &lt;- rbind(temp,temp2)\ntemp4 &lt;- rbind(temp4,temp3)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = temp4, aes(color = mean_ae,size = line_intersct)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"MAE Spatial Comparison in 3 Models\") +\n    facet_wrap(~mod)+\n    mapTheme()+\n  theme_minimal()\n\n\n\n\n\nAnd in addition to model error assessment in space, we would also like to see if the model performs more universally across time. Overall, the models perform better on weekdays, both in terms of the magnitude of the errors and the stability of the errors. Moreover, both the error of the model and the s d of the error increase with the length of the model prediction. In terms of the comparison between weekdays and weekends, the three models have a smaller error boost on weekdays with the increase of time. In terms of the distribution of errors, the models on weekdays show a generally smaller standard divination. Therefore, we can conclude that the models on weekdays can have a more stable error range.\n\n\nCode\ntemp2 &lt;- delay.Test_5 %&gt;% \n  group_by(weekend)%&gt;%\n  summarise(MAE = mean(abosulte_error,na.rm = TRUE),\n            sd_AE = sd(abosulte_error,na.rm = TRUE))%&gt;%\n  mutate(mod = '30m')\n\ntemp &lt;- delay.Test_6 %&gt;% \n  group_by(weekend)%&gt;%\n  summarise(MAE = mean(abosulte_error,na.rm = TRUE),\n            sd_AE = sd(abosulte_error,na.rm = TRUE))%&gt;%\n  mutate(mod = '60m')\n\ntemp3 &lt;- delay.Test_7 %&gt;% \n  group_by(weekend)%&gt;%\n  summarise(MAE = mean(abosulte_error,na.rm = TRUE),\n            sd_AE = sd(abosulte_error,na.rm = TRUE))%&gt;%\n  mutate(mod = '90m')\n\ntemp4 &lt;- rbind(temp2,temp)\ntemp4 &lt;- rbind(temp4,temp3)\n\ngrid.arrange(\n  ggplot(temp4, aes(x=mod, y=MAE, colour=mod)) +\n    geom_point() +\n    facet_wrap(~weekend)+\n      labs(title = \"MAE Temporal Comparison\",x = \"Model\", y= \"MAE\") +\n     theme_minimal(),\n  ggplot(temp4, aes(x=mod, y=sd_AE, colour=mod)) +\n    geom_point() +\n    facet_wrap(~weekend)+\n      labs(title = \"SD of MAE Temporal Comparison\", x = \"Model\", y= \"SD_MAE\") +\n     theme_minimal()\n)\n\n\n\n\n\nWhen we look at the spatial distribution of this modeled performance difference between weekdays and weekends, we are able to find that the line north from New York possesses a smaller weekend and weekday error difference. The spatial difference between weekdays and weekends is primarily seen for lines heading south from New York, with stations on these lines having higher model errors on weekends.The reason this error exists may stem from the fact that on weekends more passengers from cities south of New York (e.g., Philadelphia and Jersey City) commute between New York and their locations for recreational or other purposes, and thus the same frequency in the face of significantly higher traffic may create a greater likelihood of delays.\n\n\nCode\ntemp &lt;- delay.Test_6 %&gt;% \n  group_by(from,weekend)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = temp, aes(color = mean_ae)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"MAE Comaprison in Weekend\",subtitle = '60mins Model') +\n    facet_wrap(~weekend) + mapTheme()+\n  theme_minimal()\n\n\n\n\n\nn turn, the model’s performance for the line shows more similar characteristics at the spatial level. All of the routes exhibit larger errors as they are pushed farther back in time. Specifically, the lines from New York northward have smaller errors and are less affected by the different time models than the other lines. The Northeast Corridor, on the other hand, has the worst performance of the models that are closest in time. However, as we push farther back in time, the modeled errors show a similar pattern for the other lines.\n\n\nCode\ntemp &lt;- delay.Test_6 %&gt;% \n  group_by(line)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(line,by=c('line'='LINE_NAME'))%&gt;%\n  mutate(mod = '60min')\n\ntemp2 &lt;- delay.Test_5 %&gt;% \n  group_by(line)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(line,by=c('line'='LINE_NAME'))%&gt;%\n  mutate(mod = '30min')\n\ntemp3 &lt;- delay.Test_7 %&gt;% \n  group_by(line)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(line,by=c('line'='LINE_NAME'))%&gt;%\n  mutate(mod = '90min')\n\ntemp4 &lt;- rbind(temp,temp2)\ntemp4 &lt;- rbind(temp4,temp3)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = temp4,aes(color = mean_ae, geometry = geometry))+\n    facet_wrap(~mod)+\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"A\") +\n    labs(title = \"Model MAE Comparison in Line\",subtitle = '30 mins & 60 mins model') + mapTheme()+\n  theme_minimal()\n\n\n\n\n\nIn the case of over- or under-forecasting, we find that all three models exhibit under-forecasting, and that the degree of under-forecasting increases as we move farther back in the model’s time. Therefore, in the actual model prediction, we can increase the model noise according to the model to increase the accuracy of the model prediction.\n\n\nCode\ntemp &lt;- delay.Test_5%&gt;%\n  dplyr::select(delay_minutes,mod_30,weekend)%&gt;%\n  mutate(mod = '30min')%&gt;%\n  rename(pre = mod_30)\ntemp2 &lt;- delay.Test_6%&gt;%\n  dplyr::select(delay_minutes,mod_60,weekend)%&gt;%\n  mutate(mod = '60min')%&gt;%\n  rename(pre = mod_60)\ntemp3 &lt;- delay.Test_7%&gt;%\n  dplyr::select(delay_minutes,mod_90,weekend)%&gt;%\n  mutate(mod = '90min')%&gt;%\n  rename(pre = mod_90)\n\ntemp4 &lt;- rbind(temp,temp2)\ntemp4 &lt;- rbind(temp4,temp3)\n\nggplot()+\n  geom_point(data = temp4,aes(x= delay_minutes, y = pre),color = \"#2a9d8f\")+\n    geom_smooth(data = temp4,aes(x= delay_minutes, y= pre), method = \"lm\", se = FALSE, color = '#f4a261')+\n    geom_abline(slope = 1, intercept = 0)+\n  facet_grid(mod~weekend)+\n  labs(title=\"Observed vs Predicted\",\n       subtitle = 'model and weekend camparison',\n       x=\"Observed delay minutes\", \n       y=\"Predicted delay minutes\")+\n  plotTheme()+\n  theme_minimal()"
  },
  {
    "objectID": "posts/post-with-code/index.html#model-generalizability-evaluation",
    "href": "posts/post-with-code/index.html#model-generalizability-evaluation",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Model Generalizability Evaluation",
    "text": "Model Generalizability Evaluation\nIn order to better test the model’s ability to perform in real-world scenarios, i.e., on new datasets, we evaluate the model’s ability using cross-validation.\n\n\nCode\nfitControl &lt;- trainControl(method = \"cv\", number = 20)\nset.seed(825)\n\nreg.cv.30 &lt;- \n  train(delay_minutes ~ from + to + hour + weekend + Temperature + Precipitation + Wind_Speed + lag45min  + lag1h + lag1h15min + lag1h30min + lag1h45min + lag2h + lag2h15min + lag2h30min + lag2h45min + lag3h + lags3station+ lags4station+ lags5station+lags6station+ stop_sequence + line + to_inter + from_inter, merged_dataset_model, \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\nreg.cv.60 &lt;- \n  train(delay_minutes ~ from + to + hour + weekend + Temperature + Precipitation + Wind_Speed+ lag1h + lag1h15min + lag1h30min + lag1h45min + lag2h + lag2h15min + lag2h30min + lag2h45min + lag3h + lags5station + lags6station + stop_sequence + line + to_inter + from_inter, merged_dataset_model, \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\nreg.cv.90 &lt;- \n  train(delay_minutes ~ from + to + hour + weekend + Temperature + Precipitation + Wind_Speed + lag1h30min + lag1h45min + lag2h + lag2h15min + lag2h30min + lag2h45min + lag3h+ lags6station+ stop_sequence + line + to_inter + from_inter, merged_dataset_model, \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\n\nLooking at the performance of the three models, we can see that the errors of the models show a gradual increase as time is pushed farther away. Regarding the stability of the model error, we can find that the 30-minute model has better stability. And as the time of the model is pushed farther, we can find that the R-square of the model shows a decreasing trend, which indicates that the credibility of the model is also decreasing with the increase of time.\n\n\nCode\ngrid.arrange(\ndplyr::select(reg.cv.30$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(reg.cv.30$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 5)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics-30mins Model\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")+\n  theme_minimal(),\ndplyr::select(reg.cv.60$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(reg.cv.60$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 5)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics-60mins Model\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")+\n  theme_minimal(),\ndplyr::select(reg.cv.90$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(reg.cv.90$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 5)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics-90mins Model\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")+\n  theme_minimal(),nrow=3)\n\n\n\n\n\n\n\nCode\ncombined_summary &lt;- bind_rows(\n  reg.cv.30$resample %&gt;%\n    summarise(Model = \"30 min Model\",\n              MAE = mean(.[,3]),\n              sd = sd(.[,3])),\n  reg.cv.60$resample %&gt;%\n    summarise(Model = \"60 min Model\",\n              MAE = mean(.[,3]),\n              sd = sd(.[,3])),\n  reg.cv.90$resample %&gt;%\n    summarise(Model = \"90 min Model\",\n              MAE = mean(.[,3]),\n              sd = sd(.[,3]))\n)\n\ncombined_summary %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Model = factor(Model, levels = c(\"30 min Model\", \"60 min Model\", \"90 min Model\"))) %&gt;%\n  kbl(col.names = c('Model', 'Mean Absolute Error', 'Standard Deviation of MAE')) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nModel\nMean Absolute Error\nStandard Deviation of MAE\n\n\n\n\n30 min Model\n1.804856\n0.0230556\n\n\n60 min Model\n2.242522\n0.0292971\n\n\n90 min Model\n2.418186\n0.0257110"
  },
  {
    "objectID": "posts/welcome/505Final.html",
    "href": "posts/welcome/505Final.html",
    "title": "San Francisco Crime Analysis in Physical and Social Environment",
    "section": "",
    "text": "Code\n# Data manipulation libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Timer\nfrom tqdm import tqdm, tqdm_notebook\n\n# Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor \nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Model support functions\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform\nfrom sklearn.preprocessing import StandardScaler \nfrom pprint import pprint\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Geo-related libraries\nimport geopandas as gpd\nimport folium\nfrom folium.plugins import HeatMap\nimport geopy\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport contextily as ctx\nimport geofeather\nfrom geopandas import GeoDataFrame\nfrom shapely.geometry import Point\nfrom shapely import wkt\nfrom shapely.geometry import Point, MultiPoint\nfrom shapely.ops import nearest_points\nfrom shapely import wkt\n\nimport osmnx\n\n\n\n\nCode\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n\n\n\nCode\n#https://drive.google.com/file/d/1HJBU_-Yg_ylAzab8v11l1wcYEwBhO41-/view?usp=share_link\nid = \"1HJBU_-Yg_ylAzab8v11l1wcYEwBhO41-\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('Police_Department_Incident_Reports__2018_to_Present.csv')\n\n\n\n\n\n\n\nCode\n#load crime data\ncrime_df = pd.read_csv('Police_Department_Incident_Reports__2018_to_Present.csv')\ncrime_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Datetime\nIncident Date\nIncident Time\nIncident Year\nIncident Day of Week\nReport Datetime\nRow ID\nIncident ID\nIncident Number\nCAD Number\n...\nLongitude\nPoint\nNeighborhoods\nESNCAG - Boundary File\nCentral Market/Tenderloin Boundary Polygon - Updated\nCivic Center Harm Reduction Project Boundary\nHSOC Zones as of 2018-06-05\nInvest In Neighborhoods (IIN) Areas\nCurrent Supervisor Districts\nCurrent Police Districts\n\n\n\n\n0\n2023/03/13 11:41:00 PM\n2023/03/13\n23:41\n2023\nMonday\n2023/03/13 11:41:00 PM\n125373607041\n1253736\n230167874\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2023/03/01 05:02:00 AM\n2023/03/01\n05:02\n2023\nWednesday\n2023/03/11 03:40:00 PM\n125379506374\n1253795\n236046151\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2023/03/13 01:16:00 PM\n2023/03/13\n13:16\n2023\nMonday\n2023/03/13 01:17:00 PM\n125357107041\n1253571\n220343896\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2023/03/13 10:59:00 AM\n2023/03/13\n10:59\n2023\nMonday\n2023/03/13 11:00:00 AM\n125355107041\n1253551\n230174885\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2023/03/14 06:44:00 PM\n2023/03/14\n18:44\n2023\nTuesday\n2023/03/14 06:45:00 PM\n125402407041\n1254024\n230176728\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n5 rows × 35 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ncrime_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 716680 entries, 0 to 716679\nData columns (total 35 columns):\n #   Column                                                Non-Null Count   Dtype  \n---  ------                                                --------------   -----  \n 0   Incident Datetime                                     716680 non-null  object \n 1   Incident Date                                         716680 non-null  object \n 2   Incident Time                                         716680 non-null  object \n 3   Incident Year                                         716680 non-null  int64  \n 4   Incident Day of Week                                  716680 non-null  object \n 5   Report Datetime                                       716680 non-null  object \n 6   Row ID                                                716680 non-null  int64  \n 7   Incident ID                                           716680 non-null  int64  \n 8   Incident Number                                       716680 non-null  int64  \n 9   CAD Number                                            555602 non-null  float64\n 10  Report Type Code                                      716680 non-null  object \n 11  Report Type Description                               716680 non-null  object \n 12  Filed Online                                          144690 non-null  object \n 13  Incident Code                                         716680 non-null  int64  \n 14  Incident Category                                     716064 non-null  object \n 15  Incident Subcategory                                  716064 non-null  object \n 16  Incident Description                                  716680 non-null  object \n 17  Resolution                                            716680 non-null  object \n 18  Intersection                                          678513 non-null  object \n 19  CNN                                                   678513 non-null  float64\n 20  Police District                                       716680 non-null  object \n 21  Analysis Neighborhood                                 678381 non-null  object \n 22  Supervisor District                                   678138 non-null  float64\n 23  Supervisor District 2012                              678393 non-null  float64\n 24  Latitude                                              678513 non-null  float64\n 25  Longitude                                             678513 non-null  float64\n 26  Point                                                 678513 non-null  object \n 27  Neighborhoods                                         664168 non-null  float64\n 28  ESNCAG - Boundary File                                7761 non-null    float64\n 29  Central Market/Tenderloin Boundary Polygon - Updated  92127 non-null   float64\n 30  Civic Center Harm Reduction Project Boundary          91531 non-null   float64\n 31  HSOC Zones as of 2018-06-05                           149426 non-null  float64\n 32  Invest In Neighborhoods (IIN) Areas                   0 non-null       float64\n 33  Current Supervisor Districts                          678393 non-null  float64\n 34  Current Police Districts                              677736 non-null  float64\ndtypes: float64(14), int64(5), object(16)\nmemory usage: 191.4+ MB\n\n\n\n\nCode\n#convert the pandas file to geopandas file\ncrime_df_geo = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df.Longitude, crime_df.Latitude), crs={'init' :'epsg:4326'})\ncrime_df_geo.info()\n\n\n/usr/local/lib/python3.9/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 716680 entries, 0 to 716679\nData columns (total 36 columns):\n #   Column                                                Non-Null Count   Dtype   \n---  ------                                                --------------   -----   \n 0   Incident Datetime                                     716680 non-null  object  \n 1   Incident Date                                         716680 non-null  object  \n 2   Incident Time                                         716680 non-null  object  \n 3   Incident Year                                         716680 non-null  int64   \n 4   Incident Day of Week                                  716680 non-null  object  \n 5   Report Datetime                                       716680 non-null  object  \n 6   Row ID                                                716680 non-null  int64   \n 7   Incident ID                                           716680 non-null  int64   \n 8   Incident Number                                       716680 non-null  int64   \n 9   CAD Number                                            555602 non-null  float64 \n 10  Report Type Code                                      716680 non-null  object  \n 11  Report Type Description                               716680 non-null  object  \n 12  Filed Online                                          144690 non-null  object  \n 13  Incident Code                                         716680 non-null  int64   \n 14  Incident Category                                     716064 non-null  object  \n 15  Incident Subcategory                                  716064 non-null  object  \n 16  Incident Description                                  716680 non-null  object  \n 17  Resolution                                            716680 non-null  object  \n 18  Intersection                                          678513 non-null  object  \n 19  CNN                                                   678513 non-null  float64 \n 20  Police District                                       716680 non-null  object  \n 21  Analysis Neighborhood                                 678381 non-null  object  \n 22  Supervisor District                                   678138 non-null  float64 \n 23  Supervisor District 2012                              678393 non-null  float64 \n 24  Latitude                                              678513 non-null  float64 \n 25  Longitude                                             678513 non-null  float64 \n 26  Point                                                 678513 non-null  object  \n 27  Neighborhoods                                         664168 non-null  float64 \n 28  ESNCAG - Boundary File                                7761 non-null    float64 \n 29  Central Market/Tenderloin Boundary Polygon - Updated  92127 non-null   float64 \n 30  Civic Center Harm Reduction Project Boundary          91531 non-null   float64 \n 31  HSOC Zones as of 2018-06-05                           149426 non-null  float64 \n 32  Invest In Neighborhoods (IIN) Areas                   0 non-null       float64 \n 33  Current Supervisor Districts                          678393 non-null  float64 \n 34  Current Police Districts                              677736 non-null  float64 \n 35  geometry                                              716680 non-null  geometry\ndtypes: float64(14), geometry(1), int64(5), object(16)\nmemory usage: 196.8+ MB\n\n\n\n\nCode\n#intial visualization\nfig, ax = plt.subplots(figsize=(12, 10))\ncrime_df_geo.to_crs(epsg=3857).plot(ax = ax,\n                figsize=(12,12),\n                markersize=40,\n               color=\"black\",\n               edgecolor=\"white\",\n               alpha=0.8,\n               marker=\"o\"\n            );\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)"
  },
  {
    "objectID": "posts/welcome/505Final.html#clean-the-crime-data",
    "href": "posts/welcome/505Final.html#clean-the-crime-data",
    "title": "San Francisco Crime Analysis in Physical and Social Environment",
    "section": "clean the crime data",
    "text": "clean the crime data\n\n\nCode\n#clean the crime dataset\ncrime_df_copy = crime_df_new.copy()\ndrop = ['Incident Datetime','Incident Date','Report Datetime','Row ID','Incident ID','Incident Number','CAD Number','Report Type Description','Filed Online','Incident Code','Incident Description',\n        'CNN','Supervisor District','Central Market/Tenderloin Boundary Polygon - Updated','Civic Center Harm Reduction Project Boundary','HSOC Zones as of 2018-06-05','Invest In Neighborhoods (IIN) Areas',\n        'Current Supervisor Districts','Current Police Districts']\ncrime_df_drop = crime_df_copy.drop(columns = drop)\ncrime_df_drop.info()\n\n\n\n\nCode\n#select the theft type data in 2020\ncrime_df_drop = crime_df_drop[crime_df_drop.year == 2020]\ncrime_df_drop = crime_df_drop[crime_df_drop['Incident Category'] == 'Larceny Theft']\ncrime_df_drop.reset_index()\n\n\n\n\nCode\n#get the geo boundary of San Francisco\nsf_poly = osmnx.geocode_to_gdf('San Francisco, CA, USA')\nsf_poly.plot()\n\n\n\n\nCode\n#convert to geopandas file\ngpd_crime = gpd.GeoDataFrame(crime_df_drop, geometry = crime_df_drop['geometry'], crs={'init' :'epsg:4326'})\nsf_poly.crs, gpd_crime.crs #.crs -- geodataframe\n\n\n\n\nCode\n#spatial join\ngpd_crime_city = gpd.sjoin(gpd_crime,sf_poly,how='inner',op = 'intersects')\ngpd_crime_city.tail(2)\n\n\n\n\nCode\ngpd_crime_city.info()\n\n\n\n\nCode\n#drop the useless data \ngpd_crime_city = gpd_crime_city.drop(['bbox_north','bbox_south','bbox_east','bbox_west','index_right'],axis = 1)"
  },
  {
    "objectID": "posts/welcome/505Final.html#create-the-physical-environment-data",
    "href": "posts/welcome/505Final.html#create-the-physical-environment-data",
    "title": "San Francisco Crime Analysis in Physical and Social Environment",
    "section": "Create the Physical Environment Data",
    "text": "Create the Physical Environment Data\n\nStreet\n\n\nCode\n#get the street data of san francisco\nsf_streets = osmnx.graph_from_place('San Francisco, California', network_type = 'drive')\nnodes, edges = osmnx.graph_to_gdfs(sf_streets)\n\n\n\n\nCode\nnodes.head(1)\n\n\n\n\nCode\nedges.head(1)\n\n\n\n\nCode\nedges.plot()\n\n\n\n\nCode\n# brief information of road dataset\nsf_road = edges.copy()\nprint(sf_road['highway'].value_counts())\nprint('Number of rows is ' + str(sf_road.shape[0]))\nprint('Number of columns is ' + str(sf_road.shape[1]))\n\n\n\n\nCode\n# Combining similar road types to reduce classification number\nsf_road['highway'] = sf_road['highway'].str.replace('_link','')\nsf_road['highway'] = np.where(sf_road['highway']=='trunk','secondary',sf_road['highway'])\nsf_road['highway'] = np.where(sf_road['highway']=='living_street','residential',sf_road['highway'])\nprint(sf_road['highway'].value_counts())\n\n\n\n\nCode\n#based on road type to create sub-dataset\nsf_highways = sf_road[sf_road.highway == 'motorway']\nsf_pry = sf_road[sf_road.highway == 'primary']\nsf_second = sf_road[sf_road.highway == 'secondary']\nsf_resid = sf_road[sf_road.highway == 'residential']\nsf_tertiary = sf_road[sf_road.highway == 'tertiary']\n\nsf_highways.crs\n\n\n\n\nCode\n# convert to geopandas files and set function to calcualte the min distance from point to road\ngpd_crime_city_utm = gpd_crime_city.to_crs({'init': 'epsg:32610'}).copy()   \nhighway_utm = sf_highways.to_crs({'init': 'epsg:32610'}).copy()\nprimary_utm = sf_pry.to_crs({'init': 'epsg:32610'}).copy()\nsecondary_utm = sf_second.to_crs({'init': 'epsg:32610'}).copy()\ntertiary_utm = sf_tertiary.to_crs({'init': 'epsg:32610'}).copy()\nresidential_utm = sf_resid.to_crs({'init': 'epsg:32610'}).copy()\nsf_road_utm = sf_road.to_crs({'init': 'epsg:32610'}).copy()\n\ndef distance_to_roadway(gps,roadway):\n  dists = []\n  for i in roadway.geometry:\n    dists.append(i.distance(gps))\n  return (np.min(dists))\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_highway'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = highway_utm)\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_primary'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = primary_utm)\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_secondary'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = secondary_utm)\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_tertiary'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = tertiary_utm)\n\n\n\n\nCode\ngpd_crime_city.head(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Time\nIncident Year\nIncident Day of Week\nReport Type Code\nIncident Category\nIncident Subcategory\nResolution\nIntersection\nPolice District\nAnalysis Neighborhood\n...\nlat\nlon\ndisplay_name\nclass\ntype\nimportance\nClosest_highway\nClosest_primary\nClosest_secondary\nClosest_tertiary\n\n\n\n\n9676\n02:00\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nBASS CT \\ WHITNEY YOUNG CIR\nBayview\nBayview Hunters Point\n...\n37.779026\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n1468.413426\n534.950176\n535.680061\n506.558042\n\n\n13583\n20:00\n2020\nSaturday\nII\nLarceny Theft\nLarceny - From Vehicle\nOpen or Active\nCLAY ST \\ LARKIN ST\nCentral\nRussian Hill\n...\n37.779026\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n2149.153684\n390.024468\n285.145981\n97.658841\n\n\n\n\n\n2 rows × 36 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# based on the closest distance from point to each type of roads to ensure the road type where the crime point locate\ndef get_category(row):\n    if row['Closest_highway'] &lt; 10:\n        return 'highway'\n    elif row['Closest_primary'] &lt; 10:\n        return 'primary'\n    elif row['Closest_secondary'] &lt; 10:\n        return 'secondary'\n    elif row['Closest_tertiary'] &lt; 10:\n        return 'tertiary'\n    else:\n        return 'residential'\ngpd_crime_city['Crime_Location'] = gpd_crime_city.apply(get_category, axis=1)\n\n\n\n\nCode\ngpd_crime_city.head(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Time\nIncident Year\nIncident Day of Week\nReport Type Code\nIncident Category\nIncident Subcategory\nResolution\nIntersection\nPolice District\nAnalysis Neighborhood\n...\nlon\ndisplay_name\nclass\ntype\nimportance\nClosest_highway\nClosest_primary\nClosest_secondary\nClosest_tertiary\nCrime_Location\n\n\n\n\n9676\n02:00\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nBASS CT \\ WHITNEY YOUNG CIR\nBayview\nBayview Hunters Point\n...\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n1468.413426\n534.950176\n535.680061\n506.558042\nresidential\n\n\n13583\n20:00\n2020\nSaturday\nII\nLarceny Theft\nLarceny - From Vehicle\nOpen or Active\nCLAY ST \\ LARKIN ST\nCentral\nRussian Hill\n...\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n2149.153684\n390.024468\n285.145981\n97.658841\nresidential\n\n\n\n\n\n2 rows × 37 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ngpd_crime_city.Crime_Location.value_counts()\n\n\nresidential    10518\nsecondary       8670\ntertiary        6382\nprimary         2520\nhighway          123\nName: Crime_Location, dtype: int64\n\n\n\n\nZoning\n\n\nCode\n#https://drive.google.com/file/d/1sremJO16LufbhJdXzSiAViXUmtJCieCN/view?usp=share_link\nid = \"1sremJO16LufbhJdXzSiAViXUmtJCieCN\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('SF_Zoning.csv')\n\n\n\n\nCode\n# load zoning data\nzoning_df = pd.read_csv('SF_Zoning.csv')\nzoning_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nthe_geom\nzoning_sim\ndistrictname\nurl\ngen\nzoning\ncodesection\n\n\n\n\n0\nMULTIPOLYGON (((-122.41533105377357 37.7723137...\nRED-MX\nRESIDENTIAL ENCLAVE-MIXED\nhttps://codelibrary.amlegal.com/codes/san_fran...\nMixed Use\nRED-MX\n847\n\n\n1\nMULTIPOLYGON (((-122.38902391194917 37.7381094...\nNCD\nBAYVIEW NEIGHBORHOOD COMMERCIAL DISTRICT\nhttps://codelibrary.amlegal.com/codes/san_fran...\nMixed Use\nNCD-BAYVIEW\n737\n\n\n2\nMULTIPOLYGON (((-122.389357706792 37.738606785...\nNCT-3\nMODERATE SCALE NEIGHBORHOOD COMMERCIAL TRANSIT...\nhttps://codelibrary.amlegal.com/codes/san_fran...\nMixed Use\nNCT-3\n752\n\n\n3\nMULTIPOLYGON (((-122.41533105377357 37.7723137...\nRED\nRESIDENTIAL ENCLAVE\nhttps://codelibrary.amlegal.com/codes/san_fran...\nResidential\nRED\n813\n\n\n4\nMULTIPOLYGON (((-122.38155774047206 37.7381229...\nPDR-1-B\nPDR LIGHT INDUSTRIAL BUFFER\nhttps://codelibrary.amlegal.com/codes/san_fran...\nIndustrial\nPDR-1-B\n210.3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nfrom shapely.wkt import loads\n\n\n\n\nCode\nzoning_df['geometry'] = zoning_df['the_geom'].apply(loads)\n\n# Create a GeoPandas DataFrame from the Pandas DataFrame\nzoning_gdf = gpd.GeoDataFrame(zoning_df, crs='EPSG:4326', geometry='geometry')\n\n\n\n\nCode\nzoning_gdf.head()\n\n\n\n\nCode\n#drop irrelevant data\ndrop = ['zoning_sim','url','zoning','codesection']\nzoning_new = zoning_gdf.drop(columns=drop)\nzoning_new.head()\n\n\n\n\nCode\n#brief visualization \nfig, ax = plt.subplots(figsize=(6, 4))\nzoning_new.to_crs(epsg=3857).plot(ax = ax,\n                figsize=(12,12),\n                markersize=40,\n               color=\"black\",\n               edgecolor=\"white\",\n               alpha=0.8,\n               marker=\"o\"\n            );\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n\n\n\nCode\n#spatial join zoning data and crime data\ngpd_crime_city_zone = gpd.sjoin(gpd_crime_city, zoning_new, op='within')\ngpd_crime_city_zone.head(2)\n\n\n\n\nBuilding\n\n\nCode\n#https://drive.google.com/file/d/1-7NryT-DamfZnKO7mU9ZqZsISg5TInsC/view?usp=share_link\nid = \"1-7NryT-DamfZnKO7mU9ZqZsISg5TInsC\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('sf_building.csv')\n\n\n\n\nCode\n#load building footprints data\nbuilding_df = pd.read_csv('sf_building.csv')\nbuilding_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsf16_bldgid\narea_id\nmblr\np2010_name\np2010_zminn88ft\np2010_zmaxn88ft\ngnd_cells50cm\ngnd_mincm\ngnd_maxcm\ngnd_rangecm\n...\nhgt_majoritycm\nhgt_minoritycm\nhgt_mediancm\ngnd_min_m\nmedian_1st_m\nhgt_median_m\ngnd1st_delta\npeak_1st_m\nglobalid\nshape\n\n\n\n\n0\n201006.000000\n1\nSF4570025\nSanfranF_4606.flt\n16.3249\n66.2671\n178250\n507\n704\n197\n...\n813\n349\n850\n5.07\n14.16\n8.50\n9.09\n23.85\n{CF7EF595-68E6-4950-B361-CC82D77383A0}\nMULTIPOLYGON (((-122.37950387699999 37.7397994...\n\n\n1\n201006.000000\n2\nSM005050270\nSanfranI_09417.flt\n81.7802\n214.1874\n144111\n2515\n3419\n904\n...\n737\n11\n735\n25.15\n39.25\n7.35\n14.10\n66.94\n{806F8EDB-7B25-4054-898E-6F2CF65D072E}\nMULTIPOLYGON (((-122.41894297800002 37.7076482...\n\n\n2\n201006.000000\n3\nSF3794028\nSanfran_Orig_1330.flt\n12.5027\n102.5737\n115295\n164\n507\n343\n...\n734\n-86\n1157\n1.64\n15.29\n11.57\n13.65\n59.18\n{F0F234CF-3F0B-439D-B59A-8899C2135F23}\nMULTIPOLYGON (((-122.38881976200003 37.7792352...\n\n\n3\n201006.003228\n32278\nSM004154340\nSanfranP_3364.flt\n342.5260\n381.3803\n708\n10454\n10532\n78\n...\n521\n140\n456\n104.54\n109.39\n4.56\n4.85\n111.79\n{C321EB73-7E6E-49B5-822B-3E7DD3A7F0AD}\nMULTIPOLYGON (((-122.45552821000001 37.7074943...\n\n\n4\n201006.000000\n4\nSF7295021\nNaN\nNaN\nNaN\n107634\n4346\n5185\n839\n...\n1078\n16\n1153\n43.46\n61.58\n11.53\n18.12\n69.72\n{C52C089C-B0A6-4716-8358-C568A65861DC}\nMULTIPOLYGON (((-122.47734992300002 37.7289024...\n\n\n\n\n\n5 rows × 43 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n##keep the geometry and height data\nbuilding_df = building_df[['peak_1st_m','shape']]\nbuilding_df.head(2)\n\n\n\n\nCode\nbuilding_df['geometry'] = building_df['shape'].apply(loads)\n# Create a GeoPandas DataFrame from the Pandas DataFrame\nbuilding_gdf = gpd.GeoDataFrame(building_df, crs='EPSG:4326', geometry='geometry')\n\n\n\n\nCode\nbuilding_gdf.head(2)\n\n\n\n\nCode\n# Calculate the area of each building\nbuilding_gdf['area'] = building_gdf['geometry'].area\nbuilding_gdf.head(2)\n\n\n\n\nCode\n# turan the form of data from polygon to point for find nearest point\npoint_gdf = building_gdf.copy()\npoint_gdf[\"geometry\"] = building_gdf.centroid\npoint_gdf.head(2)\n\n\n\n\nCode\ngpd_crime_city_zone = gpd_crime_city_zone.rename(columns={'index_left': 'left'})\ngpd_crime_city_zone = gpd_crime_city_zone.rename(columns={'index_right': 'right'})\n\n\n\n\nCode\ngpd_crime_city_zone.head(2)\n\n\n\n\nCode\nfrom google.colab import drive\ndrive.mount('/content/drive')\n#save_path = '/content/drive/MyDrive/Colab Notebooks/data/gpd_crime_city_zone.csv'\n#df.to_csv(save_path, index=False)\n\n\nMounted at /content/drive\n\n\n\n\nCode\n#https://drive.google.com/file/d/1WkxWvKEsSpq50x-BEasGqmsgu6pi91_m/view?usp=share_link\nid = \"1WkxWvKEsSpq50x-BEasGqmsgu6pi91_m\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('gpd_crime_city_zone.csv')\n\n\n\n\nCode\ngpd_crime_city_zone = pd.read_csv('gpd_crime_city_zone.csv')\ngpd_crime_city_zone.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Time\nIncident Year\nIncident Day of Week\nReport Type Code\nIncident Category\nIncident Subcategory\nResolution\nIntersection\nPolice District\nAnalysis Neighborhood\n...\nClosest_primary\nClosest_secondary\nClosest_tertiary\nCrime_Location\nright\nthe_geom\ndistrictname\ngen\nbuffer_5m\nbuffer\n\n\n\n\n0\n02:00\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nBASS CT \\ WHITNEY YOUNG CIR\nBayview\nBayview Hunters Point\n...\n534.950176\n535.680061\n506.558042\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.38469170531177 37.7337597458953...\nPOLYGON ((-117.38469170531177 37.7337597458953...\n\n\n1\n13:59\n2020\nSunday\nVI\nLarceny Theft\nTheft From Vehicle\nOpen or Active\nWHITNEY YOUNG CIR \\ CASHMERE ST \\ DEDMAN CT\nBayview\nBayview Hunters Point\n...\n364.965970\n433.206386\n203.201740\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.38571301129124 37.7366011278019...\nPOLYGON ((-117.38571301129124 37.7366011278019...\n\n\n2\n16:28\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nLA SALLE AVE \\ OSCEOLA LN\nBayview\nBayview Hunters Point\n...\n833.743266\n461.886008\n835.429639\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.3819485575142 37.73152913469796...\nPOLYGON ((-117.3819485575142 37.73152913469796...\n\n\n3\n18:00\n2020\nTuesday\nII\nLarceny Theft\nLarceny - Auto Parts\nOpen or Active\nINGALLS ST \\ HARBOR RD\nBayview\nBayview Hunters Point\n...\n951.461748\n255.279620\n825.220932\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.3799317268322 37.73338073279184...\nPOLYGON ((-117.3799317268322 37.73338073279184...\n\n\n4\n21:00\n2020\nSunday\nVI\nLarceny Theft\nTheft From Vehicle\nOpen or Active\nCASHMERE ST \\ HUDSON AVE\nBayview\nBayview Hunters Point\n...\n582.547890\n387.780374\n420.026310\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.38342544658872 37.7358032525956...\nPOLYGON ((-117.38342544658872 37.7358032525956...\n\n\n\n\n\n5 rows × 43 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ngpd_crime_city_zone = gpd.GeoDataFrame(gpd_crime_city_zone, geometry=gpd.points_from_xy(gpd_crime_city_zone.Longitude, gpd_crime_city_zone.Latitude), crs={'init' :'epsg:4326'})\ngpd_crime_city_zone.info()\n\n\n\n\nCode\nfrom scipy.spatial import cKDTree\n\n\n\n\nCode\npd_crime_city_zone = np.array(list(gpd_crime_city_zone.geometry.apply(lambda pt: (pt.x, pt.y))))\npoint_df = np.array(list(point_gdf.geometry.apply(lambda pt: (pt.x, pt.y))))\n\n# Build cKDTree from crime points\ntree = cKDTree(point_df)\n\n# Query tree for nearest crime to each Boston point\ndist, idx = tree.query(pd_crime_city_zone, k=1)\n\n# Add nearest crime column to boston GeoDataFrame\ngpd_crime_city_zone['builidng_nn1'] = point_gdf.loc[idx, 'peak_1st_m'].values\n\n\n\n\nCode\ndrop = ['buffer_5m','buffer','right']\ngpd_crime_city_zone = gpd_crime_city_zone.drop(columns=drop)\n\n\n\n\nCode\nval_dict = {'highway': 24, 'primary': 20, 'secondary': 12, 'tertiary': 8, 'residential': 6}\ngpd_crime_city_zone['road_width'] = gpd_crime_city_zone['Crime_Location'].map(val_dict)\n\n\n\n\nCode\ngpd_crime_city_zone['rate_BS'] = gpd_crime_city_zone['builidng_nn1']/gpd_crime_city_zone['road_width']\n\n\n\n\nCensus tract\n\nload basic census tract boundary\n\n\nCode\n#https://drive.google.com/file/d/1DbA90nYecBQnwRkcqUAAP8vT7OQP9MWz/view?usp=share_link\nid = \"1DbA90nYecBQnwRkcqUAAP8vT7OQP9MWz\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('sf_centra.csv')\n\n\n\n\nCode\nct_df = pd.read_csv('sf_centra.csv')\nct_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 176 entries, 0 to 175\nData columns (total 47 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   the_geom    176 non-null    object \n 1   OBJECTID    176 non-null    int64  \n 2   FIPSSTCO    176 non-null    int64  \n 3   TRT2000     176 non-null    int64  \n 4   STFID       176 non-null    int64  \n 5   TRACTID     176 non-null    float64\n 6   STATE       176 non-null    int64  \n 7   COUNTY      176 non-null    int64  \n 8   TRACT       176 non-null    int64  \n 9   POP2000     176 non-null    int64  \n 10  WHITE       176 non-null    int64  \n 11  BLACK       176 non-null    int64  \n 12  AMERI_ES    176 non-null    int64  \n 13  ASIAN       176 non-null    int64  \n 14  HAWN_PI     176 non-null    int64  \n 15  OTHER       176 non-null    int64  \n 16  MULT_RACE   176 non-null    int64  \n 17  HISPANIC    176 non-null    int64  \n 18  MALES       176 non-null    int64  \n 19  FEMALES     176 non-null    int64  \n 20  AGE_UNDER5  176 non-null    int64  \n 21  AGE_5_17    176 non-null    int64  \n 22  AGE_18_21   176 non-null    int64  \n 23  AGE_22_29   176 non-null    int64  \n 24  AGE_30_39   176 non-null    int64  \n 25  AGE_40_49   176 non-null    int64  \n 26  AGE_50_64   176 non-null    int64  \n 27  AGE_65_UP   176 non-null    int64  \n 28  MED_AGE     176 non-null    float64\n 29  MED_AGE_M   176 non-null    float64\n 30  MED_AGE_F   176 non-null    float64\n 31  HOUSEHOLDS  176 non-null    int64  \n 32  AVE_HH_SZ   176 non-null    float64\n 33  HSEHLD_1_M  176 non-null    int64  \n 34  HSEHLD_1_F  176 non-null    int64  \n 35  MARHH_CHD   176 non-null    int64  \n 36  MARHH_NO_C  176 non-null    int64  \n 37  MHH_CHILD   176 non-null    int64  \n 38  FHH_CHILD   176 non-null    int64  \n 39  FAMILIES    176 non-null    int64  \n 40  AVE_FAM_SZ  176 non-null    float64\n 41  HSE_UNITS   176 non-null    int64  \n 42  URBAN       176 non-null    int64  \n 43  RURAL       176 non-null    int64  \n 44  VACANT      176 non-null    int64  \n 45  OWNER_OCC   176 non-null    int64  \n 46  RENTER_OCC  176 non-null    int64  \ndtypes: float64(6), int64(40), object(1)\nmemory usage: 64.8+ KB\n\n\n\n\nCode\nct_df['TRACTID'] = ct_df['TRACTID'].astype('object')\n\n\n\n\nload social-economic data based on the census tract\n\n\nCode\nimport censusdata\nfrom census import Census\nfrom us import states\n\n\n\n\nCode\n# Define the Census API key\ncensusdata.censuskey = \"93db71b5bb2f994846b61d1cb53de6b204ed41b8\"\nc = Census(\"93db71b5bb2f994846b61d1cb53de6b204ed41b8\")\nyear = 2020\nstate_fips = '06'  # California\ncounty_fips = '075'  # San Francisco County\n\n#   B07010_004 -- Estimate!!Total:!!With income:!!$1 to $9,999 or loss -- poverty indicator\n# B06009_002 -- Estimate!!Total:!!Less than high school graduate\n# B09010_002 -- Estimate!!Total:!!Living in household with Supplemental Security Income (SSI), cash public assistance income, or Food Stamps/SNAP in the past 12 months:\n# B27011_008 -- Estimate!!Total:!!In labor force:!!Unemployed:\n# B11013_002 -- Estimate!!Total:!!Married-couple subfamily\n# B11013_005 -- Estimate!!Total:!!Mother-child subfamily\n# B11013_006 -- Estimate!!Total:!!Father-child subfamily\nva_census = c.acs5.state_county_tract(fields = ('NAME','B07010_004E','B06009_002E', 'B09010_002E', 'B27011_008E','B11014_005E','B11014_006E','B11014_009E'),\n                                      state_fips = '06',\n                                      county_fips = '075',\n                                      tract = \"*\",\n                                      year = 2020)\n\n\n\n\nCode\nsfp_df = pd.DataFrame(va_census)\nsfp_df.head(2)\n\n\n\n\nCode\nsfp_df['tract'] = sfp_df['NAME'].str.extract(r'Census Tract (\\d+\\.\\d+)', expand=False)\n\n\n\n\nCode\nsfp_df.head(2)\n\n\n\n\nCode\nsfp_df = sfp_df.rename(columns={'B07010_004E': 'low_income', 'B06009_002E':'low_education','B09010_002E':'fd_secure','B27011_008E':'unemploy','B11014_005E':'couple_fa',\n                                'B11014_006E':'ma-child','B11014_009E':'fa-child'})\nsfp_df.head(2)\n\n\n\n\nCode\nsfp_df = sfp_df.rename(columns={'tract': 'TRACTID'})\nsfp_df.head(2)\n\n\n\n\nCode\nsocieco_df = pd.merge(ct_df, sfp_df, on='TRACTID',how='left')\n\n\n\n\nCode\nct_df['geometry'] = ct_df['the_geom'].apply(loads)\n# Create a GeoPandas DataFrame from the Pandas DataFrame\nct_gdf = gpd.GeoDataFrame(ct_df, crs='EPSG:4326', geometry='geometry')"
  },
  {
    "objectID": "posts/germantown/index.html",
    "href": "posts/germantown/index.html",
    "title": "Germantown Equity Strategic Planning",
    "section": "",
    "text": "If want to see the complete version of our planning, please check the link and see the full planning book.\n\nIntroduction\nLocated in Northwest Philadelphia, Germantown is a culturally vibrant, historic neighborhood with over 50,000 residents calling the neighborhood home. Germantown’s borders are roughly bounded by Wissahickon Ave. to the west, Johnson & Washington Streets to the north, Stenton Ave. to east, Wister Street to the southwest, and Roberts Ave to the south. Originally an independent township established in the 1680s by German-Dutch settlers, Germantown was incorporated into Philadelphia almost 170 years ago in 1854. Today, the residential neighborhood has retained its character, with its namesake Avenue being a vital component of Germantown’s distinct identity. However, with increasing development occurring in the neighborhood, Germantown is at several crossroads after being affected by decades of disinvestment, population decline, high crime, and economic downturns."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/NY/550 Final.html",
    "href": "posts/NY/550 Final.html",
    "title": "How to Find My Bar in New York?",
    "section": "",
    "text": "After a busy week at work why not head to New York for a great weekend! But because there are so many options in New York, choosing where to stay and where to go for the night can sometimes be a difficult decision. Therefore, this analysis aims to analyze and visualize the data of Airbnb and bars in New York to provide a guide for future visitors to New York.\nThe dataset I would use in the project includes following: 1. New York City Airbnb Open Data. The dataset includes the features, prices, and location of the room. It will be the main dataset that for final Airbnb selection. 2. 2016 Parties in New York. The dataset includes the Location of the bar and the number of noise record for the bars. It will identify the number of entertainment venues in the vicinity of each site and the likely noise levels. 3. Uber picks up in New York City. The dataset includes the the pick up location and time of Uber. This dataset demonstrates the ease of travel behavior. 4. Census Data. The data will include the basic regional unit for discussion in the new step suggestion for visitors to choose their preferred airbnb.\n\n\nCode\n%env MYPATH=C:/Folder Name/file.txt\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\nimport folium\nimport xyzservices\nimport panel as pn\n\nimport datetime\nimport time\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nimport hvplot.pandas\nimport contextily as ctx\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\n\nimport altair as alt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport requests\nfrom sodapy import Socrata\nimport missingno as msno\nfrom scipy.stats import gaussian_kde\n\nimport osmnx as ox\nimport folium\nimport altair as alt\nfrom wordcloud import WordCloud\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# load dataset from google drive \nurl_basic = 'https://drive.google.com/uc?id='\nair='https://drive.google.com/file/d/1b0Hih_3K-xS3DZ6YKPSM_97yrGPEYlHE/view?usp=sharing'\nurl_air= url_basic + air.split('/')[-2]\n\nuber = 'https://drive.google.com/file/d/1p_BO6Kd_R-dRGLliP8cDBdC01pF2OqmJ/view?usp=sharing'\nurl_uber= url_basic + uber.split('/')[-2]\n\nbar = 'https://drive.google.com/file/d/1gLf9IyT6Vy2_ZiWfaQvOXRMMwOWPog3z/view?usp=sharing'\nurl_bar = url_basic + bar.split('/')[-2]\n\nparties = 'https://drive.google.com/file/d/1U0eZyg1UhuCSpDobZZgClj1v1STCfNvM/view?usp=sharing'\nurl_prt = url_basic + parties.split('/')[-2]\n\nparties_test = 'https://drive.google.com/file/d/1LVlJ64Wvj-p43AGpkH09BzUtJJeIAVwt/view?usp=sharing'\nurl_prt_test =  url_basic + parties_test.split('/')[-2]\n\nparties_train = 'https://drive.google.com/file/d/1ww_K4UF-xSagwnqz7nNoKH_Ojv0QL9Iy/view?usp=sharing'\nurl_prt_train =  url_basic + parties_train.split('/')[-2]\n\nboundary = 'https://drive.google.com/file/d/1nZz5GG3pPcNhNcvQYA_DTAaKAI5w6J-f/view?usp=sharing'\nurl_bund =  url_basic + boundary.split('/')[-2]\n\nnbhd = 'https://drive.google.com/file/d/1hI840aCWK2vbam-6SVT-NqTBM0BkLaY5/view?usp=sharing'\nurl_nbhd =  url_basic + nbhd.split('/')[-2]\n\nair_df = pd.read_csv(url_air)\nuber_df = pd.read_csv(url_uber,parse_dates=['Date/Time'])\nbar_df = pd.read_csv(url_bar)\nprt_df = pd.read_csv(url_prt,parse_dates=['Created Date','Closed Date'])\nprt_test_df = pd.read_csv(url_prt_test)\nprt_train_df = pd.read_csv(url_prt_train)\nbdry_gdf = gpd.read_file(url_bund,crs='EPSG:4326')\nnbhd_gdf = gpd.read_file(url_nbhd,crs='EPSG:4326')\n\n\n\n\nCode\n# airbnb dataset clean\ndropy = ['id','host_id','host_name','last_review']\nair_df = air_df.drop(dropy,axis=1)\nair_df['reviews_per_month'] = air_df['reviews_per_month'].fillna(0)\nair_df = air_df.dropna()\nair_df = air_df[air_df['price'] &lt;= 4000]\nair_gdf = gpd.GeoDataFrame(air_df, geometry=[Point(xy) for xy in zip(air_df.longitude, air_df.latitude)])\nair_gdf.set_crs(epsg=4326, inplace=True)\n\n\n\n\n\n\n\n\n\nname\nneighbourhood_group\nneighbourhood\nlatitude\nlongitude\nroom_type\nprice\nminimum_nights\nnumber_of_reviews\nreviews_per_month\ncalculated_host_listings_count\navailability_365\ngeometry\n\n\n\n\n0\nClean & quiet apt home by the park\nBrooklyn\nKensington\n40.64749\n-73.97237\nPrivate room\n149\n1\n9\n0.21\n6\n365\nPOINT (-73.97237 40.64749)\n\n\n1\nSkylit Midtown Castle\nManhattan\nMidtown\n40.75362\n-73.98377\nEntire home/apt\n225\n1\n45\n0.38\n2\n355\nPOINT (-73.98377 40.75362)\n\n\n2\nTHE VILLAGE OF HARLEM....NEW YORK !\nManhattan\nHarlem\n40.80902\n-73.94190\nPrivate room\n150\n3\n0\n0.00\n1\n365\nPOINT (-73.94190 40.80902)\n\n\n3\nCozy Entire Floor of Brownstone\nBrooklyn\nClinton Hill\n40.68514\n-73.95976\nEntire home/apt\n89\n1\n270\n4.64\n1\n194\nPOINT (-73.95976 40.68514)\n\n\n4\nEntire Apt: Spacious Studio/Loft by central park\nManhattan\nEast Harlem\n40.79851\n-73.94399\nEntire home/apt\n80\n10\n9\n0.10\n1\n0\nPOINT (-73.94399 40.79851)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48890\nCharming one bedroom - newly renovated rowhouse\nBrooklyn\nBedford-Stuyvesant\n40.67853\n-73.94995\nPrivate room\n70\n2\n0\n0.00\n2\n9\nPOINT (-73.94995 40.67853)\n\n\n48891\nAffordable room in Bushwick/East Williamsburg\nBrooklyn\nBushwick\n40.70184\n-73.93317\nPrivate room\n40\n4\n0\n0.00\n2\n36\nPOINT (-73.93317 40.70184)\n\n\n48892\nSunny Studio at Historical Neighborhood\nManhattan\nHarlem\n40.81475\n-73.94867\nEntire home/apt\n115\n10\n0\n0.00\n1\n27\nPOINT (-73.94867 40.81475)\n\n\n48893\n43rd St. Time Square-cozy single bed\nManhattan\nHell's Kitchen\n40.75751\n-73.99112\nShared room\n55\n1\n0\n0.00\n6\n2\nPOINT (-73.99112 40.75751)\n\n\n48894\nTrendy duplex in the very heart of Hell's Kitchen\nManhattan\nHell's Kitchen\n40.76404\n-73.98933\nPrivate room\n90\n7\n0\n0.00\n1\n23\nPOINT (-73.98933 40.76404)\n\n\n\n\n48847 rows × 13 columns\n\n\n\n\n\nCode\n# party dataset clean\nprt_df['duration'] = (prt_df['Closed Date'].view('int64') // 10**9 - prt_df['Created Date'].view('int64') // 10**9)/60\nprt_df['hour'] = prt_df['Closed Date'].dt.hour\nprt_df['dow'] = prt_df['Created Date'].dt.weekday\nprt_df['date'] = prt_df['Created Date'].dt.date\nprt_gdf = gpd.GeoDataFrame(prt_df, geometry=[Point(xy) for xy in zip(prt_df.Longitude, prt_df.Latitude)])\nprt_gdf.set_crs(epsg=4326, inplace=True)\nprt_gdf = prt_gdf[(prt_gdf['duration'] &gt; 0) & (prt_gdf['duration'] &lt; 480)]\nprt_gdf.head(5)\n\n\n\n\n\n\n\n\n\nCreated Date\nClosed Date\nLocation Type\nIncident Zip\nCity\nBorough\nLatitude\nLongitude\nduration\nhour\ndow\ndate\ngeometry\n\n\n\n\n0\n2015-12-31 00:01:15\n2015-12-31 03:48:04\nStore/Commercial\n10034.0\nNEW YORK\nMANHATTAN\n40.866183\n-73.918930\n226.816667\n3.0\n3\n2015-12-31\nPOINT (-73.91893 40.86618)\n\n\n1\n2015-12-31 00:02:48\n2015-12-31 04:36:13\nStore/Commercial\n10040.0\nNEW YORK\nMANHATTAN\n40.859324\n-73.931237\n273.416667\n4.0\n3\n2015-12-31\nPOINT (-73.93124 40.85932)\n\n\n2\n2015-12-31 00:03:25\n2015-12-31 00:40:15\nResidential Building/House\n10026.0\nNEW YORK\nMANHATTAN\n40.799415\n-73.953371\n36.833333\n0.0\n3\n2015-12-31\nPOINT (-73.95337 40.79942)\n\n\n3\n2015-12-31 00:03:26\n2015-12-31 01:53:38\nResidential Building/House\n11231.0\nBROOKLYN\nBROOKLYN\n40.678285\n-73.994668\n110.200000\n1.0\n3\n2015-12-31\nPOINT (-73.99467 40.67829)\n\n\n4\n2015-12-31 00:05:10\n2015-12-31 03:49:10\nResidential Building/House\n10033.0\nNEW YORK\nMANHATTAN\n40.850304\n-73.938516\n224.000000\n3.0\n3\n2015-12-31\nPOINT (-73.93852 40.85030)\n\n\n\n\n\n\n\n\n\nCode\nuber_df['hour'] = uber_df['Date/Time'].dt.hour\nuber_df['dow'] = uber_df['Date/Time'].dt.weekday\nuber_df['date'] = uber_df['Date/Time'].dt.date\nuber_gdf = gpd.GeoDataFrame(uber_df, geometry=[Point(xy) for xy in zip(uber_df.Lon, uber_df.Lat)])\nuber_gdf.set_crs(epsg=4326, inplace=True)\nuber_gdf.head(5)\n\n\n\n\n\n\n\n\n\nDate/Time\nLat\nLon\nBase\nhour\ndow\ndate\ngeometry\n\n\n\n\n0\n2014-06-01 00:00:00\n40.7293\n-73.9920\nB02512\n0\n6\n2014-06-01\nPOINT (-73.99200 40.72930)\n\n\n1\n2014-06-01 00:01:00\n40.7131\n-74.0097\nB02512\n0\n6\n2014-06-01\nPOINT (-74.00970 40.71310)\n\n\n2\n2014-06-01 00:04:00\n40.3461\n-74.6610\nB02512\n0\n6\n2014-06-01\nPOINT (-74.66100 40.34610)\n\n\n3\n2014-06-01 00:04:00\n40.7555\n-73.9833\nB02512\n0\n6\n2014-06-01\nPOINT (-73.98330 40.75550)\n\n\n4\n2014-06-01 00:07:00\n40.6880\n-74.1831\nB02512\n0\n6\n2014-06-01\nPOINT (-74.18310 40.68800)\n\n\n\n\n\n\n\n\n\nCode\nbar_gdf = gpd.GeoDataFrame(bar_df, geometry=[Point(xy) for xy in zip(bar_df.Longitude, bar_df.Latitude)])\nbar_gdf.set_crs(epsg=4326, inplace=True)\nbar_gdf = bar_gdf[bar_gdf['num_calls']&lt;250]\nbar_gdf.head(5)\n\n\n\n\n\n\n\n\n\nLocation Type\nIncident Zip\nCity\nBorough\nLatitude\nLongitude\nnum_calls\ngeometry\n\n\n\n\n0\nClub/Bar/Restaurant\n10308.0\nSTATEN ISLAND\nSTATEN ISLAND\n40.544096\n-74.141155\n40\nPOINT (-74.14115 40.54410)\n\n\n1\nClub/Bar/Restaurant\n10012.0\nNEW YORK\nMANHATTAN\n40.729793\n-73.998842\n18\nPOINT (-73.99884 40.72979)\n\n\n2\nClub/Bar/Restaurant\n10308.0\nSTATEN ISLAND\nSTATEN ISLAND\n40.544209\n-74.141040\n21\nPOINT (-74.14104 40.54421)\n\n\n3\nClub/Bar/Restaurant\n10034.0\nNEW YORK\nMANHATTAN\n40.866376\n-73.928258\n160\nPOINT (-73.92826 40.86638)\n\n\n4\nClub/Bar/Restaurant\n11220.0\nBROOKLYN\nBROOKLYN\n40.635207\n-74.020285\n17\nPOINT (-74.02028 40.63521)"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-should-i-live-airbnb-analysis-visualization",
    "href": "posts/NY/550 Final.html#where-should-i-live-airbnb-analysis-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where should I live? – Airbnb Analysis & Visualization",
    "text": "Where should I live? – Airbnb Analysis & Visualization\n\n\nCode\n# group by bonough\nnbhd_price = air_df.groupby('neighbourhood_group')['price'].mean()\nnbhd_price  = nbhd_price .reset_index()\nbdry_price = bdry_gdf.merge(nbhd_price,left_on='boro_name', right_on='neighbourhood_group', how='inner')\n\n\n\n\nCode\n# group by neighborhood\nair_cen_gdf =air_gdf.sjoin(nbhd_gdf, how='inner')\ncensus_price = air_cen_gdf.groupby('ntacode')['price'].mean()\ncensus_price = census_price.reset_index()\ncensus_review = air_cen_gdf.groupby('ntacode')['number_of_reviews'].mean()\ncensus_review = census_review.reset_index()\ncensus_rm = air_cen_gdf.groupby('ntacode')['reviews_per_month'].mean()\ncensus_rm = census_rm.reset_index()\ncensus_pr = census_price.merge(census_review,on='ntacode')\ncensus_prm = census_pr.merge(census_rm,on = 'ntacode')\ncensus_prm = nbhd_gdf.merge(census_prm[['price','number_of_reviews','reviews_per_month','ntacode']],on='ntacode')\n\n\nTo better understand the airbnb situation in New York, I firstly visualized the statistical distribution of Airbnb data. We were able to find a distribution of prices that, with the exception of some of the higher-priced listings, was close to a normal distribution for most of the homes, with a concentration in the $20-$500 a night range. As for the number of reviews, We were able to find that the vast majority of listings received lower reviews. When it comes to room type, the ‘entire room’ and ‘private room’ took the major part. What’s more, most home located in Manhattan and Brooklyn, which can be explained by the fact that Brooklyn and Manhattan have most of New York’s places to hang out.\n\n\nCode\n# airbnb situation in New York\n_,axss = plt.subplots(2,2,figsize = [20,8])\nsns.histplot(air_df['price'], bins=80, kde=False, color='#2a9d8f',ax = axss[0,0])\nsns.histplot(air_df['number_of_reviews'], bins=100, kde=False, color='#2a9d8f',ax = axss[0,1])\nsns.countplot(x='room_type', data=air_df,ax = axss[1,0])\nsns.countplot(x='neighbourhood_group', data=air_df,ax = axss[1,1])\nplt.show()\n\n\n\n\n\nTo better understand Airbnb’s geographic distribution patterns on New York, we used map visualizations for further analysis. The spatial distribution of locations shows that the density of listings gradually decreases in all directions, centered on Manhattan, while prices show an accumulation of higher prices at the center of density. Looking at average home prices in the greater region, Manhattan and Brooklyn have the first and second highest.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5), facecolor=\"#e5e7eb\")\n\n# Plot\nair_gdf.plot(\n    ax=ax,\n    column=\"price\",\n    edgecolor=\"black\",\n    linewidth=0.1,\n    legend=True,\n    legend_kwds=dict(loc=\"lower right\", fontsize=10),\n    cmap=\"Reds\",\n    markersize=2,\n    scheme=\"Quantiles\",\n    k=5,\n)\n\nax.set_title(\"Airbnb Price Location in NY\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n\nCode\nm = bdry_price.explore(column=\"price\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd when we look at the distribution of prices and reviews for Audemars Piguet at a smaller neighborhood scale, we are able to see that Audemars Piguet’s prices are also gradually decreasing in all directions, centered on Upper Manhattan. As for the reviews of the listings, we were able to find that the number of reviews in the higher priced areas is relatively low. The lower priced areas have a higher number of reviews as well as a higher average number of reviews per month. This can be explained by the fact that higher priced homes have a relatively smaller audience and less affordable people.\n\n\nCode\nm = census_prm.explore(column=\"number_of_reviews\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\nm = census_prm.explore(column=\"price\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nOverall, the price of a listing and more comprehensive information is overall not available at the same time. Those who are not price-sensitive have more options located in Mankato as well as Northwest Brooklyn. For a more cost-effective and comprehensive option, consider listings in the Bronx and Queens!"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-can-i-have-fun-entertainment-analysis-visualization",
    "href": "posts/NY/550 Final.html#where-can-i-have-fun-entertainment-analysis-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where Can I have fun? – Entertainment Analysis & Visualization",
    "text": "Where Can I have fun? – Entertainment Analysis & Visualization\nIn addition to accommodation options, we also paid equal attention to what ‘post-fun’ places there are to choose from in New York outside of everyday play. So why not go to a bar? From the point of view of the distribution of bars in New York, most of the bars are still concentrated in Manhattan, where the most prosperous business activities and nightlife in New York. However, when we look at complaints, we find that the average number of complaints received by bars in areas other than the Bronx is about the same, with Queens having the highest average, which is probably due to the fact that Queens itself is a large neighborhood. And as noisy areas are accompanied by disturbances at night, areas near bars with high complaints should be avoided as much as possible when choosing an Airbnb.\n\n\nCode\nbar_call = bar_gdf.groupby('Borough')['num_calls'].mean()\nbar_call = bar_call.reset_index()\nbar_call['Borough'] = bar_call['Borough'].str.lower()\nbar_count = bar_gdf.groupby('Borough').count()\nbar_count = bar_count.reset_index()\nbar_count = bar_count[['Borough','City']]\nbar_count['Borough'] = bar_count['Borough'].str.lower()\nbar_count.columns = ['Borough', 'Count']\nbar_cc = bar_call.merge(bar_count,on='Borough')\nbdry_gdf['boro_name'] = bdry_gdf['boro_name'].str.lower()\nbar_bd = bdry_gdf.merge(bar_cc,left_on='boro_name', right_on='Borough', how='inner')\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5), facecolor=\"#e5e7eb\")\n\n# Plot\nbar_gdf.plot(\n    ax=ax,\n    column=\"num_calls\",\n    edgecolor=\"black\",\n    linewidth=0.1,\n    legend=True,\n    legend_kwds=dict(loc=\"lower right\", fontsize=10),\n    cmap=\"Reds\",\n    markersize=2,\n    scheme=\"Quantiles\",\n    k=5,\n)\n\nax.set_title(\"Bar Complaints in NY\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n\nCode\n_,axss = plt.subplots(2,figsize = [20,10])\nsns.barplot(x = 'boro_name', y ='num_calls',data = bar_bd,ax = axss[1])\nsns.barplot(x = 'boro_name', y ='Count',data = bar_bd,ax = axss[0])\n\n\n&lt;Axes: xlabel='boro_name', ylabel='Count'&gt;\n\n\n\n\n\n\n\nCode\nbar_cen_gdf =bar_gdf.sjoin(nbhd_gdf, how='inner')\nbar_call = bar_cen_gdf.groupby('ntacode')['num_calls'].mean()\nbar_call = bar_call.reset_index()\nbar_call_sum = bar_cen_gdf.groupby('ntacode').count()\nbar_call_sum = bar_call_sum.reset_index()\nbar_call_sum = bar_call_sum[['ntacode','City']]\ncen_bar = nbhd_gdf.merge(bar_call,on='ntacode')\ncen_bar = cen_bar.merge(bar_call_sum,on='ntacode')\n\n\n\n\nCode\nbar_cc = bar_call.merge(bar_call_sum,on='ntacode')\nbar_cc.head(4)\n\n\n\n\n\n\n\n\n\nntacode\nnum_calls\nCity\n\n\n\n\n0\nBK09\n12.000000\n4\n\n\n1\nBK17\n37.714286\n7\n\n\n2\nBK19\n58.800000\n5\n\n\n3\nBK21\n14.000000\n1\n\n\n\n\n\n\n\n\n\nCode\nm = cen_bar.explore(column=\"num_calls\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\nm = cen_bar.explore(column=\"City\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd when we looked at where and when parties were held in New York, we were able to see that the most parties were held in residential buildings, and the vast majority of parties ended between nighttime hours and 5 a.m. the next day, which means that most of the venues where parties took place probably weren’t a good choice for an Airbnb location!\n\n\nCode\n_,axss = plt.subplots(2,2,figsize = [20,10])\nsns.countplot(x = 'dow',data = prt_gdf,ax=axss[0,0])\nsns.countplot(x = 'hour',data = prt_gdf,ax=axss[0,1])\nsns.histplot(prt_gdf['duration'],bins=80, kde=False, color='#2a9d8f',ax = axss[1,0])\nsns.countplot(x = 'Location Type',data = prt_gdf,ax=axss[1,1])\n\n\n&lt;Axes: xlabel='Location Type', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-can-i-find-a-bar-clustering-visualization",
    "href": "posts/NY/550 Final.html#where-can-i-find-a-bar-clustering-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where Can I Find a Bar? – Clustering Visualization",
    "text": "Where Can I Find a Bar? – Clustering Visualization\nBased on cluster analysis, we are able to provide travelers with Airbnb location options that meet their needs for different needs. For example, if our traveler is a person who does not require a lot of accommodation but wants to go to a bar in the evening and wants to take a taxi home quickly, we analyzed the clustering of bar and Uber related metrics and found that areas with label 3 and 4 are very suitable for the location where he/she is going to visit.\n\n\nCode\npbua_nb_gdf.groupby(\"travel_bar\", as_index=False)[['num_uber','num_bar','num_party','duration']].mean().sort_values(by=\"travel_bar\")\n\n\n\n\n\n\n\n\n\ntravel_bar\nnum_uber\nnum_bar\nnum_party\nduration\n\n\n\n\n0\n0\n809.397436\n9.487179\n780.089744\n142.621888\n\n\n1\n1\n100.377049\n6.524590\n733.622951\n205.199978\n\n\n2\n2\n763.187500\n27.875000\n3645.875000\n149.463024\n\n\n3\n3\n22529.000000\n69.500000\n2057.250000\n119.591318\n\n\n4\n4\n5410.800000\n106.000000\n2492.000000\n147.806346\n\n\n\n\n\n\n\n\n\nCode\n# setup the figure\nf, ax = plt.subplots(figsize=(10, 8))\n\n# plot, coloring by label column\n# specify categorical data and add legend\npbua_nb_gdf.plot(\n    column=\"travel_bar\",\n    cmap=\"Dark2\",\n    categorical=True,\n    legend=True,\n    edgecolor=\"k\",\n    lw=0.5,\n    ax=ax,\n)\n\n\nax.set_axis_off()\nplt.axis(\"equal\");"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-can-i-find-a-quite-airbnb-clustering-visualization",
    "href": "posts/NY/550 Final.html#where-can-i-find-a-quite-airbnb-clustering-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where Can I Find A Quite Airbnb? – Clustering Visualization",
    "text": "Where Can I Find A Quite Airbnb? – Clustering Visualization\nLet’s take another example. Emily wants to spend a nice weekend in New York City, but she wants to avoid too many bars in the neighborhood because they are loud and potentially dangerous. On the other hand, Emily doesn’t have a big budget, so she doesn’t want to spend too much money on Airbnb. From the cluster analysis of ‘Airbnb-Bar’, we can find that the area represented by cluster 2 meets Emily’s needs. Overall, from the map, the intersection of Bronx Grove and Queens would be a great residential option for Emily.\n\n\nCode\npbua_nb_gdf.groupby(\"bar_air\", as_index=False)[['num_party','num_calls','num_bar','price','number_of_reviews','reviews_per_month']].mean().sort_values(by=\"bar_air\")\n\n\n\n\n\n\n\n\n\nbar_air\nnum_party\nnum_calls\nnum_bar\nprice\nnumber_of_reviews\nreviews_per_month\n\n\n\n\n0\n0\n3594.937500\n39.004224\n29.125000\n112.422663\n24.804384\n1.061521\n\n\n1\n1\n933.514286\n34.081496\n5.942857\n91.453277\n38.928529\n2.204802\n\n\n2\n2\n2465.166667\n33.000383\n107.333333\n180.279458\n23.902023\n1.028439\n\n\n3\n3\n994.100000\n27.607487\n24.800000\n209.540961\n18.689871\n0.836661\n\n\n4\n4\n686.850575\n32.515925\n6.643678\n95.245483\n19.849280\n1.219340\n\n\n\n\n\n\n\n\n\nCode\n# setup the figure\nf, ax = plt.subplots(figsize=(10, 8))\n\n# plot, coloring by label column\n# specify categorical data and add legend\npbua_nb_gdf.plot(\n    column=\"bar_air\",\n    cmap=\"Dark2\",\n    categorical=True,\n    legend=True,\n    edgecolor=\"k\",\n    lw=0.5,\n    ax=ax,\n)\n\n\nax.set_axis_off()\nplt.axis(\"equal\");"
  },
  {
    "objectID": "posts/NY/550 Final.html#interactive-airbnb-location-selection-tools",
    "href": "posts/NY/550 Final.html#interactive-airbnb-location-selection-tools",
    "title": "How to Find My Bar in New York?",
    "section": "Interactive Airbnb Location Selection Tools",
    "text": "Interactive Airbnb Location Selection Tools\nAnd similarly, we provide an interactive map of the specific locations of Airbnb’s in the community, where visitors can see the specific prices and number and frequency of reviews of Airbnb’s in their preferred neighborhood, which can further help them make the right choice for them.\n\n\nCode\npn.extension(\"tabulator\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nair = air_gdf.sjoin(pbua_nb_gdf,how='inner')\n\n\n\n\nCode\nnt_names = list(pbua_nb_gdf['ntaname'].unique())\n\nneighborhoodSelect = pn.widgets.Select(\n    value=\"St. Albans\", options=nt_names, name=\"Neighborhood\"\n)\n\nneighborhoodSelect\n\n\n\n\nCode\ndef filter_by_neighborhood(data, neighborhood_name):\n    sel = data[\"ntaname\"] == neighborhood_name\n    return data.loc[sel]\n\ndef airbnb_data(data, neighborhood_name):\n    sel = nbhd_gdf[\"ntaname\"] == neighborhood_name\n    hood_geo = nbhd_gdf.loc[sel]\n\n    m = hood_geo.explore(\n        style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n        name=\"Neighborhood boundary\",\n        tiles=xyzservices.providers.CartoDB.Voyager,\n    )\n\n    data.explore(\n        m=m,  # Add to the existing map!\n        marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n        marker_type=\"circle_marker\",  # or 'marker' or 'circle'\n        name=\"Tickets\",\n    )\n    return m\n\ndef create_dashboard_1(neighborhood_name):\n    tickets = filter_by_neighborhood(air, neighborhood_name)\n    m = airbnb_data(tickets, neighborhood_name)\n    return pn.pane.plot.Folium(m, height=600)\n\n\n\n\nCode\nticket_dashboard_1 = pn.Column(\n    pn.Column(\"## Airbnb in Your Neighborhood\", neighborhoodSelect),\n    # Add a height spacer\n    pn.Spacer(height=45),\n    # Bottom: the main chart, bind widgets to the function\n    pn.bind(create_dashboard_1, neighborhood_name=neighborhoodSelect),\n)\n\nticket_dashboard_1"
  },
  {
    "objectID": "posts/NY/550 Final.html#general-feature-of-airbnb-in-one-neighborhood",
    "href": "posts/NY/550 Final.html#general-feature-of-airbnb-in-one-neighborhood",
    "title": "How to Find My Bar in New York?",
    "section": "General Feature of Airbnb in One Neighborhood",
    "text": "General Feature of Airbnb in One Neighborhood\nBased on the above analysis, we have been able to provide different tourists with the range of Airbnb’s they need for their choice of community. But again, the generalization about Airbnb’s within such a community is something we would like to describe to visitors. Therefore, we have selected the names of Airbnb within the community range (as they contain some attractive features of the listings) for word cloud analysis to get the common features of Airbnb within a community to help the tourists in further screening.\n\n\nCode\ndef fnstring(name):\n    fi = air.loc[air['ntaname'] == name]\n    str_list = fi['name'].astype(str).tolist()\n    combined_string = ' '.join(str_list)\n    return combined_string\n\n\n\n\nCode\ndef wcloud(name):\n    wc = WordCloud(\n    background_color=\"black\", max_words=100, width=1000, height=500, colormap=\"tab20c\"\n)\n    text = fnstring(name)\n    img = wc.generate(text)\n    fig, ax = plt.subplots()\n    ax.imshow(img, interpolation=\"bilinear\")\n    ax.set_axis_off()\n    plt.show();\n\n\n\n\nCode\ntemp = pn.Column(\n    pn.Column(\"## Airbnb in Your Neighborhood\", neighborhoodSelect),\n    # Add a height spacer\n    pn.Spacer(height=45),\n    # Bottom: the main chart, bind widgets to the function\n    pn.bind(wcloud, name=neighborhoodSelect),\n)\ntemp"
  },
  {
    "objectID": "posts/design/index.html",
    "href": "posts/design/index.html",
    "title": "Landscape Architecture & Urban Design Work",
    "section": "",
    "text": "If want to see the complete version of my portfolio, please check the link and see the full working sample.\nSince the outbreak of COVID-19, the social distance kept between each other seems to become the new norm of life. In the mid-summer of 2020, I went out to a park and lay down on the open sloped lawn. I felt that I was no longer a spec of sand in isolation. At that moment, I realized that the environment that I had taken for granted had, in a subtle way, provided so many windows to connect with the world, and there were more subjects in the world sharing this beauty together.\n\nAs a medium, the landscape provides the possibility of sharing stories in a temporal dimension. the landscape in this process is like a window that builds up a temporal connection, reflecting the transparent superimposed relationship of different historical processes. My hometown’s 3000 years historical memory has been lost in the washout of commercialization. The urban renewal should not simply function as a fresh new sheet, but a palimpsest, an overlay of new and old footprint, so that the future can have the opportunity to dialogue with the past through the gap between the layers. Therefore, in the design of the Palace Ruins Park, through the secondary transformation of the sound generated by the users’ activities at present, I built a temporal connection with the sound generated by the ritual procedures that took place in the past palace, and therefore reveal the transparency of the site in the temporal dimension.\n\n\nLikewise, landscape as a medium offers the opportunity of sharing space and function for different users. Unfortunately, during urbanization, the social stratification of the landscape is happening inexorably. In the present day, all participants in urban activities have unconsciously become recipients of that information, leading to the spontaneity of the people involved in public activities. In the project on urban agriculture, I learned that rural community has lost their farming land during urbanization and gentrification. More and more vulnerable groups lost their right to live freely and had to follow the direction of the social hierarchical order implied by the established space. The rediscovery of the existing urban space should give a new meaning to the public space that was considered as low value. By combining urban agriculture and transportation trails, I created re-employment for lost-land farmers and offered the possibility of a space for all people to engage in activities that eliminate class differences.\n\nMy academic experience in landscape has provided me a chance to reflect on the drawbacks of my home city’s development. I always remember the delightful memory of weekend hiking and the pure environment when I was a kid. Meanwhile, I will never forget the children’s running and climbing in the temporary installation that I constructed, which reminds me what contribution I can make to society. In the future, my goal is to go back to my home city and create public space without class differences and time limits."
  }
]