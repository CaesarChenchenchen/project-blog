[
  {
    "objectID": "posts/my_first_post/index.html",
    "href": "posts/my_first_post/index.html",
    "title": "My First post",
    "section": "",
    "text": "This is an example first post. Whatever you write here will appear inside your post.\nIt is possible to write lists:\nAnd make headers"
  },
  {
    "objectID": "posts/my_first_post/index.html#this-is-a-second-level-heading",
    "href": "posts/my_first_post/index.html#this-is-a-second-level-heading",
    "title": "My First post",
    "section": "This is a second level heading",
    "text": "This is a second level heading\nYou can include images, and many other kinds of content.\nIf you are using the visual editor in Rstudio you can drag images onto the editor to insert them into the document. Otherwise, you need to place the image inside of the folder for this post, and then you can insert it to your post directly, like this:\n\nThe first image in a blog post will also be used on the listings page."
  },
  {
    "objectID": "posts/Example_assignment/index.html",
    "href": "posts/Example_assignment/index.html",
    "title": "Example assignment",
    "section": "",
    "text": "This assignment engages you in the process of introspection. Your task is to use introspection to evaluate and describe your own mental imagery abilities. Attempt to answer the following kinds of questions. What is your mental imagery like? Do you have mental imagery for different kinds of senses? Is your mental imagery vivid and life-like or very different from normal perception? How would you describe your mental imagery?\nYou should write a minimum of 250 words, but feel free to write more. Submit your document on blackboard by the due date."
  },
  {
    "objectID": "posts/Example_assignment/index.html#my-mental-imagery",
    "href": "posts/Example_assignment/index.html#my-mental-imagery",
    "title": "Example assignment",
    "section": "My mental imagery",
    "text": "My mental imagery\nMy mental imagery is like…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explore the World in Data!",
    "section": "",
    "text": "How to Find My Bar in New York?\n\n\n\n\n\n\n\nspatial\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nTianxiao Chen\n\n\n\n\n\n\n  \n\n\n\n\nCamden Gateway District Plan\n\n\n\n\n\n\n\nplanning\n\n\nbook\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nAlexander Nelms, Tianxiao Chen, Jayden Schultz, Kaye LI, chenglin Hu, Avery Weiss, Jackson LaSarso, Yuanhao Zhai\n\n\n\n\n\n\n  \n\n\n\n\nNJ Transit Delay Time Prediction\n\n\n\n\n\n\n\ntransit\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nTianxiao & Ling\n\n\n\n\n\n\n  \n\n\n\n\nShared Bike Usage Prediciton\n\n\n\n\n\n\n\ntransit\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nTianxiao\n\n\n\n\n\n\n  \n\n\n\n\nHousing Subsidy\n\n\n\n\n\n\n\nreal estate\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\nTianxiao\n\n\n\n\n\n\n  \n\n\n\n\nHouse Price Prediction, Philadelphia\n\n\n\n\n\n\n\nreal estate\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nTianxiao & Ling\n\n\n\n\n\n\n  \n\n\n\n\nTOD Development – Washington DC for example\n\n\n\n\n\n\n\ntransit\n\n\nplanning\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nTianxiao Chen\n\n\n\n\n\n\n  \n\n\n\n\nGermantown Equity Strategic Planning\n\n\n\n\n\n\n\nplanning\n\n\nbook\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nTre Ambroise, Tianxiao Chen, Shuting Li, Sophie Maes, Brenna Schmidt, Mimi Tran, Evan Zhao\n\n\n\n\n\n\n  \n\n\n\n\nSan Francisco Crime Analysis in Physical and Social Environment\n\n\n\n\n\n\n\nspatial\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nTianxiao Chen\n\n\n\n\n\n\n  \n\n\n\n\nLandscape Architecture & Urban Design Work\n\n\n\n\n\n\n\nurban design\n\n\nlandscape architecture\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2022\n\n\nTianxiao Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "This is a template for using quarto to create a course blog. A course blog can be used in many ways to engage with course content. For example, you could use the blog to post assignments, or to dive more deeply into course material that interests you. By the end of the course, your blog can serve as a portfolio of ways that you engaged in the material. By learning how to use quarto for your blog, you will also be learning new skills for creating and sharing reproducible documents that could be useful to you in the future.\nThe purpose of this page is to provide tips and pointers about blogging with quarto. Quarto is simple enough for creating a basic course blog. However, it is also very deep and can be used to create all sorts of documents, from website, to slide decks, and books.\n\n\nMany questions about quarto can be answered from the quarto documentation located at: https://quarto.org.\nIf you are looking for something specific and don’t know where to find it on the website, use the search tool in the top right corner.\n\n\n\n\nTo use this template you will need a Github.com account, and access to R and Rstudio.\nYou can get access by creating a Github.com account, and downloading the necessary open-source software to your machine.\nIt is also possible to use Rstudio in your web-browser, which does not require downloading any software. There are two sets of instructions, one for the downloading approach, and the other for using Rstudio cloud. Scroll to the bottom for instructions on using Rstudio cloud.\n\n\n\nIn order to use this template you will need to install some free open-source software on your computer.\n\nSign up for a free account at https://github.com. This a website for sharing open-source software, but it can also be used to serve your blog as a website for free.\nDownload Github Desktop and install it on your machine. This should install the version control software git on your system, and you can use Github Desktop to easily push your blog from your local computer so that it can be viewed on Github.com.\nDownload R and install it on your machine. R is a programming language capable of many things, and it needs to be on your machine before you can run R Studio.\nDownload R Studio Desktop and install it on your machine. R Studio is called an “IDE” or integrated development environment, that you can use to write your blog with quarto.\n\n\n\n\nOnce you have the software installed, the next step is to create a quarto blog project in R studio. The collection of files in this template is a pre-made quarto blog project that you can modify for your own purposes. You can also make one yourself in Rstudio.\nAssuming you have downloaded this template, and you have installed the above software, then you need to open quartoCourseBlog.Rproj.\n\n\n\n\nTo find out if everything is working, try rendering the blog. Go to the “Build” Tab and press “Render Website”.\n\nAfter the rendering is complete, you should be able to view your blog. It might show up in the viewer pane like this:\n\nAnd if you press the ‘window-with-an-arrow’ button, you can view the website in your default browser. Quarto websites automatically adjust for the size of the window, so it may appear differently in the viewer pane versus the browser.\n\n\n\nAll of the blog posts are located in the posts folder.\n\nTo make a new post, copy an existing post and then modify it. For example, my posts folder currently contains one post, and it is inside the my_first_post folder.\n\nI can copy the folder and make a new one with the same contents from the Rstudio gear-box menu:\n\nI made a folder for a second post called Example_assignment.\n\nThese are the two files inside the folder. The .qmd file is a plain text file where you will write the blog post. This folder can also be used to store other assets you might put in the post, such as pictures.\n\n\n\nTo write a new post, open the .qmd file, edit the text, and then re-render the website. This is what the text in the .qmd file looked like when I copied it.\n\nThe text at the top between the “---” is called YAML, and provides meta-data for your document. This is where you can change the title, date, name, and add keywords if you want.\nThe rest of the document is for the main body of the post. For example, I changed the text to read:\n\n\n\n\n\nRender the website from the build tab again to see your new post.\n\n\n\nTo share your blog online you will have to publish it on a server that can be accessed by other people on the internet. There are multiple ways to do this step, and I recommend using Github pages. You can view more in-depth instructions from quarto here https://quarto.org/docs/publishing/github-pages.html.\nHere are the steps:\n\nOpen Github Desktop\nGo to preferences and sign in to your Github.com account\n“Add” your blog project folder to Github Desktop\nThere should be an option for a commit message, write a note in there like “first commit”.\nPublish to github.com and uncheck private repository so that other people will be able to see your repository.\nYou should now be able to see your new repository in your github.com profile, which means you should be able to see a copy of your blog files in the repository.\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder.\nAccess the blog from the url generated by the github pages settings page.\n\n\n\n\nWhenever you make changes to your blog project that you want to share online follow these steps:\n\nMake changes to your blog, like writing a new post, or editing an old one.\nRender the website in R-studio. What you see here should be what you will see later on Github.com\nOpen Github Desktop and Commit your changes, by writing brief commit title, and pressing commit.\nThen, use Github Desktop to Push your changes to github.com.\nWait half a minute or so, and you should see your new content appear on the website.\n\n\n\n\nI am planning to add a video overview of these steps soon. In the meantime, these instructions may be enough to get started with R studio cloud and github.com.\n\nSign up for a free account with posit cloud here https://posit.cloud/plans/free\nSign up for a free https://github.com account.\nLog in to Github, and search for this repository https://github.com/CrumpLab/quartoCourseBlog.\n\n\nClick the green “Use this template” Button\nThis will make a copy of the template in your github account, it will show up as one of your repositories\nGive your new repository a name\n\n\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder. You should now be able to view the blog from the url given by github pages.\nLog into Posit Cloud\nCreate a New Project, choose “New Project from Git Repository”\n\n\nenter the URL to the github repository you just made\n\n\nLoad the project, and edit/modify the files (see above for examples of creating new posts etc.)\nTo send your changed files back to github.com you need to do a few steps\n\n\nFrom the Git tab: stage your changes, commit your changes, and push your changes using the green up arrow.\nYou will also need to authenticate your git credentials, and allow Rstudio cloud to update your github repository\nIn the terminal run these two lines, but replace with your name and email\n\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\nWhen you “push” your changes back to Github using the green up arrow, you will be asked to enter a username and password. You can enter the username for your Github.com account, but your password won’t work. You need to set up a personal access token.\nGo to your github.com profile &gt; settings page. Scroll down, click on “&lt;&gt; Developer Settings”, on the left\nClick on personal access tokens, generate a new token, give it repo access. Save the text somewhere and use it as your github password when pushing from RStudio.\n\n\nAt this point you should be able to work on your blog in Rstudio Cloud, and push your changes to have them updated on github.com, which serves your blog online.\n\n\n\n\nSee this growing list of quarto resources for much, much more:\nhttps://github.com/mcanouil/awesome-quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Future urban analyst and planner\nHi! I am Tianxiao, from University of Pennsylvania Master of City Planning 24’. From my perspective, data-driven is the future of decision-making in the field of regional planning. Therefore, that’s my passionate of the application of data analyst and machine learning in the urban field."
  },
  {
    "objectID": "readme.html#quarto-documentation",
    "href": "readme.html#quarto-documentation",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Many questions about quarto can be answered from the quarto documentation located at: https://quarto.org.\nIf you are looking for something specific and don’t know where to find it on the website, use the search tool in the top right corner."
  },
  {
    "objectID": "readme.html#using-this-template",
    "href": "readme.html#using-this-template",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "To use this template you will need a Github.com account, and access to R and Rstudio.\nYou can get access by creating a Github.com account, and downloading the necessary open-source software to your machine.\nIt is also possible to use Rstudio in your web-browser, which does not require downloading any software. There are two sets of instructions, one for the downloading approach, and the other for using Rstudio cloud. Scroll to the bottom for instructions on using Rstudio cloud."
  },
  {
    "objectID": "readme.html#downloading-the-free-software",
    "href": "readme.html#downloading-the-free-software",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "In order to use this template you will need to install some free open-source software on your computer.\n\nSign up for a free account at https://github.com. This a website for sharing open-source software, but it can also be used to serve your blog as a website for free.\nDownload Github Desktop and install it on your machine. This should install the version control software git on your system, and you can use Github Desktop to easily push your blog from your local computer so that it can be viewed on Github.com.\nDownload R and install it on your machine. R is a programming language capable of many things, and it needs to be on your machine before you can run R Studio.\nDownload R Studio Desktop and install it on your machine. R Studio is called an “IDE” or integrated development environment, that you can use to write your blog with quarto."
  },
  {
    "objectID": "readme.html#make-a-quarto-blog-project",
    "href": "readme.html#make-a-quarto-blog-project",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Once you have the software installed, the next step is to create a quarto blog project in R studio. The collection of files in this template is a pre-made quarto blog project that you can modify for your own purposes. You can also make one yourself in Rstudio.\nAssuming you have downloaded this template, and you have installed the above software, then you need to open quartoCourseBlog.Rproj."
  },
  {
    "objectID": "readme.html#render-the-blog",
    "href": "readme.html#render-the-blog",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "To find out if everything is working, try rendering the blog. Go to the “Build” Tab and press “Render Website”.\n\nAfter the rendering is complete, you should be able to view your blog. It might show up in the viewer pane like this:\n\nAnd if you press the ‘window-with-an-arrow’ button, you can view the website in your default browser. Quarto websites automatically adjust for the size of the window, so it may appear differently in the viewer pane versus the browser."
  },
  {
    "objectID": "readme.html#make-a-new-blog-post",
    "href": "readme.html#make-a-new-blog-post",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "All of the blog posts are located in the posts folder.\n\nTo make a new post, copy an existing post and then modify it. For example, my posts folder currently contains one post, and it is inside the my_first_post folder.\n\nI can copy the folder and make a new one with the same contents from the Rstudio gear-box menu:\n\nI made a folder for a second post called Example_assignment.\n\nThese are the two files inside the folder. The .qmd file is a plain text file where you will write the blog post. This folder can also be used to store other assets you might put in the post, such as pictures.\n\n\n\nTo write a new post, open the .qmd file, edit the text, and then re-render the website. This is what the text in the .qmd file looked like when I copied it.\n\nThe text at the top between the “---” is called YAML, and provides meta-data for your document. This is where you can change the title, date, name, and add keywords if you want.\nThe rest of the document is for the main body of the post. For example, I changed the text to read:"
  },
  {
    "objectID": "readme.html#re-render-to-see-your-changes",
    "href": "readme.html#re-render-to-see-your-changes",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Render the website from the build tab again to see your new post."
  },
  {
    "objectID": "readme.html#share-your-blog-on-github.com",
    "href": "readme.html#share-your-blog-on-github.com",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "To share your blog online you will have to publish it on a server that can be accessed by other people on the internet. There are multiple ways to do this step, and I recommend using Github pages. You can view more in-depth instructions from quarto here https://quarto.org/docs/publishing/github-pages.html.\nHere are the steps:\n\nOpen Github Desktop\nGo to preferences and sign in to your Github.com account\n“Add” your blog project folder to Github Desktop\nThere should be an option for a commit message, write a note in there like “first commit”.\nPublish to github.com and uncheck private repository so that other people will be able to see your repository.\nYou should now be able to see your new repository in your github.com profile, which means you should be able to see a copy of your blog files in the repository.\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder.\nAccess the blog from the url generated by the github pages settings page."
  },
  {
    "objectID": "readme.html#pushing-new-posts-to-github.com",
    "href": "readme.html#pushing-new-posts-to-github.com",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "Whenever you make changes to your blog project that you want to share online follow these steps:\n\nMake changes to your blog, like writing a new post, or editing an old one.\nRender the website in R-studio. What you see here should be what you will see later on Github.com\nOpen Github Desktop and Commit your changes, by writing brief commit title, and pressing commit.\nThen, use Github Desktop to Push your changes to github.com.\nWait half a minute or so, and you should see your new content appear on the website."
  },
  {
    "objectID": "readme.html#posit-cloud-formerly-rstudio-cloud",
    "href": "readme.html#posit-cloud-formerly-rstudio-cloud",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "I am planning to add a video overview of these steps soon. In the meantime, these instructions may be enough to get started with R studio cloud and github.com.\n\nSign up for a free account with posit cloud here https://posit.cloud/plans/free\nSign up for a free https://github.com account.\nLog in to Github, and search for this repository https://github.com/CrumpLab/quartoCourseBlog.\n\n\nClick the green “Use this template” Button\nThis will make a copy of the template in your github account, it will show up as one of your repositories\nGive your new repository a name\n\n\nActivate Github pages for your repository (under repository settings), and serve the page from the “docs” folder. You should now be able to view the blog from the url given by github pages.\nLog into Posit Cloud\nCreate a New Project, choose “New Project from Git Repository”\n\n\nenter the URL to the github repository you just made\n\n\nLoad the project, and edit/modify the files (see above for examples of creating new posts etc.)\nTo send your changed files back to github.com you need to do a few steps\n\n\nFrom the Git tab: stage your changes, commit your changes, and push your changes using the green up arrow.\nYou will also need to authenticate your git credentials, and allow Rstudio cloud to update your github repository\nIn the terminal run these two lines, but replace with your name and email\n\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\nWhen you “push” your changes back to Github using the green up arrow, you will be asked to enter a username and password. You can enter the username for your Github.com account, but your password won’t work. You need to set up a personal access token.\nGo to your github.com profile &gt; settings page. Scroll down, click on “&lt;&gt; Developer Settings”, on the left\nClick on personal access tokens, generate a new token, give it repo access. Save the text somewhere and use it as your github password when pushing from RStudio.\n\n\nAt this point you should be able to work on your blog in Rstudio Cloud, and push your changes to have them updated on github.com, which serves your blog online."
  },
  {
    "objectID": "readme.html#more-quarto",
    "href": "readme.html#more-quarto",
    "title": "Using this quarto course blog template",
    "section": "",
    "text": "See this growing list of quarto resources for much, much more:\nhttps://github.com/mcanouil/awesome-quarto"
  },
  {
    "objectID": "posts/Camden/index.html",
    "href": "posts/Camden/index.html",
    "title": "Camden Gateway District Plan",
    "section": "",
    "text": "If want to see the complete version of our planning, please check the link and see the full planning book.\n\nIntroduction\nCamden, NJ, a historic and industrial urban center located directly across the Delaware River from Philadelphia, is in many ways a polemic example of the story of the post-industrial city. Once a key player in the most industrious region of the country, Camden was home to shipbuilding, canning, and many other manufacturing uses due to its proximity to markets, ports, heavy rail and related infrastructure. Like many similar Northeast and Midwest cities with an industrial and manufacturing past, the period of de-industrialization and urban renewal had drastically disparate impacts among Camden’s communities of color. Many of the challenges Camden still faces are attributable to the discrimminatory socio-economic doctrines and unjust political power imbalances that existed during these eras of urban development.”\n\n\n\nImplementation\nHighways are not inherently bad, but in the context of our study area, they consume a large percentage of area and are a barrier to local mobility. The I-676 and Admiral Wilson Blvd provide a greater regional mobility to largely non-Camden residents but at the cost of Camden residents. Symbolically, the highways memorialize the large-scale displacement of the 60s and the past half century of City turmoil. Camden residents have a lack of economic and social opportunities that are typically afforded to residents of other cities like those that use Camden’s highways. At the same time, Camden has a bright future. The recent wave of community-led neighborhood plans underline that Camden residents want to define Camden’s legacy. At the same time, the recent economic and transportation investments highlight that Camden is a city worth investing in. The issue is that this planning and investing are site and neighborhood specific. This siloed decision making is not addressing all of the factors at play. Although our plan is technically only for the Gateway District, we believe that this district can be a unifying force for Camden’s neighborhood planning and site investments. So even though we are creating a vision for the Gateway, our vision can affect the City as a whole.\n\n\n\nDevelopment Strategies\nUsing a area-based, scenario-based development and zoning method, our principle is to center residents and residential areas at the heart of the neighborhood, to create a counterforce against the dominant uses of transportation and office in Camden nowadays. Based on the new proposed land use, shown on the map on the right, there will be 110 plus acres of open space along the Cooper river and throughout the neighborhoods, 96 plus acres of developable land, 3373 new residential units, all of which will generate 33 million property and tax revenues."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "NJ Transit Delay Time Prediction",
    "section": "",
    "text": "Serving as the second largest commuter rail network in the United State, the NJ Transit spans New Jersey and the state to New York City. However, their delays are getting worse with more office workers returning, on the other hand, there are no interactive apps for commuters that can predict the immediate delays that may happen by chance or fixedly occur on their daily commute routes during the day. As such, they cannot foresee them instantly and mitigate accordingly.\nIn our project, we further investigate into the delay performance on NJ transit’s commuter rail routes and come up with some interactive & instant predictive strategies targeted at commuters within NJ Transit Commuter Rail Routes.\nAs such, Delay detective is designed with commuters in mind. The functions include the notifications of the estimated arrival time before scheduled arrival time, on-time & average-delay performance, as well as passenger feedback.\n\n\nCode\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(tigris)\nlibrary(viridis)\nlibrary(riem)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(RSocrata)\nlibrary(spdep)\nlibrary(caret)\nlibrary(ckanr)\nlibrary(FNN)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(ggcorrplot) # plot correlation plot\nlibrary(corrr)      # another way to plot correlation plot\nlibrary(kableExtra)\nlibrary(jtools)     # for regression model plots\nlibrary(ggstance) # to support jtools plots\nlibrary(ggpubr)    # plotting R^2 value on ggplot point scatter\nlibrary(broom.mixed) # needed for effects plots\nlibrary(vtable)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(purrr)\nlibrary(geosphere)\nlibrary(googlesheets4)\nlibrary(corrplot)\n\nplotTheme &lt;- theme(\n  plot.title =element_text(size=12),\n  plot.subtitle = element_text(size=8),\n  plot.caption = element_text(size = 6),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title.y = element_text(size = 10),\n  # Set the entire chart region to blank\n  panel.background=element_blank(),\n  plot.background=element_blank(),\n  #panel.border=element_rect(colour=\"#F0F0F0\"),\n  # Format the grid\n  panel.grid.major=element_line(colour=\"#D0D0D0\",size=.2),\n  axis.ticks=element_blank())\n\nmapTheme &lt;- theme(plot.title =element_text(size=10),\n                  plot.subtitle = element_text(size=8),\n                  plot.caption = element_text(size = 6),\n                  axis.line=element_blank(),\n                  axis.text.x=element_blank(),\n                  axis.text.y=element_blank(),\n                  axis.ticks=element_blank(),\n                  axis.title.x=element_blank(),\n                  axis.title.y=element_blank(),\n                  panel.background=element_blank(),\n                  panel.border=element_blank(),\n                  panel.grid.major=element_line(colour = 'transparent'),\n                  panel.grid.minor=element_blank(),\n                  legend.direction = \"vertical\", \n                  legend.position = \"right\",\n                  plot.margin = margin(1, 1, 1, 1, 'cm'),\n                  legend.key.height = unit(1, \"cm\"), legend.key.width = unit(0.2, \"cm\"))\n\nsource(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r\")\n\npalette6 &lt;- c(\"#264653\",\"#2a9d8f\",'#8AB17D',\"#e9c46a\",'#f4a261',\"#e76f51\")\npalette5 &lt;- c(\"#264653\",\"#2a9d8f\",\"#e9c46a\",'#f4a261',\"#e76f51\")\npalette4 &lt;- c(\"#264653\",\"#2a9d8f\",\"#e9c46a\",\"#e76f51\")\npalette2 &lt;- c(\"#264653\",\"#2a9d8f\")\n\n\n\n\nCode\n#geometry data\nline &lt;- st_read('https://services6.arcgis.com/M0t0HPE53pFK525U/arcgis/rest/services/NJTRANSIT_RAIL_LINES_1/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson')%&gt;%\n  mutate(LINE_NAME = ifelse(LINE_NAME == 'Bergen County Line','Bergen Co. Line ',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Montclair-Boonton Line','Montclair-Boonton',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'North Jersey Coast Line','No Jersey Coast',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Northeast Corridor','Northeast Corrdr',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Pascack Valley Line','Pascack Valley',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Princeton Dinky','Princeton Shuttle',LINE_NAME),\n         LINE_NAME = ifelse(LINE_NAME == 'Raritan Valley Line','Raritan Valley',LINE_NAME))%&gt;%\n  dplyr::select(LINE_NAME,geometry)\n\nstop &lt;- st_read(\"https://services6.arcgis.com/M0t0HPE53pFK525U/arcgis/rest/services/NJTransit_Rail_Stations/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\") %&gt;%\n  mutate(STATION_ID = ifelse(STATION_ID == 'Atlantic City', 'Atlantic City Rail Terminal', STATION_ID),\n         STATION_ID = ifelse((STATION_ID == 'Middletown')&(COUNTY == 'Orange, NY'), 'Middletown NY', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Princeton Jct.', 'Princeton Junction', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Secaucus Junction Upper Level', 'Secaucus Upper Lvl', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Anderson Street-Hackensack', 'Anderson Street', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Bay Street-Montclair', 'Bay Street', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Broadway', 'Broadway Fair Lawn', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Essex Street-Hackensack', 'Essex Street', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Glen Rock-Boro Hall', 'Glen Rock Boro Hall', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Glen Rock-Main', 'Glen Rock Main Line', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Hoboken Terminal', 'Hoboken', STATION_ID),\n         STATION_ID = ifelse((STATION_ID == 'Middletown')&(COUNTY == 'Monmouth'), 'Middletown NJ', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Montclair St Univ', 'Montclair State U', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Mountain View-Wayne', 'Mountain View', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Pennsauken Transit Center', 'Pennsauken', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Radburn', 'Radburn Fair Lawn', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Ramsey', 'Ramsey Main St', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Rte 17 Ramsey', 'Ramsey Route 17', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Secaucus Junction Lower Level', 'Secaucus Lower Lvl', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Teterboro-Williams Ave', 'Teterboro', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Watsessing', 'Watsessing Avenue', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Wayne Route 23 Transit Center', 'Wayne-Route 23', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == 'Wood-Ridge', 'Wood Ridge', STATION_ID),\n         STATION_ID = ifelse(STATION_ID == '30th Street Station', 'Philadelphia', STATION_ID),\n         line_intersct = str_count(RAIL_SERVICE, \",\") + 1)%&gt;%\n  dplyr::select(STATION_ID,LATITUDE,LONGITUDE,line_intersct)%&gt;%\n  st_drop_geometry()\n\nid &lt;- '1V_kl3QKOxrTlwA8UG7kdv_VpJdaojKMj'\ndelay_df &lt;- read.csv(sprintf(\"https://docs.google.com/uc?id=%s&export=download\", id))%&gt;%\n  filter(type == 'NJ Transit')%&gt;%\n  na.omit()\n\nmerged_dataset &lt;- merge(delay_df, stop, by.x = \"from\", by.y = \"STATION_ID\",all.x = TRUE)\nmerged_dataset &lt;- merge(merged_dataset, stop, by.x = \"to\", by.y = \"STATION_ID\",all.x = TRUE)\nmerged_dataset &lt;- merged_dataset%&gt;%\n  filter(to != 'Mount Arlington')%&gt;%\n  filter(from != 'Mount Arlington')%&gt;%\n  rename(from_lat = LATITUDE.x,\n         from_lon = LONGITUDE.x,\n         from_inter = line_intersct.x,\n         to_lat = LATITUDE.y,\n         to_lon = LONGITUDE.y,\n         to_inter = line_intersct.y\n         )%&gt;%\n  mutate(distance = distHaversine(cbind(from_lon, from_lat), cbind(to_lon, to_lat)),\n         interval60 = floor_date(ymd_hms(scheduled_time), unit = \"hour\"),\n         week = week(interval60),\n         dotw = wday(interval60, label=TRUE),\n         time_of_day = case_when(hour(interval60) &lt; 7 | hour(interval60) &gt; 19 ~ \"Overnight\",\n                                 hour(interval60) &gt;= 7 & hour(interval60) &lt; 10 ~ \"AM Rush\",\n                                 hour(interval60) &gt;= 10 & hour(interval60) &lt; 15 ~ \"Mid-Day\",\n                                 hour(interval60) &gt;= 15 & hour(interval60) &lt;= 19 ~ \"PM Rush\"),\n         weekend = ifelse(dotw %in% c(\"Sun\", \"Sat\"), \"Weekend\", \"Weekday\"))\n\n#wweather data\nweather.Panel &lt;- \n  riem_measures(station = \"EWR\", date_start = \"2019-10-01\", date_end = \"2019-11-02\") %&gt;%\n  dplyr::select(valid, tmpf, p01i, sknt)%&gt;%\n  replace(is.na(.), 0) %&gt;%\n    mutate(interval60 = ymd_h(substr(valid,1,13))) %&gt;%\n    mutate(week = week(interval60),\n           dotw = wday(interval60, label=TRUE)) %&gt;%\n    group_by(interval60) %&gt;%\n    summarize(Temperature = max(tmpf),\n              Precipitation = sum(p01i),\n              Wind_Speed = max(sknt)) %&gt;%\n    mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))\n\n# census data\nNJCensus &lt;- \n  get_acs(geography = \"county subdivision\", \n          variables = c(\"B01003_001\", \"B19013_001\", \n                        \"B02001_002\", \"B08013_001\",\n                        \"B08012_001\", \"B08301_001\", \n                        \"B08301_010\", \"B01002_001\"), \n          year = 2019, \n          state = \"NJ\", \n          geometry = TRUE, \n          output = \"wide\") %&gt;%\n  rename(Total_Pop =  B01003_001E,\n         Med_Inc = B19013_001E,\n         Med_Age = B01002_001E,\n         White_Pop = B02001_002E,\n         Travel_Time = B08013_001E,\n         Num_Commuters = B08012_001E,\n         Means_of_Transport = B08301_001E,\n         Total_Public_Trans = B08301_010E) %&gt;%\n  select(Total_Pop, Med_Inc, White_Pop, Travel_Time,\n         Means_of_Transport, Total_Public_Trans,\n         Med_Age,\n         GEOID, geometry) %&gt;%\n  mutate(Percent_White = White_Pop / Total_Pop,\n         Mean_Commute_Time = Travel_Time / Total_Public_Trans,\n         Percent_Taking_Public_Trans = Total_Public_Trans / Means_of_Transport)\n\nNJCensus_select &lt;- NJCensus%&gt;%\n  mutate(bigcity = ifelse(Total_Pop &gt;= 100000, 'big city','small city'))%&gt;%\n  select(geometry, Total_Pop,bigcity, GEOID)\n\nNJTracts &lt;- \n  NJCensus %&gt;%\n  as.data.frame() %&gt;%\n  distinct(GEOID, .keep_all = TRUE) %&gt;%\n  select(GEOID, geometry) %&gt;% \n  st_sf\n\ntrain_census &lt;- st_join(merged_dataset %&gt;% \n          filter(is.na(from_lon) == FALSE &\n                   is.na(from_lat) == FALSE &\n                   is.na(to_lat) == FALSE &\n                   is.na(to_lon) == FALSE) %&gt;%\n          st_as_sf(., coords = c(\"from_lon\", \"from_lat\"), crs = 4326),\n        NJTracts %&gt;%\n          st_transform(crs=4326),\n        join=st_intersects,\n              left = TRUE) %&gt;%\n  rename(From.Tract = GEOID) %&gt;%\n  mutate(from_lon = unlist(map(geometry, 1)),\n         from_lat = unlist(map(geometry, 2)))%&gt;%\n  as.data.frame() %&gt;%\n  select(-geometry)\n\ntrain_census &lt;- train_census %&gt;%\n  st_as_sf(., coords = c(\"to_lon\", \"to_lat\"), crs = 4326) %&gt;%\n  st_join(., NJTracts %&gt;%\n            st_transform(crs=4326),\n          join=st_intersects,\n          left = TRUE) %&gt;%\n  rename(To.Tract = GEOID)  %&gt;%\n  mutate(to_lon = unlist(map(geometry, 1)),\n         to_lat = unlist(map(geometry, 2)))%&gt;%\n  as.data.frame() %&gt;%\n  select(-geometry)\n\ntrain_dataset &lt;-train_census  %&gt;%\n  left_join(weather.Panel, by =\"interval60\")\n\nmerged_dataset &lt;- train_dataset  %&gt;%\n  left_join(NJCensus_select, by = c(\"From.Tract\" = \"GEOID\")) %&gt;%\n  left_join(NJCensus_select, by =c(\"To.Tract\"=\"GEOID\")) %&gt;%\n  select(-geometry.x,-geometry.y) %&gt;%\n  rename(From_Total_Pop = Total_Pop.x,\n         To_Total_Pop = Total_Pop.y,\n         From_city = bigcity.x,\n         To_city = bigcity.y)%&gt;%\n  mutate(From_Total_Pop = ifelse(from == \"Philadelphia\", 1579075, From_Total_Pop),\n         To_Total_Pop = ifelse(to == \"Philadelphia\", 1579075, To_Total_Pop),\n         From_Total_Pop = ifelse(from == \"Middletown NY\", 1631993, From_Total_Pop),\n         To_Total_Pop = ifelse(to == \"Middletown NY\", 1631993, To_Total_Pop),\n         From_city = ifelse(from == \"Philadelphia\", 'big city', From_city),\n         To_city = ifelse(to == \"Philadelphia\", 'big city', To_city),\n         From_city = ifelse(from == \"Middletown NY\", 'big city', From_city),\n         To_city = ifelse(to == \"Middletown NY\", 'big city', To_city))\n\nmedian_value_f &lt;- median(merged_dataset$From_Total_Pop, na.rm = TRUE)\nmerged_dataset$From_Total_Pop[is.na(merged_dataset$From_Total_Pop)] &lt;- median_value_f\nmedian_value_t &lt;- median(merged_dataset$To_Total_Pop, na.rm = TRUE)\nmerged_dataset$To_Total_Pop[is.na(merged_dataset$To_Total_Pop)] &lt;- median_value_t\n\nmerged_dataset &lt;- merged_dataset%&gt;%\n  mutate(From_city = ifelse(From_Total_Pop == 24784, 'big city', From_city),\n         To_city = ifelse(To_Total_Pop == 24784, 'big city', To_city))"
  },
  {
    "objectID": "posts/post-with-code/index.html#data-source",
    "href": "posts/post-with-code/index.html#data-source",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Data Source",
    "text": "Data Source\n*NJ Transit Delay Data — The dataset provides delay data for each month between NJ transit 2018-2019, and delay data for October 2019 was used in this project.\n*NJ Rail Station & Line Data — The dataset provides the geometry data of the line and station, and be used for further data visualization.\n*Weather Data — The dataset provides the weather data collected from the weather stations. And the dataset include the precipitation, wind speed and temperature data.\n*Census Data — The dataset is provided by Census Bureau and gives the social-ecnomic situation of city."
  },
  {
    "objectID": "posts/post-with-code/index.html#serial-autocorrelation---fixed-effects",
    "href": "posts/post-with-code/index.html#serial-autocorrelation---fixed-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Serial Autocorrelation - Fixed Effects",
    "text": "Serial Autocorrelation - Fixed Effects\nFrom the Temporal Series Analysis, We can find that Delay in NJ Transit has an obvious regularity in the temporal field. We were able to find that weekend delays had a longer average length than weekdays. And when we look at latency over a 24-hour period, we can see that latency reaches its maximum between 2:00 a.m. and 3:00 a.m., and overall latency stays on an upward trend from 4:00 a.m. onwards. And when we look at the relationship between stop sequence and delay time, we are able to see that the average latency time increases as the station sequence increases. We were able to find the highest latency in the PM Rush phase, followed by the overnight phase. When we wanted to explore the compounding of time, we were able to find that the PM Rush phase and the overnight phase had significantly higher latency times on weekends than on weekdays. And when we look at the pattern of average delay times for 24 hours in a day compared to weekdays and weekends, we find that they both maintain a similar pattern.\n\n\nCode\ndelay_time &lt;- merged_dataset %&gt;%\n  group_by(time_of_day)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_day &lt;- merged_dataset %&gt;%\n  group_by(dotw)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_week &lt;- merged_dataset %&gt;%\n  group_by(weekend)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_hour &lt;- merged_dataset %&gt;%\n  group_by(hour(interval60))%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(hour = 'hour(interval60)')\n\ndelay_sequence &lt;- merged_dataset %&gt;%\n  group_by(stop_sequence)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_time_week &lt;- merged_dataset %&gt;%\n  group_by(time_of_day,weekend)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_week_hour &lt;- merged_dataset %&gt;%\n  group_by(weekend,hour(interval60))%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(hour = 'hour(interval60)')\n\ngrid.arrange(ggplot(data = delay_day, aes(x = dotw, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in a week\", x = \"Day of The Week\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_week, aes(x = weekend, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = palette2) +\n  labs(title = \"Delay comparison in Weekend\", x = \"Weekend or Weekday\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_hour, aes(x = hour, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in 24 hours\", x = \"Hour in a day\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_sequence, aes(x = stop_sequence, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in each sequence\", x = \"Stop Sequence\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_time, aes(x = time_of_day, y = mean_delay)) +\n  geom_bar(stat = \"identity\",fill = \"#2a9d8f\") +\n  labs(title = \"Delay minutes in a week\", x = \"Day of The Week\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_week_hour, aes(x = hour, y = mean_delay, color = weekend)) +\n  geom_line() +\n  scale_color_manual(values = palette2) +\n  labs(title = \"The delay under week and time\", x = \"Hour\", y = \"Delay Minutes\") +\n  theme_minimal(),nrow=3)\n\n\n\n\n\nCode\nmerged_dataset%&gt;%\n  dplyr::select(interval60, from, delay_minutes) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value)) + \n    geom_line(size = 0.8,colour=\"#2a9d8f\")+\n      labs(title = \"Delay distribution in A Month\", subtitle = \"NJ, Oct, 2019\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal()"
  },
  {
    "objectID": "posts/post-with-code/index.html#spatial-autocorrelation---fixed-effects",
    "href": "posts/post-with-code/index.html#spatial-autocorrelation---fixed-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Spatial Autocorrelation - Fixed Effects",
    "text": "Spatial Autocorrelation - Fixed Effects\nFrom the charts and maps, we can observe that the delay time is highly associated with station and line. Specifically, the Atlantic city line has the most serious delay situation.Same with the stations along the Atlantic city Line. However, NJ Transit Commuter Routes as a whole have similar trends in delays. Also, in order to compare the difference in delay time, we compared the size of the city where the station is located with delay time based on census tracts under counties and defined cities with populations over 100,000 as big cities. It was found that delay time was an insignificant factor. In the meantime, from the map we can also conclude that the direction doesn’t seem to have a significant influence on delay in general.\n\n\nCode\ndelay_line &lt;- merged_dataset %&gt;%\n  group_by(line)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  arrange(., mean_delay)\n\ndelay_from &lt;- merged_dataset %&gt;%\n  group_by(from)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  arrange(., -mean_delay)%&gt;%\n  head(20)\n\nbig_from &lt;- merged_dataset %&gt;%\n  group_by(From_city)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'from')%&gt;%\n  rename(city_type = From_city)\n\nbig_to &lt;- merged_dataset %&gt;%\n  group_by(To_city)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'to')%&gt;%\n  rename(city_type = To_city)\n\ngrid.arrange(ggplot(data = delay_line, aes(x = line, y = mean_delay, fill = mean_delay)) +\n  geom_col(position = \"dodge\")+\n  labs(title = \"Delay minutes comparison in lines\", x = \"line\", y = \"Mean Delay\") + \n  scale_fill_gradient(low = \"#2a9d8f\", high = \"#264653\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 15, hjust = 1)) ,\n  ggplot(data = rbind(big_from,big_to), aes(x = status , y = mean_delay, fill = city_type)) +\n  geom_col(position = \"dodge\")+\n  labs(title = \"Delay minutes comparison in big and small city\", x = \"City Type\", y = \"Mean Delay\") + \n  scale_fill_manual(values = palette2)+\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 0 , hjust = 1)),\n  ggplot(data = delay_from, aes(x = from, y = mean_delay, fill = mean_delay)) +\n  geom_col(position = \"dodge\")+\n  labs(title = \"Delay minutes comparison in lines\", x = \"line\", y = \"Mean Delay\") + \n  scale_fill_gradient(low = \"#2a9d8f\", high = \"#264653\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 30, hjust = 1)))\n\n\n\n\n\n\n\nCode\nmap_from &lt;- merged_dataset %&gt;%\n  group_by(from)%&gt;%\n  summarize(mean_delay = mean(delay_minutes)) %&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Orientation')%&gt;%\n  rename(station = from)\n\nmap_to &lt;- merged_dataset %&gt;%\n  group_by(to)%&gt;%\n  summarize(mean_delay = mean(delay_minutes)) %&gt;%\n  left_join(stop,by=c('to'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Destination')%&gt;%\n  rename(station = to)\n\nggplot() +\n  geom_sf(data = NJTracts, color = 'grey') + \n  geom_sf(data = rbind(map_from,map_to), aes(size = mean_delay,color = mean_delay), alpha = 0.5) +\n  scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\")+\n  scale_size_continuous(name = \"Delay Minutes\") +\n  coord_sf()+\n  labs(title=\"Delayed Time in Station, October, 2019\")+\n  facet_grid(~status)+\n  mapTheme()+\n  theme_minimal()"
  },
  {
    "objectID": "posts/post-with-code/index.html#operation-effects",
    "href": "posts/post-with-code/index.html#operation-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Operation Effects",
    "text": "Operation Effects\nIn addition to the influence of the temporal and spatial dimensions on the delay time, we also wanted to explore the influence of some external factors as well as the operational factors of the train system on the delay.Looking at the weather conditions in October, the temperatures showed a fluctuating downward trend, while at the precipitation level several large precipitation events were found in the second half of October.\n\n\nCode\ngrid.arrange(\n  ggplot(weather.Panel, aes(interval60,Precipitation)) + geom_line(color=\"#2a9d8f\") + \n  labs(title=\"Percipitation\", x=\"Hour\", y=\"Perecipitation\") + theme_minimal(),\n  ggplot(weather.Panel, aes(interval60,Wind_Speed)) + geom_line(color=\"#2a9d8f\") + \n    labs(title=\"Wind Speed\", x=\"Hour\", y=\"Wind Speed\") + theme_minimal(),\n  ggplot(weather.Panel, aes(interval60,Temperature)) + geom_line(color=\"#2a9d8f\") + \n    labs(title=\"Temperature\", x=\"Hour\", y=\"Temperature\") + theme_minimal(),\n  top=\"Weather Data - NJ EWR - Nov, 2019\")\n\n\n\n\n\nFor the number of intersections, overall the delay time decreases as more lines pass through the station, while the direction of the line has no significant effect on the intersection. When we focus on the effect of weather on the delay, we can find that rainy weather will have higher delay time, and as the weather rises the delay time will show a decreasing trend. This result may be explained by the fact that trains travel at lower speeds in rainy weather and that trains take more time to start up in low temperatures. The distance between stations does not have a significant effect on the delay time.\n\n\nCode\ndelay_distance &lt;- merged_dataset %&gt;%\n  group_by(distance)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_intersct &lt;- rbind(\n  merged_dataset %&gt;%\n  group_by(from_inter)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'from')%&gt;%\n  rename(inter = from_inter),\n  merged_dataset %&gt;%\n  group_by(to_inter)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  mutate(status = 'to')%&gt;%\n  rename(inter = to_inter))\n\ndelay_rain_week &lt;- merged_dataset %&gt;%\n  mutate(rain = ifelse(Precipitation == 0,'NoRain','Rain'))%&gt;%\n  group_by(rain)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ndelay_temp &lt;- merged_dataset %&gt;%\n  group_by(Temperature)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n# just calculate the intersecation delay\ngrid.arrange(\n  ggplot(data = delay_intersct, aes(x = inter, y = mean_delay,fill=status)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = palette2) +\n  labs(title = \"Delay minutes in each intersection\", x = \"Num of Intersection\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_rain_week, aes(x = rain, y = mean_delay,fill=rain)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = palette2) +\n  labs(title = \"Delay minutes with Rain\", x = \"Rain\", y = \"Mean Delay\") +\n  theme_minimal(),\n  ggplot(data = delay_distance, aes(x = distance, y = mean_delay)) +\n  geom_line(color = \"#2a9d8f\") +\n  labs(title = \"The relationship between delay and distance\", x = \"Distance\", y = \"Value\") +\n  geom_smooth(method = \"lm\", se = TRUE)+\n  theme_minimal(),\n  ggplot(data = delay_temp, aes(x = Temperature, y = mean_delay)) +\n  geom_line(color = \"#2a9d8f\") +\n  labs(title = \"The relationship between delay and temperature\", x = \"Temperature\", y = \"Value\") +\n  geom_smooth(method = \"lm\", se = TRUE)+\n  theme_minimal())"
  },
  {
    "objectID": "posts/post-with-code/index.html#space-time-autocorrelation",
    "href": "posts/post-with-code/index.html#space-time-autocorrelation",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Space-time Autocorrelation",
    "text": "Space-time Autocorrelation\nAfter the spatial and temporal analysis, We would like to explore more deeply the autocorrelation of space-time with delay time. Looking at the distribution of the average delay time in terms of time and site, we were able to find a delay effect of the average delay time on the site. Therefore, we can draw the inference that the front site on a route will have a lagging effect on the delay time of the back site. And overall, the commuting area around New York has much smaller and more consistent delays, relative to the Atlantic City line.\n\n\nCode\ndelay_stop_time &lt;- merged_dataset %&gt;%\n  group_by(from,to,hour(interval60))%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(hour = 'hour(interval60)')%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = delay_stop_time, aes(size = line_intersct,color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay For One Day in Oct, 2019\",\n         subtitle = \"Hours in a day: {current_frame}\") +\n    transition_manual(hour)+ mapTheme()+theme_minimal()\n\n\n\n\n\nAnd when we want to consider the effect of intersections on delay times, we are able to find that the number of intersections on weekdays does not have a large impact on the degree of delay, except for the New York station which has a smaller delay time. This may be due to the fact that New York station has more passenger throughput resulting in a tighter departure frequency. On weekends, we are able to find that stations that are intersections have lower average delay times.\n\n\nCode\ndelay_intersct_week_f &lt;-merged_dataset %&gt;%\n  group_by(from_inter,weekend)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))\n\ninter_stop &lt;- stop%&gt;%\n  right_join(delay_intersct_week_f,by=c('line_intersct' = 'from_inter'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = inter_stop, aes(size = line_intersct,color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay Intersection Comparison, 2019\") +\n    facet_wrap(~weekend)+ mapTheme()+theme_minimal()\n\n\n\n\n\nAnd in addition to exploring the phenomenon of temporal pattern in a day, we also hope to discover temporal patterns over long periods of time. In a weekly dimension, we find that weekday and weekend delays are relatively stable, and the direction of the line does not have a significant spatial effect on the delay. Similarly, when we go to look at the spatial distribution of the average delay time for each week in a month, the variation in delay time is small and very stable.\n\n\nCode\ndelay_to_time &lt;- merged_dataset %&gt;%\n  group_by(to,dotw)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(Day = dotw)%&gt;%\n  left_join(stop,by=c('to'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Orientation')%&gt;%\n  rename(station = to)\n\ndelay_from_time &lt;- merged_dataset %&gt;%\n  group_by(from,dotw)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  rename(Day = dotw)%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Destination')%&gt;%\n  rename(station = from)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = rbind(delay_from_time,delay_to_time), aes(size =mean_delay, color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay For One Week in Oct, 2019\",\n         subtitle = \"Day in a week: {current_frame}\") +\n    facet_wrap(~status)+\n    transition_manual(Day)+ mapTheme()+theme_minimal()\n\n\n\n\n\n\n\nCode\ndelay_to_time &lt;- merged_dataset %&gt;%\n  group_by(to,week)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  left_join(stop,by=c('to'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Orientation')%&gt;%\n  rename(station = to)\n\ndelay_from_time &lt;- merged_dataset %&gt;%\n  group_by(from,week)%&gt;%\n  summarize(mean_delay = mean(delay_minutes))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(status = 'Destination')%&gt;%\n  rename(station = from)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = rbind(delay_from_time,delay_to_time), aes(size =mean_delay, color = mean_delay)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"Station Delay For One Month in Oct, 2019\",\n         subtitle = \"Week in a month: {current_frame}\") +\n    facet_wrap(~status)+\n    transition_manual(week)+ mapTheme()+theme_minimal()\n\n\n\n\n\nOverall, the spatial-temporal distribution of delay times is consistent with the findings of the temporal and spatial analyses conducted in the previous At the same time, we found a spatial manifestation of the lag effect of delay times at the site level. This provides us with a choice of new independent variables for the subsequent construction of the predictive model."
  },
  {
    "objectID": "posts/post-with-code/index.html#lag-effects",
    "href": "posts/post-with-code/index.html#lag-effects",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Lag Effects",
    "text": "Lag Effects\nBased on the exploratory analysis described above, we found lag effects on delay times at the spatial and temporal levels, so we created temporal lag and spatial lag variables to make predictions in our real-world model. In a practical sense, the temporal lag can be interpreted as the effect of the amount of delay that occurs before a certain time at a passenger’s stop on the delay of the schedule he is traveling on. The spatial lag can be interpreted as the effect of the delay of the stop of the passenger’s trip before a certain time at his stop on the delay of the stop at which he is traveling.\nBecause for the USE CASE we want to realize the delay prediction of passengers in the period before boarding, we choose a relatively small time lag of 15 minutes as the unit, so that we can provide more time lag variables to be added in the model the closer the boarding time is to the boarding time.\n\nTime Lag\nIn terms of the correlation of the time-lagged variables with respect to the delay time, their correlation with the delay time decreases as the lag time increases. However, in general, the correlation of delay time in the same site is overall low.\n\n\nCode\nmerged_dataset &lt;- merged_dataset %&gt;%\n   mutate(interval15 = floor_date(ymd_hms(scheduled_time), unit = \"15 mins\"))\n\nmerged_dataset &lt;- \n  merged_dataset %&gt;% \n  arrange(from_id, interval15) %&gt;% \n  mutate(lag15min = dplyr::lag(delay_minutes,1),\n         lag30min = dplyr::lag(delay_minutes,2),\n         lag45min = dplyr::lag(delay_minutes,3),\n         lag1h = dplyr::lag(delay_minutes,4),\n         lag1h15min = dplyr::lag(delay_minutes,5),\n         lag1h30min = dplyr::lag(delay_minutes,6),\n         lag1h45min = dplyr::lag(delay_minutes,7),\n         lag2h = dplyr::lag(delay_minutes,8),\n         lag2h15min = dplyr::lag(delay_minutes,9),\n         lag2h30min = dplyr::lag(delay_minutes,10),\n         lag2h45min = dplyr::lag(delay_minutes,11),\n         lag3h = dplyr::lag(delay_minutes,12))\n\nas.data.frame(merged_dataset) %&gt;%\n    group_by(interval15) %&gt;% \n    summarise_at(vars(starts_with(\"lag\"), \"delay_minutes\"), mean, na.rm = TRUE) %&gt;%\n    gather(Variable, Value, -interval15, -delay_minutes) %&gt;%\n    mutate(Variable = factor(Variable, levels=c(\"lag15min\",\"lag30min\",\"lag45min\",\"lag1h\",                       \"lag1h15min\",\"lag1h30min\",\"lag1h45min\",\"lag2h\",\"lag2h15min\",\"lag2h30min\",\"lag2h45min\",\"lag3h\")))%&gt;%\n    group_by(Variable) %&gt;%  \n    summarize(correlation = round(cor(Value, delay_minutes),2))\n\n\n# A tibble: 12 × 2\n   Variable   correlation\n   &lt;fct&gt;            &lt;dbl&gt;\n 1 lag15min          0.59\n 2 lag30min          0.44\n 3 lag45min          0.4 \n 4 lag1h             0.4 \n 5 lag1h15min        0.35\n 6 lag1h30min        0.33\n 7 lag1h45min        0.23\n 8 lag2h             0.15\n 9 lag2h15min        0.18\n10 lag2h30min        0.22\n11 lag2h45min        0.23\n12 lag3h             0.23\n\n\n\n\nCode\nmerged_dataset_station &lt;- \n  merged_dataset %&gt;% \n  arrange(train_id, interval15,stop_sequence) %&gt;% \n  mutate(lagsstation = if_else(stop_sequence == 1, 0, lag(delay_minutes, 1)),\n         lags2station = if_else(stop_sequence == 1 | stop_sequence == 2, 0, lag(delay_minutes, 2)),\n         lags3station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3, 0, lag(delay_minutes, 3)),\n         lags4station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4, 0, lag(delay_minutes, 4)),\n         lags5station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5, 0, lag(delay_minutes, 5)),\n         lags6station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5| stop_sequence == 6, 0, lag(delay_minutes, 6)),\n         lags7station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5 | stop_sequence ==6 | stop_sequence == 7, 0, lag(delay_minutes, 7)),\n         lags8station = if_else(stop_sequence == 1 | stop_sequence == 2| stop_sequence == 3| stop_sequence == 4| stop_sequence == 5| stop_sequence == 6| stop_sequence == 7| stop_sequence == 8 , 0, lag(delay_minutes, 8))\n         )\n\nselected_columns &lt;- merged_dataset_station[, c(\"delay_minutes\", \"lag15min\",\"lag30min\",\"lag45min\",\"lag1h\",                       \"lag1h15min\",\"lag1h30min\",\"lag1h45min\",\"lag2h\",\"lag2h15min\",\"lag2h30min\",\"lag2h45min\",\"lag3h\",\"week\")]\ncor_delay_all_time &lt;- cor(selected_columns, use = \"complete.obs\")[\"delay_minutes\", -1]%&gt;%\n  as.data.frame()%&gt;%rename(cor_score = '.')\n\nplotData.lag_time &lt;-\n  filter(as.data.frame(merged_dataset_station), week == 43) %&gt;%\n  dplyr::select(lag15min,lag30min,lag45min,lag1h,                       lag1h15min,lag1h30min,lag1h45min,lag2h,lag2h15min,lag2h30min,lag2h45min,lag3h, delay_minutes) %&gt;%\n  gather(Variable, Value, -delay_minutes) %&gt;%\n  mutate(Variable = fct_relevel(Variable,\"lag15min\",\"lag30min\",\"lag45min\",\"lag1h\",                       \"lag1h15min\",\"lag1h30min\",\"lag1h45min\",\"lag2h\",\"lag2h15min\",\"lag2h30min\",\"lag2h45min\",\"lag3h\"))\n\ncorrelation.lag_time &lt;-\n  group_by(plotData.lag_time, Variable) %&gt;%\n    summarize(correlation = round(cor(Value, delay_minutes, use = \"complete.obs\"), 2))\n\nggplot(plotData.lag_time, aes(Value,delay_minutes))+\n  geom_point(size = 0.1) +\n  geom_text(data = correlation.lag_time, aes(label = paste(\"r =\", round(correlation, 2))),\n            x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +\n  geom_smooth(method = 'lm', se=FALSE, color =\"#2a9d8f\")+\n  facet_wrap(~Variable, ncol = 4, scales = 'free') +\n  labs(title = \"Delay minute from previous time as a function of spatial lags\",\n       subtitle = \"One week in Oct, 2019\") +\n  mapTheme()+theme_minimal()\n\n\n\n\n\n\n\nStation Lag\nAs for the correlation of spatial lags, we were able to find that the overall correlation of the lagged variables of the sites for the delay time decreases as the number of lagged sites increases. However, overall, the correlation of spatially lagged variables to delay time is high.\n\n\n\n\n\nOverall, at the level of lagged impacts, spatial lagged impacts possess a high correlation for delay times, while temporal lagged impacts do not have a high correlation for delay times."
  },
  {
    "objectID": "posts/post-with-code/index.html#correlation-matrix",
    "href": "posts/post-with-code/index.html#correlation-matrix",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\nIn summary, we found the influence of factors on delay time at the spatial, temporal, operational, and lag effect levels, and we constructed a correlation matrix to visualize the correlation between the factors and the delay time for better screening of effective independent variables for subsequent data modeling. From the matrix, we can find that the spatial lag and time lag variables have more obvious effects on delay time.\n\n\nCode\nmerged_dataset_station &lt;- merged_dataset_station%&gt;%\n  mutate(isbig_from = ifelse(From_city == 'big city',1,0),\n         isbig_to = ifelse(To_city == 'big city',1,0),\n         isweekday = ifelse(weekend == 'Weekday',1,0))\n\ncor_matrix &lt;- merged_dataset_station %&gt;%\n  dplyr::select(delay_minutes,stop_sequence,distance,Temperature,Precipitation,Wind_Speed,from_inter,to_inter,isweekday,lag15min,lag30min,lag45min,lag1h,                       lag1h15min,lag1h30min,lag1h45min,lag2h,lag2h15min,lag2h30min,lag2h45min,lag3h,lagsstation,lags2station,lags3station, lags4station, lags5station, lags6station, lags7station, lags8station)\n\ncor_matrix &lt;- cor(cor_matrix, method = \"pearson\", use = \"complete.obs\")\n\ncorrplot(cor_matrix, method = 'shade', order = 'AOE', diag = FALSE,tl.col = 'black')"
  },
  {
    "objectID": "posts/post-with-code/index.html#model-evaluation",
    "href": "posts/post-with-code/index.html#model-evaluation",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nFrom the comparison of predicted and actual delay times, we can see that the closer the model is to the predicted time has a better model predictive ability. Whereas the model has worse predictive ability for very large and very small values.\n\n\nCode\ndelay.Test_5 &lt;- delay.Test%&gt;%\n  mutate(pre_5 = predict(reg.30, newdata = delay.Test),\n         abosulte_error = abs(pre_5 - delay_minutes),\n         MAE = mean(abosulte_error,na.rm = TRUE),\n         sd_AE = sd(abosulte_error,na.rm = TRUE),\n         per_error = (pre_5 - delay_minutes)/delay_minutes,\n         per_error = ifelse(per_error == Inf,0,per_error),\n         per_error = ifelse(per_error == -Inf,0,per_error))%&gt;%\n  rename(mod_30 = pre_5)\n\ndelay.Test_6 &lt;- delay.Test%&gt;%\n  mutate(pre_6 = predict(reg.60, newdata = delay.Test),\n         abosulte_error = abs(pre_6 - delay_minutes),\n         MAE = mean(abosulte_error,na.rm = TRUE),\n         sd_AE = sd(abosulte_error,na.rm = TRUE),\n         per_error = (pre_6 - delay_minutes)/delay_minutes,\n         per_error = ifelse(per_error == Inf,0,per_error),\n         per_error = ifelse(per_error == -Inf,0,per_error))%&gt;%\n  rename(mod_60 = pre_6)\n\ndelay.Test_7 &lt;- delay.Test%&gt;%\n  mutate(pre_7 = predict(reg.90, newdata = delay.Test),\n         abosulte_error = abs(pre_7 - delay_minutes),\n         MAE = mean(abosulte_error,na.rm = TRUE),\n         sd_AE = sd(abosulte_error,na.rm = TRUE),\n         per_error = (pre_7 - delay_minutes)/delay_minutes,\n         per_error = ifelse(per_error == Inf,0,per_error),\n         per_error = ifelse(per_error == -Inf,0,per_error))%&gt;%\n  rename(mod_90 = pre_7)\n\ngrid.arrange(\n  delay.Test_5%&gt;%\n  dplyr::select(interval60, from, delay_minutes, mod_30) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n    geom_line(size = 0.9)+\n      labs(title = \"Predicted/Observed delay time series\", subtitle = \"30 Minustes-Pre Predict\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal(),\n  delay.Test_6%&gt;%\n  dplyr::select(interval60, from, delay_minutes, mod_60) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n    geom_line(size = 0.9)+\n      labs(title = \"Predicted/Observed delay time series\", subtitle = \"60 Minustes-Pre Predict\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal(),\n  delay.Test_7%&gt;%\n  dplyr::select(interval60, from, delay_minutes, mod_90) %&gt;%\n  gather(Variable, Value, -interval60, -from) %&gt;%\n    group_by(Variable, interval60) %&gt;%\n    summarize(Value = mean(Value))%&gt;%\n    ggplot(aes(interval60, Value, colour=Variable)) + \n    geom_line(size = 0.9)+\n      labs(title = \"Predicted/Observed delay time series\", subtitle = \"90 Minustes-Pre Predict\",  x = \"Day\", y= \"Mean Delay\") +\n     theme_minimal(),\n  ncol=1)\n\n\n\n\n\nAnd when we focus on the spatial generalizability of the model’s performance, we are um able to find that all three models show a more even MAE, except for the line from New York to the north. Other than that, we are able to find that the models have larger model errors for stations in and around New York. This phenomenon may stem from the fact that in New York there is a higher frequency of trips and the same station may be affected by the delay time of trips on different lines.\n\n\nCode\ntemp &lt;- delay.Test_5 %&gt;% \n  group_by(from)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(mod = '30m-Predict')\n\ntemp2 &lt;- delay.Test_6 %&gt;% \n  group_by(from)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(mod = '60m-Predict')\n\ntemp3 &lt;- delay.Test_7 %&gt;% \n  group_by(from)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)%&gt;%\n  mutate(mod = '90m-Predict')\n\ntemp4 &lt;- rbind(temp,temp2)\ntemp4 &lt;- rbind(temp4,temp3)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = temp4, aes(color = mean_ae,size = line_intersct)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"MAE Spatial Comparison in 3 Models\") +\n    facet_wrap(~mod)+\n    mapTheme()+\n  theme_minimal()\n\n\n\n\n\nAnd in addition to model error assessment in space, we would also like to see if the model performs more universally across time. Overall, the models perform better on weekdays, both in terms of the magnitude of the errors and the stability of the errors. Moreover, both the error of the model and the s d of the error increase with the length of the model prediction. In terms of the comparison between weekdays and weekends, the three models have a smaller error boost on weekdays with the increase of time. In terms of the distribution of errors, the models on weekdays show a generally smaller standard divination. Therefore, we can conclude that the models on weekdays can have a more stable error range.\n\n\nCode\ntemp2 &lt;- delay.Test_5 %&gt;% \n  group_by(weekend)%&gt;%\n  summarise(MAE = mean(abosulte_error,na.rm = TRUE),\n            sd_AE = sd(abosulte_error,na.rm = TRUE))%&gt;%\n  mutate(mod = '30m')\n\ntemp &lt;- delay.Test_6 %&gt;% \n  group_by(weekend)%&gt;%\n  summarise(MAE = mean(abosulte_error,na.rm = TRUE),\n            sd_AE = sd(abosulte_error,na.rm = TRUE))%&gt;%\n  mutate(mod = '60m')\n\ntemp3 &lt;- delay.Test_7 %&gt;% \n  group_by(weekend)%&gt;%\n  summarise(MAE = mean(abosulte_error,na.rm = TRUE),\n            sd_AE = sd(abosulte_error,na.rm = TRUE))%&gt;%\n  mutate(mod = '90m')\n\ntemp4 &lt;- rbind(temp2,temp)\ntemp4 &lt;- rbind(temp4,temp3)\n\ngrid.arrange(\n  ggplot(temp4, aes(x=mod, y=MAE, colour=mod)) +\n    geom_point() +\n    facet_wrap(~weekend)+\n      labs(title = \"MAE Temporal Comparison\",x = \"Model\", y= \"MAE\") +\n     theme_minimal(),\n  ggplot(temp4, aes(x=mod, y=sd_AE, colour=mod)) +\n    geom_point() +\n    facet_wrap(~weekend)+\n      labs(title = \"SD of MAE Temporal Comparison\", x = \"Model\", y= \"SD_MAE\") +\n     theme_minimal()\n)\n\n\n\n\n\nWhen we look at the spatial distribution of this modeled performance difference between weekdays and weekends, we are able to find that the line north from New York possesses a smaller weekend and weekday error difference. The spatial difference between weekdays and weekends is primarily seen for lines heading south from New York, with stations on these lines having higher model errors on weekends.The reason this error exists may stem from the fact that on weekends more passengers from cities south of New York (e.g., Philadelphia and Jersey City) commute between New York and their locations for recreational or other purposes, and thus the same frequency in the face of significantly higher traffic may create a greater likelihood of delays.\n\n\nCode\ntemp &lt;- delay.Test_6 %&gt;% \n  group_by(from,weekend)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(stop,by=c('from'='STATION_ID'))%&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = temp, aes(color = mean_ae)) +\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"D\") +\n    labs(title = \"MAE Comaprison in Weekend\",subtitle = '60mins Model') +\n    facet_wrap(~weekend) + mapTheme()+\n  theme_minimal()\n\n\n\n\n\nn turn, the model’s performance for the line shows more similar characteristics at the spatial level. All of the routes exhibit larger errors as they are pushed farther back in time. Specifically, the lines from New York northward have smaller errors and are less affected by the different time models than the other lines. The Northeast Corridor, on the other hand, has the worst performance of the models that are closest in time. However, as we push farther back in time, the modeled errors show a similar pattern for the other lines.\n\n\nCode\ntemp &lt;- delay.Test_6 %&gt;% \n  group_by(line)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(line,by=c('line'='LINE_NAME'))%&gt;%\n  mutate(mod = '60min')\n\ntemp2 &lt;- delay.Test_5 %&gt;% \n  group_by(line)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(line,by=c('line'='LINE_NAME'))%&gt;%\n  mutate(mod = '30min')\n\ntemp3 &lt;- delay.Test_7 %&gt;% \n  group_by(line)%&gt;%\n  summarise(mean_ae = mean(abosulte_error),\n            mean_pe = mean(per_error))%&gt;%\n  left_join(line,by=c('line'='LINE_NAME'))%&gt;%\n  mutate(mod = '90min')\n\ntemp4 &lt;- rbind(temp,temp2)\ntemp4 &lt;- rbind(temp4,temp3)\n\nggplot() +\n    geom_sf(data = NJTracts, color = 'grey') + \n    geom_sf(data = temp4,aes(color = mean_ae, geometry = geometry))+\n    facet_wrap(~mod)+\n    scale_colour_viridis(direction = -1,discrete = FALSE, option = \"A\") +\n    labs(title = \"Model MAE Comparison in Line\",subtitle = '30 mins & 60 mins model') + mapTheme()+\n  theme_minimal()\n\n\n\n\n\nIn the case of over- or under-forecasting, we find that all three models exhibit under-forecasting, and that the degree of under-forecasting increases as we move farther back in the model’s time. Therefore, in the actual model prediction, we can increase the model noise according to the model to increase the accuracy of the model prediction.\n\n\nCode\ntemp &lt;- delay.Test_5%&gt;%\n  dplyr::select(delay_minutes,mod_30,weekend)%&gt;%\n  mutate(mod = '30min')%&gt;%\n  rename(pre = mod_30)\ntemp2 &lt;- delay.Test_6%&gt;%\n  dplyr::select(delay_minutes,mod_60,weekend)%&gt;%\n  mutate(mod = '60min')%&gt;%\n  rename(pre = mod_60)\ntemp3 &lt;- delay.Test_7%&gt;%\n  dplyr::select(delay_minutes,mod_90,weekend)%&gt;%\n  mutate(mod = '90min')%&gt;%\n  rename(pre = mod_90)\n\ntemp4 &lt;- rbind(temp,temp2)\ntemp4 &lt;- rbind(temp4,temp3)\n\nggplot()+\n  geom_point(data = temp4,aes(x= delay_minutes, y = pre),color = \"#2a9d8f\")+\n    geom_smooth(data = temp4,aes(x= delay_minutes, y= pre), method = \"lm\", se = FALSE, color = '#f4a261')+\n    geom_abline(slope = 1, intercept = 0)+\n  facet_grid(mod~weekend)+\n  labs(title=\"Observed vs Predicted\",\n       subtitle = 'model and weekend camparison',\n       x=\"Observed delay minutes\", \n       y=\"Predicted delay minutes\")+\n  plotTheme()+\n  theme_minimal()"
  },
  {
    "objectID": "posts/post-with-code/index.html#model-generalizability-evaluation",
    "href": "posts/post-with-code/index.html#model-generalizability-evaluation",
    "title": "NJ Transit Delay Time Prediction",
    "section": "Model Generalizability Evaluation",
    "text": "Model Generalizability Evaluation\nIn order to better test the model’s ability to perform in real-world scenarios, i.e., on new datasets, we evaluate the model’s ability using cross-validation.\n\n\nCode\nfitControl &lt;- trainControl(method = \"cv\", number = 20)\nset.seed(825)\n\nreg.cv.30 &lt;- \n  train(delay_minutes ~ from + to + hour + weekend + Temperature + Precipitation + Wind_Speed + lag45min  + lag1h + lag1h15min + lag1h30min + lag1h45min + lag2h + lag2h15min + lag2h30min + lag2h45min + lag3h + lags3station+ lags4station+ lags5station+lags6station+ stop_sequence + line + to_inter + from_inter, merged_dataset_model, \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\nreg.cv.60 &lt;- \n  train(delay_minutes ~ from + to + hour + weekend + Temperature + Precipitation + Wind_Speed+ lag1h + lag1h15min + lag1h30min + lag1h45min + lag2h + lag2h15min + lag2h30min + lag2h45min + lag3h + lags5station + lags6station + stop_sequence + line + to_inter + from_inter, merged_dataset_model, \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\nreg.cv.90 &lt;- \n  train(delay_minutes ~ from + to + hour + weekend + Temperature + Precipitation + Wind_Speed + lag1h30min + lag1h45min + lag2h + lag2h15min + lag2h30min + lag2h45min + lag3h+ lags6station+ stop_sequence + line + to_inter + from_inter, merged_dataset_model, \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\n\nLooking at the performance of the three models, we can see that the errors of the models show a gradual increase as time is pushed farther away. Regarding the stability of the model error, we can find that the 30-minute model has better stability. And as the time of the model is pushed farther, we can find that the R-square of the model shows a decreasing trend, which indicates that the credibility of the model is also decreasing with the increase of time.\n\n\nCode\ngrid.arrange(\ndplyr::select(reg.cv.30$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(reg.cv.30$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 5)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics-30mins Model\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")+\n  theme_minimal(),\ndplyr::select(reg.cv.60$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(reg.cv.60$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 5)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics-60mins Model\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")+\n  theme_minimal(),\ndplyr::select(reg.cv.90$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(reg.cv.90$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 5)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics-90mins Model\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")+\n  theme_minimal(),nrow=3)\n\n\n\n\n\n\n\nCode\ncombined_summary &lt;- bind_rows(\n  reg.cv.30$resample %&gt;%\n    summarise(Model = \"30 min Model\",\n              MAE = mean(.[,3]),\n              sd = sd(.[,3])),\n  reg.cv.60$resample %&gt;%\n    summarise(Model = \"60 min Model\",\n              MAE = mean(.[,3]),\n              sd = sd(.[,3])),\n  reg.cv.90$resample %&gt;%\n    summarise(Model = \"90 min Model\",\n              MAE = mean(.[,3]),\n              sd = sd(.[,3]))\n)\n\ncombined_summary %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Model = factor(Model, levels = c(\"30 min Model\", \"60 min Model\", \"90 min Model\"))) %&gt;%\n  kbl(col.names = c('Model', 'Mean Absolute Error', 'Standard Deviation of MAE')) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nModel\nMean Absolute Error\nStandard Deviation of MAE\n\n\n\n\n30 min Model\n1.804856\n0.0230556\n\n\n60 min Model\n2.242522\n0.0292971\n\n\n90 min Model\n2.418186\n0.0257110"
  },
  {
    "objectID": "posts/welcome/505Final.html",
    "href": "posts/welcome/505Final.html",
    "title": "San Francisco Crime Analysis in Physical and Social Environment",
    "section": "",
    "text": "Code\n# Data manipulation libraries\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Timer\nfrom tqdm import tqdm, tqdm_notebook\n\n# Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor \nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Model support functions\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform\nfrom sklearn.preprocessing import StandardScaler \nfrom pprint import pprint\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Geo-related libraries\nimport geopandas as gpd\nimport folium\nfrom folium.plugins import HeatMap\nimport geopy\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport contextily as ctx\nimport geofeather\nfrom geopandas import GeoDataFrame\nfrom shapely.geometry import Point\nfrom shapely import wkt\nfrom shapely.geometry import Point, MultiPoint\nfrom shapely.ops import nearest_points\nfrom shapely import wkt\n\nimport osmnx\n\n\n\n\nCode\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n\n\n\nCode\n#https://drive.google.com/file/d/1HJBU_-Yg_ylAzab8v11l1wcYEwBhO41-/view?usp=share_link\nid = \"1HJBU_-Yg_ylAzab8v11l1wcYEwBhO41-\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('Police_Department_Incident_Reports__2018_to_Present.csv')\n\n\n\n\n\n\n\nCode\n#load crime data\ncrime_df = pd.read_csv('Police_Department_Incident_Reports__2018_to_Present.csv')\ncrime_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Datetime\nIncident Date\nIncident Time\nIncident Year\nIncident Day of Week\nReport Datetime\nRow ID\nIncident ID\nIncident Number\nCAD Number\n...\nLongitude\nPoint\nNeighborhoods\nESNCAG - Boundary File\nCentral Market/Tenderloin Boundary Polygon - Updated\nCivic Center Harm Reduction Project Boundary\nHSOC Zones as of 2018-06-05\nInvest In Neighborhoods (IIN) Areas\nCurrent Supervisor Districts\nCurrent Police Districts\n\n\n\n\n0\n2023/03/13 11:41:00 PM\n2023/03/13\n23:41\n2023\nMonday\n2023/03/13 11:41:00 PM\n125373607041\n1253736\n230167874\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2023/03/01 05:02:00 AM\n2023/03/01\n05:02\n2023\nWednesday\n2023/03/11 03:40:00 PM\n125379506374\n1253795\n236046151\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2023/03/13 01:16:00 PM\n2023/03/13\n13:16\n2023\nMonday\n2023/03/13 01:17:00 PM\n125357107041\n1253571\n220343896\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2023/03/13 10:59:00 AM\n2023/03/13\n10:59\n2023\nMonday\n2023/03/13 11:00:00 AM\n125355107041\n1253551\n230174885\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2023/03/14 06:44:00 PM\n2023/03/14\n18:44\n2023\nTuesday\n2023/03/14 06:45:00 PM\n125402407041\n1254024\n230176728\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n5 rows × 35 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ncrime_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 716680 entries, 0 to 716679\nData columns (total 35 columns):\n #   Column                                                Non-Null Count   Dtype  \n---  ------                                                --------------   -----  \n 0   Incident Datetime                                     716680 non-null  object \n 1   Incident Date                                         716680 non-null  object \n 2   Incident Time                                         716680 non-null  object \n 3   Incident Year                                         716680 non-null  int64  \n 4   Incident Day of Week                                  716680 non-null  object \n 5   Report Datetime                                       716680 non-null  object \n 6   Row ID                                                716680 non-null  int64  \n 7   Incident ID                                           716680 non-null  int64  \n 8   Incident Number                                       716680 non-null  int64  \n 9   CAD Number                                            555602 non-null  float64\n 10  Report Type Code                                      716680 non-null  object \n 11  Report Type Description                               716680 non-null  object \n 12  Filed Online                                          144690 non-null  object \n 13  Incident Code                                         716680 non-null  int64  \n 14  Incident Category                                     716064 non-null  object \n 15  Incident Subcategory                                  716064 non-null  object \n 16  Incident Description                                  716680 non-null  object \n 17  Resolution                                            716680 non-null  object \n 18  Intersection                                          678513 non-null  object \n 19  CNN                                                   678513 non-null  float64\n 20  Police District                                       716680 non-null  object \n 21  Analysis Neighborhood                                 678381 non-null  object \n 22  Supervisor District                                   678138 non-null  float64\n 23  Supervisor District 2012                              678393 non-null  float64\n 24  Latitude                                              678513 non-null  float64\n 25  Longitude                                             678513 non-null  float64\n 26  Point                                                 678513 non-null  object \n 27  Neighborhoods                                         664168 non-null  float64\n 28  ESNCAG - Boundary File                                7761 non-null    float64\n 29  Central Market/Tenderloin Boundary Polygon - Updated  92127 non-null   float64\n 30  Civic Center Harm Reduction Project Boundary          91531 non-null   float64\n 31  HSOC Zones as of 2018-06-05                           149426 non-null  float64\n 32  Invest In Neighborhoods (IIN) Areas                   0 non-null       float64\n 33  Current Supervisor Districts                          678393 non-null  float64\n 34  Current Police Districts                              677736 non-null  float64\ndtypes: float64(14), int64(5), object(16)\nmemory usage: 191.4+ MB\n\n\n\n\nCode\n#convert the pandas file to geopandas file\ncrime_df_geo = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(crime_df.Longitude, crime_df.Latitude), crs={'init' :'epsg:4326'})\ncrime_df_geo.info()\n\n\n/usr/local/lib/python3.9/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=&lt;authority&gt;:&lt;code&gt;' syntax is deprecated. '&lt;authority&gt;:&lt;code&gt;' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 716680 entries, 0 to 716679\nData columns (total 36 columns):\n #   Column                                                Non-Null Count   Dtype   \n---  ------                                                --------------   -----   \n 0   Incident Datetime                                     716680 non-null  object  \n 1   Incident Date                                         716680 non-null  object  \n 2   Incident Time                                         716680 non-null  object  \n 3   Incident Year                                         716680 non-null  int64   \n 4   Incident Day of Week                                  716680 non-null  object  \n 5   Report Datetime                                       716680 non-null  object  \n 6   Row ID                                                716680 non-null  int64   \n 7   Incident ID                                           716680 non-null  int64   \n 8   Incident Number                                       716680 non-null  int64   \n 9   CAD Number                                            555602 non-null  float64 \n 10  Report Type Code                                      716680 non-null  object  \n 11  Report Type Description                               716680 non-null  object  \n 12  Filed Online                                          144690 non-null  object  \n 13  Incident Code                                         716680 non-null  int64   \n 14  Incident Category                                     716064 non-null  object  \n 15  Incident Subcategory                                  716064 non-null  object  \n 16  Incident Description                                  716680 non-null  object  \n 17  Resolution                                            716680 non-null  object  \n 18  Intersection                                          678513 non-null  object  \n 19  CNN                                                   678513 non-null  float64 \n 20  Police District                                       716680 non-null  object  \n 21  Analysis Neighborhood                                 678381 non-null  object  \n 22  Supervisor District                                   678138 non-null  float64 \n 23  Supervisor District 2012                              678393 non-null  float64 \n 24  Latitude                                              678513 non-null  float64 \n 25  Longitude                                             678513 non-null  float64 \n 26  Point                                                 678513 non-null  object  \n 27  Neighborhoods                                         664168 non-null  float64 \n 28  ESNCAG - Boundary File                                7761 non-null    float64 \n 29  Central Market/Tenderloin Boundary Polygon - Updated  92127 non-null   float64 \n 30  Civic Center Harm Reduction Project Boundary          91531 non-null   float64 \n 31  HSOC Zones as of 2018-06-05                           149426 non-null  float64 \n 32  Invest In Neighborhoods (IIN) Areas                   0 non-null       float64 \n 33  Current Supervisor Districts                          678393 non-null  float64 \n 34  Current Police Districts                              677736 non-null  float64 \n 35  geometry                                              716680 non-null  geometry\ndtypes: float64(14), geometry(1), int64(5), object(16)\nmemory usage: 196.8+ MB\n\n\n\n\nCode\n#intial visualization\nfig, ax = plt.subplots(figsize=(12, 10))\ncrime_df_geo.to_crs(epsg=3857).plot(ax = ax,\n                figsize=(12,12),\n                markersize=40,\n               color=\"black\",\n               edgecolor=\"white\",\n               alpha=0.8,\n               marker=\"o\"\n            );\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)"
  },
  {
    "objectID": "posts/welcome/505Final.html#clean-the-crime-data",
    "href": "posts/welcome/505Final.html#clean-the-crime-data",
    "title": "San Francisco Crime Analysis in Physical and Social Environment",
    "section": "clean the crime data",
    "text": "clean the crime data\n\n\nCode\n#clean the crime dataset\ncrime_df_copy = crime_df_new.copy()\ndrop = ['Incident Datetime','Incident Date','Report Datetime','Row ID','Incident ID','Incident Number','CAD Number','Report Type Description','Filed Online','Incident Code','Incident Description',\n        'CNN','Supervisor District','Central Market/Tenderloin Boundary Polygon - Updated','Civic Center Harm Reduction Project Boundary','HSOC Zones as of 2018-06-05','Invest In Neighborhoods (IIN) Areas',\n        'Current Supervisor Districts','Current Police Districts']\ncrime_df_drop = crime_df_copy.drop(columns = drop)\ncrime_df_drop.info()\n\n\n\n\nCode\n#select the theft type data in 2020\ncrime_df_drop = crime_df_drop[crime_df_drop.year == 2020]\ncrime_df_drop = crime_df_drop[crime_df_drop['Incident Category'] == 'Larceny Theft']\ncrime_df_drop.reset_index()\n\n\n\n\nCode\n#get the geo boundary of San Francisco\nsf_poly = osmnx.geocode_to_gdf('San Francisco, CA, USA')\nsf_poly.plot()\n\n\n\n\nCode\n#convert to geopandas file\ngpd_crime = gpd.GeoDataFrame(crime_df_drop, geometry = crime_df_drop['geometry'], crs={'init' :'epsg:4326'})\nsf_poly.crs, gpd_crime.crs #.crs -- geodataframe\n\n\n\n\nCode\n#spatial join\ngpd_crime_city = gpd.sjoin(gpd_crime,sf_poly,how='inner',op = 'intersects')\ngpd_crime_city.tail(2)\n\n\n\n\nCode\ngpd_crime_city.info()\n\n\n\n\nCode\n#drop the useless data \ngpd_crime_city = gpd_crime_city.drop(['bbox_north','bbox_south','bbox_east','bbox_west','index_right'],axis = 1)"
  },
  {
    "objectID": "posts/welcome/505Final.html#create-the-physical-environment-data",
    "href": "posts/welcome/505Final.html#create-the-physical-environment-data",
    "title": "San Francisco Crime Analysis in Physical and Social Environment",
    "section": "Create the Physical Environment Data",
    "text": "Create the Physical Environment Data\n\nStreet\n\n\nCode\n#get the street data of san francisco\nsf_streets = osmnx.graph_from_place('San Francisco, California', network_type = 'drive')\nnodes, edges = osmnx.graph_to_gdfs(sf_streets)\n\n\n\n\nCode\nnodes.head(1)\n\n\n\n\nCode\nedges.head(1)\n\n\n\n\nCode\nedges.plot()\n\n\n\n\nCode\n# brief information of road dataset\nsf_road = edges.copy()\nprint(sf_road['highway'].value_counts())\nprint('Number of rows is ' + str(sf_road.shape[0]))\nprint('Number of columns is ' + str(sf_road.shape[1]))\n\n\n\n\nCode\n# Combining similar road types to reduce classification number\nsf_road['highway'] = sf_road['highway'].str.replace('_link','')\nsf_road['highway'] = np.where(sf_road['highway']=='trunk','secondary',sf_road['highway'])\nsf_road['highway'] = np.where(sf_road['highway']=='living_street','residential',sf_road['highway'])\nprint(sf_road['highway'].value_counts())\n\n\n\n\nCode\n#based on road type to create sub-dataset\nsf_highways = sf_road[sf_road.highway == 'motorway']\nsf_pry = sf_road[sf_road.highway == 'primary']\nsf_second = sf_road[sf_road.highway == 'secondary']\nsf_resid = sf_road[sf_road.highway == 'residential']\nsf_tertiary = sf_road[sf_road.highway == 'tertiary']\n\nsf_highways.crs\n\n\n\n\nCode\n# convert to geopandas files and set function to calcualte the min distance from point to road\ngpd_crime_city_utm = gpd_crime_city.to_crs({'init': 'epsg:32610'}).copy()   \nhighway_utm = sf_highways.to_crs({'init': 'epsg:32610'}).copy()\nprimary_utm = sf_pry.to_crs({'init': 'epsg:32610'}).copy()\nsecondary_utm = sf_second.to_crs({'init': 'epsg:32610'}).copy()\ntertiary_utm = sf_tertiary.to_crs({'init': 'epsg:32610'}).copy()\nresidential_utm = sf_resid.to_crs({'init': 'epsg:32610'}).copy()\nsf_road_utm = sf_road.to_crs({'init': 'epsg:32610'}).copy()\n\ndef distance_to_roadway(gps,roadway):\n  dists = []\n  for i in roadway.geometry:\n    dists.append(i.distance(gps))\n  return (np.min(dists))\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_highway'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = highway_utm)\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_primary'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = primary_utm)\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_secondary'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = secondary_utm)\n\n\n\n\nCode\ntqdm.pandas()\ngpd_crime_city['Closest_tertiary'] = gpd_crime_city_utm['geometry'].progress_apply(distance_to_roadway,roadway = tertiary_utm)\n\n\n\n\nCode\ngpd_crime_city.head(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Time\nIncident Year\nIncident Day of Week\nReport Type Code\nIncident Category\nIncident Subcategory\nResolution\nIntersection\nPolice District\nAnalysis Neighborhood\n...\nlat\nlon\ndisplay_name\nclass\ntype\nimportance\nClosest_highway\nClosest_primary\nClosest_secondary\nClosest_tertiary\n\n\n\n\n9676\n02:00\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nBASS CT \\ WHITNEY YOUNG CIR\nBayview\nBayview Hunters Point\n...\n37.779026\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n1468.413426\n534.950176\n535.680061\n506.558042\n\n\n13583\n20:00\n2020\nSaturday\nII\nLarceny Theft\nLarceny - From Vehicle\nOpen or Active\nCLAY ST \\ LARKIN ST\nCentral\nRussian Hill\n...\n37.779026\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n2149.153684\n390.024468\n285.145981\n97.658841\n\n\n\n\n\n2 rows × 36 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n# based on the closest distance from point to each type of roads to ensure the road type where the crime point locate\ndef get_category(row):\n    if row['Closest_highway'] &lt; 10:\n        return 'highway'\n    elif row['Closest_primary'] &lt; 10:\n        return 'primary'\n    elif row['Closest_secondary'] &lt; 10:\n        return 'secondary'\n    elif row['Closest_tertiary'] &lt; 10:\n        return 'tertiary'\n    else:\n        return 'residential'\ngpd_crime_city['Crime_Location'] = gpd_crime_city.apply(get_category, axis=1)\n\n\n\n\nCode\ngpd_crime_city.head(2)\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Time\nIncident Year\nIncident Day of Week\nReport Type Code\nIncident Category\nIncident Subcategory\nResolution\nIntersection\nPolice District\nAnalysis Neighborhood\n...\nlon\ndisplay_name\nclass\ntype\nimportance\nClosest_highway\nClosest_primary\nClosest_secondary\nClosest_tertiary\nCrime_Location\n\n\n\n\n9676\n02:00\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nBASS CT \\ WHITNEY YOUNG CIR\nBayview\nBayview Hunters Point\n...\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n1468.413426\n534.950176\n535.680061\n506.558042\nresidential\n\n\n13583\n20:00\n2020\nSaturday\nII\nLarceny Theft\nLarceny - From Vehicle\nOpen or Active\nCLAY ST \\ LARKIN ST\nCentral\nRussian Hill\n...\n-122.419906\nSan Francisco, CAL Fire Northern Region, Calif...\nboundary\nadministrative\n1.025131\n2149.153684\n390.024468\n285.145981\n97.658841\nresidential\n\n\n\n\n\n2 rows × 37 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ngpd_crime_city.Crime_Location.value_counts()\n\n\nresidential    10518\nsecondary       8670\ntertiary        6382\nprimary         2520\nhighway          123\nName: Crime_Location, dtype: int64\n\n\n\n\nZoning\n\n\nCode\n#https://drive.google.com/file/d/1sremJO16LufbhJdXzSiAViXUmtJCieCN/view?usp=share_link\nid = \"1sremJO16LufbhJdXzSiAViXUmtJCieCN\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('SF_Zoning.csv')\n\n\n\n\nCode\n# load zoning data\nzoning_df = pd.read_csv('SF_Zoning.csv')\nzoning_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nthe_geom\nzoning_sim\ndistrictname\nurl\ngen\nzoning\ncodesection\n\n\n\n\n0\nMULTIPOLYGON (((-122.41533105377357 37.7723137...\nRED-MX\nRESIDENTIAL ENCLAVE-MIXED\nhttps://codelibrary.amlegal.com/codes/san_fran...\nMixed Use\nRED-MX\n847\n\n\n1\nMULTIPOLYGON (((-122.38902391194917 37.7381094...\nNCD\nBAYVIEW NEIGHBORHOOD COMMERCIAL DISTRICT\nhttps://codelibrary.amlegal.com/codes/san_fran...\nMixed Use\nNCD-BAYVIEW\n737\n\n\n2\nMULTIPOLYGON (((-122.389357706792 37.738606785...\nNCT-3\nMODERATE SCALE NEIGHBORHOOD COMMERCIAL TRANSIT...\nhttps://codelibrary.amlegal.com/codes/san_fran...\nMixed Use\nNCT-3\n752\n\n\n3\nMULTIPOLYGON (((-122.41533105377357 37.7723137...\nRED\nRESIDENTIAL ENCLAVE\nhttps://codelibrary.amlegal.com/codes/san_fran...\nResidential\nRED\n813\n\n\n4\nMULTIPOLYGON (((-122.38155774047206 37.7381229...\nPDR-1-B\nPDR LIGHT INDUSTRIAL BUFFER\nhttps://codelibrary.amlegal.com/codes/san_fran...\nIndustrial\nPDR-1-B\n210.3\n\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\nfrom shapely.wkt import loads\n\n\n\n\nCode\nzoning_df['geometry'] = zoning_df['the_geom'].apply(loads)\n\n# Create a GeoPandas DataFrame from the Pandas DataFrame\nzoning_gdf = gpd.GeoDataFrame(zoning_df, crs='EPSG:4326', geometry='geometry')\n\n\n\n\nCode\nzoning_gdf.head()\n\n\n\n\nCode\n#drop irrelevant data\ndrop = ['zoning_sim','url','zoning','codesection']\nzoning_new = zoning_gdf.drop(columns=drop)\nzoning_new.head()\n\n\n\n\nCode\n#brief visualization \nfig, ax = plt.subplots(figsize=(6, 4))\nzoning_new.to_crs(epsg=3857).plot(ax = ax,\n                figsize=(12,12),\n                markersize=40,\n               color=\"black\",\n               edgecolor=\"white\",\n               alpha=0.8,\n               marker=\"o\"\n            );\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n\n\n\nCode\n#spatial join zoning data and crime data\ngpd_crime_city_zone = gpd.sjoin(gpd_crime_city, zoning_new, op='within')\ngpd_crime_city_zone.head(2)\n\n\n\n\nBuilding\n\n\nCode\n#https://drive.google.com/file/d/1-7NryT-DamfZnKO7mU9ZqZsISg5TInsC/view?usp=share_link\nid = \"1-7NryT-DamfZnKO7mU9ZqZsISg5TInsC\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('sf_building.csv')\n\n\n\n\nCode\n#load building footprints data\nbuilding_df = pd.read_csv('sf_building.csv')\nbuilding_df.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nsf16_bldgid\narea_id\nmblr\np2010_name\np2010_zminn88ft\np2010_zmaxn88ft\ngnd_cells50cm\ngnd_mincm\ngnd_maxcm\ngnd_rangecm\n...\nhgt_majoritycm\nhgt_minoritycm\nhgt_mediancm\ngnd_min_m\nmedian_1st_m\nhgt_median_m\ngnd1st_delta\npeak_1st_m\nglobalid\nshape\n\n\n\n\n0\n201006.000000\n1\nSF4570025\nSanfranF_4606.flt\n16.3249\n66.2671\n178250\n507\n704\n197\n...\n813\n349\n850\n5.07\n14.16\n8.50\n9.09\n23.85\n{CF7EF595-68E6-4950-B361-CC82D77383A0}\nMULTIPOLYGON (((-122.37950387699999 37.7397994...\n\n\n1\n201006.000000\n2\nSM005050270\nSanfranI_09417.flt\n81.7802\n214.1874\n144111\n2515\n3419\n904\n...\n737\n11\n735\n25.15\n39.25\n7.35\n14.10\n66.94\n{806F8EDB-7B25-4054-898E-6F2CF65D072E}\nMULTIPOLYGON (((-122.41894297800002 37.7076482...\n\n\n2\n201006.000000\n3\nSF3794028\nSanfran_Orig_1330.flt\n12.5027\n102.5737\n115295\n164\n507\n343\n...\n734\n-86\n1157\n1.64\n15.29\n11.57\n13.65\n59.18\n{F0F234CF-3F0B-439D-B59A-8899C2135F23}\nMULTIPOLYGON (((-122.38881976200003 37.7792352...\n\n\n3\n201006.003228\n32278\nSM004154340\nSanfranP_3364.flt\n342.5260\n381.3803\n708\n10454\n10532\n78\n...\n521\n140\n456\n104.54\n109.39\n4.56\n4.85\n111.79\n{C321EB73-7E6E-49B5-822B-3E7DD3A7F0AD}\nMULTIPOLYGON (((-122.45552821000001 37.7074943...\n\n\n4\n201006.000000\n4\nSF7295021\nNaN\nNaN\nNaN\n107634\n4346\n5185\n839\n...\n1078\n16\n1153\n43.46\n61.58\n11.53\n18.12\n69.72\n{C52C089C-B0A6-4716-8358-C568A65861DC}\nMULTIPOLYGON (((-122.47734992300002 37.7289024...\n\n\n\n\n\n5 rows × 43 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\n##keep the geometry and height data\nbuilding_df = building_df[['peak_1st_m','shape']]\nbuilding_df.head(2)\n\n\n\n\nCode\nbuilding_df['geometry'] = building_df['shape'].apply(loads)\n# Create a GeoPandas DataFrame from the Pandas DataFrame\nbuilding_gdf = gpd.GeoDataFrame(building_df, crs='EPSG:4326', geometry='geometry')\n\n\n\n\nCode\nbuilding_gdf.head(2)\n\n\n\n\nCode\n# Calculate the area of each building\nbuilding_gdf['area'] = building_gdf['geometry'].area\nbuilding_gdf.head(2)\n\n\n\n\nCode\n# turan the form of data from polygon to point for find nearest point\npoint_gdf = building_gdf.copy()\npoint_gdf[\"geometry\"] = building_gdf.centroid\npoint_gdf.head(2)\n\n\n\n\nCode\ngpd_crime_city_zone = gpd_crime_city_zone.rename(columns={'index_left': 'left'})\ngpd_crime_city_zone = gpd_crime_city_zone.rename(columns={'index_right': 'right'})\n\n\n\n\nCode\ngpd_crime_city_zone.head(2)\n\n\n\n\nCode\nfrom google.colab import drive\ndrive.mount('/content/drive')\n#save_path = '/content/drive/MyDrive/Colab Notebooks/data/gpd_crime_city_zone.csv'\n#df.to_csv(save_path, index=False)\n\n\nMounted at /content/drive\n\n\n\n\nCode\n#https://drive.google.com/file/d/1WkxWvKEsSpq50x-BEasGqmsgu6pi91_m/view?usp=share_link\nid = \"1WkxWvKEsSpq50x-BEasGqmsgu6pi91_m\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('gpd_crime_city_zone.csv')\n\n\n\n\nCode\ngpd_crime_city_zone = pd.read_csv('gpd_crime_city_zone.csv')\ngpd_crime_city_zone.head()\n\n\n\n  \n    \n      \n\n\n\n\n\n\nIncident Time\nIncident Year\nIncident Day of Week\nReport Type Code\nIncident Category\nIncident Subcategory\nResolution\nIntersection\nPolice District\nAnalysis Neighborhood\n...\nClosest_primary\nClosest_secondary\nClosest_tertiary\nCrime_Location\nright\nthe_geom\ndistrictname\ngen\nbuffer_5m\nbuffer\n\n\n\n\n0\n02:00\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nBASS CT \\ WHITNEY YOUNG CIR\nBayview\nBayview Hunters Point\n...\n534.950176\n535.680061\n506.558042\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.38469170531177 37.7337597458953...\nPOLYGON ((-117.38469170531177 37.7337597458953...\n\n\n1\n13:59\n2020\nSunday\nVI\nLarceny Theft\nTheft From Vehicle\nOpen or Active\nWHITNEY YOUNG CIR \\ CASHMERE ST \\ DEDMAN CT\nBayview\nBayview Hunters Point\n...\n364.965970\n433.206386\n203.201740\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.38571301129124 37.7366011278019...\nPOLYGON ((-117.38571301129124 37.7366011278019...\n\n\n2\n16:28\n2020\nFriday\nII\nLarceny Theft\nLarceny Theft - Other\nOpen or Active\nLA SALLE AVE \\ OSCEOLA LN\nBayview\nBayview Hunters Point\n...\n833.743266\n461.886008\n835.429639\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.3819485575142 37.73152913469796...\nPOLYGON ((-117.3819485575142 37.73152913469796...\n\n\n3\n18:00\n2020\nTuesday\nII\nLarceny Theft\nLarceny - Auto Parts\nOpen or Active\nINGALLS ST \\ HARBOR RD\nBayview\nBayview Hunters Point\n...\n951.461748\n255.279620\n825.220932\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.3799317268322 37.73338073279184...\nPOLYGON ((-117.3799317268322 37.73338073279184...\n\n\n4\n21:00\n2020\nSunday\nVI\nLarceny Theft\nTheft From Vehicle\nOpen or Active\nCASHMERE ST \\ HUDSON AVE\nBayview\nBayview Hunters Point\n...\n582.547890\n387.780374\n420.026310\nresidential\n815\nMULTIPOLYGON (((-122.38222192626027 37.7365910...\nRESIDENTIAL- HOUSE, TWO FAMILY\nResidential\nPOLYGON ((-117.38342544658872 37.7358032525956...\nPOLYGON ((-117.38342544658872 37.7358032525956...\n\n\n\n\n\n5 rows × 43 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nCode\ngpd_crime_city_zone = gpd.GeoDataFrame(gpd_crime_city_zone, geometry=gpd.points_from_xy(gpd_crime_city_zone.Longitude, gpd_crime_city_zone.Latitude), crs={'init' :'epsg:4326'})\ngpd_crime_city_zone.info()\n\n\n\n\nCode\nfrom scipy.spatial import cKDTree\n\n\n\n\nCode\npd_crime_city_zone = np.array(list(gpd_crime_city_zone.geometry.apply(lambda pt: (pt.x, pt.y))))\npoint_df = np.array(list(point_gdf.geometry.apply(lambda pt: (pt.x, pt.y))))\n\n# Build cKDTree from crime points\ntree = cKDTree(point_df)\n\n# Query tree for nearest crime to each Boston point\ndist, idx = tree.query(pd_crime_city_zone, k=1)\n\n# Add nearest crime column to boston GeoDataFrame\ngpd_crime_city_zone['builidng_nn1'] = point_gdf.loc[idx, 'peak_1st_m'].values\n\n\n\n\nCode\ndrop = ['buffer_5m','buffer','right']\ngpd_crime_city_zone = gpd_crime_city_zone.drop(columns=drop)\n\n\n\n\nCode\nval_dict = {'highway': 24, 'primary': 20, 'secondary': 12, 'tertiary': 8, 'residential': 6}\ngpd_crime_city_zone['road_width'] = gpd_crime_city_zone['Crime_Location'].map(val_dict)\n\n\n\n\nCode\ngpd_crime_city_zone['rate_BS'] = gpd_crime_city_zone['builidng_nn1']/gpd_crime_city_zone['road_width']\n\n\n\n\nCensus tract\n\nload basic census tract boundary\n\n\nCode\n#https://drive.google.com/file/d/1DbA90nYecBQnwRkcqUAAP8vT7OQP9MWz/view?usp=share_link\nid = \"1DbA90nYecBQnwRkcqUAAP8vT7OQP9MWz\"\nfile = drive.CreateFile({'id':id}) \nfile.GetContentFile('sf_centra.csv')\n\n\n\n\nCode\nct_df = pd.read_csv('sf_centra.csv')\nct_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 176 entries, 0 to 175\nData columns (total 47 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   the_geom    176 non-null    object \n 1   OBJECTID    176 non-null    int64  \n 2   FIPSSTCO    176 non-null    int64  \n 3   TRT2000     176 non-null    int64  \n 4   STFID       176 non-null    int64  \n 5   TRACTID     176 non-null    float64\n 6   STATE       176 non-null    int64  \n 7   COUNTY      176 non-null    int64  \n 8   TRACT       176 non-null    int64  \n 9   POP2000     176 non-null    int64  \n 10  WHITE       176 non-null    int64  \n 11  BLACK       176 non-null    int64  \n 12  AMERI_ES    176 non-null    int64  \n 13  ASIAN       176 non-null    int64  \n 14  HAWN_PI     176 non-null    int64  \n 15  OTHER       176 non-null    int64  \n 16  MULT_RACE   176 non-null    int64  \n 17  HISPANIC    176 non-null    int64  \n 18  MALES       176 non-null    int64  \n 19  FEMALES     176 non-null    int64  \n 20  AGE_UNDER5  176 non-null    int64  \n 21  AGE_5_17    176 non-null    int64  \n 22  AGE_18_21   176 non-null    int64  \n 23  AGE_22_29   176 non-null    int64  \n 24  AGE_30_39   176 non-null    int64  \n 25  AGE_40_49   176 non-null    int64  \n 26  AGE_50_64   176 non-null    int64  \n 27  AGE_65_UP   176 non-null    int64  \n 28  MED_AGE     176 non-null    float64\n 29  MED_AGE_M   176 non-null    float64\n 30  MED_AGE_F   176 non-null    float64\n 31  HOUSEHOLDS  176 non-null    int64  \n 32  AVE_HH_SZ   176 non-null    float64\n 33  HSEHLD_1_M  176 non-null    int64  \n 34  HSEHLD_1_F  176 non-null    int64  \n 35  MARHH_CHD   176 non-null    int64  \n 36  MARHH_NO_C  176 non-null    int64  \n 37  MHH_CHILD   176 non-null    int64  \n 38  FHH_CHILD   176 non-null    int64  \n 39  FAMILIES    176 non-null    int64  \n 40  AVE_FAM_SZ  176 non-null    float64\n 41  HSE_UNITS   176 non-null    int64  \n 42  URBAN       176 non-null    int64  \n 43  RURAL       176 non-null    int64  \n 44  VACANT      176 non-null    int64  \n 45  OWNER_OCC   176 non-null    int64  \n 46  RENTER_OCC  176 non-null    int64  \ndtypes: float64(6), int64(40), object(1)\nmemory usage: 64.8+ KB\n\n\n\n\nCode\nct_df['TRACTID'] = ct_df['TRACTID'].astype('object')\n\n\n\n\nload social-economic data based on the census tract\n\n\nCode\nimport censusdata\nfrom census import Census\nfrom us import states\n\n\n\n\nCode\n# Define the Census API key\ncensusdata.censuskey = \"93db71b5bb2f994846b61d1cb53de6b204ed41b8\"\nc = Census(\"93db71b5bb2f994846b61d1cb53de6b204ed41b8\")\nyear = 2020\nstate_fips = '06'  # California\ncounty_fips = '075'  # San Francisco County\n\n#   B07010_004 -- Estimate!!Total:!!With income:!!$1 to $9,999 or loss -- poverty indicator\n# B06009_002 -- Estimate!!Total:!!Less than high school graduate\n# B09010_002 -- Estimate!!Total:!!Living in household with Supplemental Security Income (SSI), cash public assistance income, or Food Stamps/SNAP in the past 12 months:\n# B27011_008 -- Estimate!!Total:!!In labor force:!!Unemployed:\n# B11013_002 -- Estimate!!Total:!!Married-couple subfamily\n# B11013_005 -- Estimate!!Total:!!Mother-child subfamily\n# B11013_006 -- Estimate!!Total:!!Father-child subfamily\nva_census = c.acs5.state_county_tract(fields = ('NAME','B07010_004E','B06009_002E', 'B09010_002E', 'B27011_008E','B11014_005E','B11014_006E','B11014_009E'),\n                                      state_fips = '06',\n                                      county_fips = '075',\n                                      tract = \"*\",\n                                      year = 2020)\n\n\n\n\nCode\nsfp_df = pd.DataFrame(va_census)\nsfp_df.head(2)\n\n\n\n\nCode\nsfp_df['tract'] = sfp_df['NAME'].str.extract(r'Census Tract (\\d+\\.\\d+)', expand=False)\n\n\n\n\nCode\nsfp_df.head(2)\n\n\n\n\nCode\nsfp_df = sfp_df.rename(columns={'B07010_004E': 'low_income', 'B06009_002E':'low_education','B09010_002E':'fd_secure','B27011_008E':'unemploy','B11014_005E':'couple_fa',\n                                'B11014_006E':'ma-child','B11014_009E':'fa-child'})\nsfp_df.head(2)\n\n\n\n\nCode\nsfp_df = sfp_df.rename(columns={'tract': 'TRACTID'})\nsfp_df.head(2)\n\n\n\n\nCode\nsocieco_df = pd.merge(ct_df, sfp_df, on='TRACTID',how='left')\n\n\n\n\nCode\nct_df['geometry'] = ct_df['the_geom'].apply(loads)\n# Create a GeoPandas DataFrame from the Pandas DataFrame\nct_gdf = gpd.GeoDataFrame(ct_df, crs='EPSG:4326', geometry='geometry')"
  },
  {
    "objectID": "posts/germantown/index.html",
    "href": "posts/germantown/index.html",
    "title": "Germantown Equity Strategic Planning",
    "section": "",
    "text": "If want to see the complete version of our planning, please check the link and see the full planning book.\n\nIntroduction\nLocated in Northwest Philadelphia, Germantown is a culturally vibrant, historic neighborhood with over 50,000 residents calling the neighborhood home. Germantown’s borders are roughly bounded by Wissahickon Ave. to the west, Johnson & Washington Streets to the north, Stenton Ave. to east, Wister Street to the southwest, and Roberts Ave to the south. Originally an independent township established in the 1680s by German-Dutch settlers, Germantown was incorporated into Philadelphia almost 170 years ago in 1854. Today, the residential neighborhood has retained its character, with its namesake Avenue being a vital component of Germantown’s distinct identity. However, with increasing development occurring in the neighborhood, Germantown is at several crossroads after being affected by decades of disinvestment, population decline, high crime, and economic downturns."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/NY/550 Final.html",
    "href": "posts/NY/550 Final.html",
    "title": "How to Find My Bar in New York?",
    "section": "",
    "text": "After a busy week at work why not head to New York for a great weekend! But because there are so many options in New York, choosing where to stay and where to go for the night can sometimes be a difficult decision. Therefore, this analysis aims to analyze and visualize the data of Airbnb and bars in New York to provide a guide for future visitors to New York.\nThe dataset I would use in the project includes following: 1. New York City Airbnb Open Data. The dataset includes the features, prices, and location of the room. It will be the main dataset that for final Airbnb selection. 2. 2016 Parties in New York. The dataset includes the Location of the bar and the number of noise record for the bars. It will identify the number of entertainment venues in the vicinity of each site and the likely noise levels. 3. Uber picks up in New York City. The dataset includes the the pick up location and time of Uber. This dataset demonstrates the ease of travel behavior. 4. Census Data. The data will include the basic regional unit for discussion in the new step suggestion for visitors to choose their preferred airbnb.\n\n\nCode\n%env MYPATH=C:/Folder Name/file.txt\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\nimport folium\nimport xyzservices\nimport panel as pn\n\nimport datetime\nimport time\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport holoviews as hv\nimport hvplot.pandas\nimport contextily as ctx\nimport geoviews as gv\nimport geoviews.tile_sources as gvts\n\nimport altair as alt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport requests\nfrom sodapy import Socrata\nimport missingno as msno\nfrom scipy.stats import gaussian_kde\n\nimport osmnx as ox\nimport folium\nimport altair as alt\nfrom wordcloud import WordCloud\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# load dataset from google drive \nurl_basic = 'https://drive.google.com/uc?id='\nair='https://drive.google.com/file/d/1b0Hih_3K-xS3DZ6YKPSM_97yrGPEYlHE/view?usp=sharing'\nurl_air= url_basic + air.split('/')[-2]\n\nuber = 'https://drive.google.com/file/d/1p_BO6Kd_R-dRGLliP8cDBdC01pF2OqmJ/view?usp=sharing'\nurl_uber= url_basic + uber.split('/')[-2]\n\nbar = 'https://drive.google.com/file/d/1gLf9IyT6Vy2_ZiWfaQvOXRMMwOWPog3z/view?usp=sharing'\nurl_bar = url_basic + bar.split('/')[-2]\n\nparties = 'https://drive.google.com/file/d/1U0eZyg1UhuCSpDobZZgClj1v1STCfNvM/view?usp=sharing'\nurl_prt = url_basic + parties.split('/')[-2]\n\nparties_test = 'https://drive.google.com/file/d/1LVlJ64Wvj-p43AGpkH09BzUtJJeIAVwt/view?usp=sharing'\nurl_prt_test =  url_basic + parties_test.split('/')[-2]\n\nparties_train = 'https://drive.google.com/file/d/1ww_K4UF-xSagwnqz7nNoKH_Ojv0QL9Iy/view?usp=sharing'\nurl_prt_train =  url_basic + parties_train.split('/')[-2]\n\nboundary = 'https://drive.google.com/file/d/1nZz5GG3pPcNhNcvQYA_DTAaKAI5w6J-f/view?usp=sharing'\nurl_bund =  url_basic + boundary.split('/')[-2]\n\nnbhd = 'https://drive.google.com/file/d/1hI840aCWK2vbam-6SVT-NqTBM0BkLaY5/view?usp=sharing'\nurl_nbhd =  url_basic + nbhd.split('/')[-2]\n\nair_df = pd.read_csv(url_air)\nuber_df = pd.read_csv(url_uber,parse_dates=['Date/Time'])\nbar_df = pd.read_csv(url_bar)\nprt_df = pd.read_csv(url_prt,parse_dates=['Created Date','Closed Date'])\nprt_test_df = pd.read_csv(url_prt_test)\nprt_train_df = pd.read_csv(url_prt_train)\nbdry_gdf = gpd.read_file(url_bund,crs='EPSG:4326')\nnbhd_gdf = gpd.read_file(url_nbhd,crs='EPSG:4326')\n\n\n\n\nCode\n# airbnb dataset clean\ndropy = ['id','host_id','host_name','last_review']\nair_df = air_df.drop(dropy,axis=1)\nair_df['reviews_per_month'] = air_df['reviews_per_month'].fillna(0)\nair_df = air_df.dropna()\nair_df = air_df[air_df['price'] &lt;= 4000]\nair_gdf = gpd.GeoDataFrame(air_df, geometry=[Point(xy) for xy in zip(air_df.longitude, air_df.latitude)])\nair_gdf.set_crs(epsg=4326, inplace=True)\n\n\n\n\n\n\n\n\n\nname\nneighbourhood_group\nneighbourhood\nlatitude\nlongitude\nroom_type\nprice\nminimum_nights\nnumber_of_reviews\nreviews_per_month\ncalculated_host_listings_count\navailability_365\ngeometry\n\n\n\n\n0\nClean & quiet apt home by the park\nBrooklyn\nKensington\n40.64749\n-73.97237\nPrivate room\n149\n1\n9\n0.21\n6\n365\nPOINT (-73.97237 40.64749)\n\n\n1\nSkylit Midtown Castle\nManhattan\nMidtown\n40.75362\n-73.98377\nEntire home/apt\n225\n1\n45\n0.38\n2\n355\nPOINT (-73.98377 40.75362)\n\n\n2\nTHE VILLAGE OF HARLEM....NEW YORK !\nManhattan\nHarlem\n40.80902\n-73.94190\nPrivate room\n150\n3\n0\n0.00\n1\n365\nPOINT (-73.94190 40.80902)\n\n\n3\nCozy Entire Floor of Brownstone\nBrooklyn\nClinton Hill\n40.68514\n-73.95976\nEntire home/apt\n89\n1\n270\n4.64\n1\n194\nPOINT (-73.95976 40.68514)\n\n\n4\nEntire Apt: Spacious Studio/Loft by central park\nManhattan\nEast Harlem\n40.79851\n-73.94399\nEntire home/apt\n80\n10\n9\n0.10\n1\n0\nPOINT (-73.94399 40.79851)\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n48890\nCharming one bedroom - newly renovated rowhouse\nBrooklyn\nBedford-Stuyvesant\n40.67853\n-73.94995\nPrivate room\n70\n2\n0\n0.00\n2\n9\nPOINT (-73.94995 40.67853)\n\n\n48891\nAffordable room in Bushwick/East Williamsburg\nBrooklyn\nBushwick\n40.70184\n-73.93317\nPrivate room\n40\n4\n0\n0.00\n2\n36\nPOINT (-73.93317 40.70184)\n\n\n48892\nSunny Studio at Historical Neighborhood\nManhattan\nHarlem\n40.81475\n-73.94867\nEntire home/apt\n115\n10\n0\n0.00\n1\n27\nPOINT (-73.94867 40.81475)\n\n\n48893\n43rd St. Time Square-cozy single bed\nManhattan\nHell's Kitchen\n40.75751\n-73.99112\nShared room\n55\n1\n0\n0.00\n6\n2\nPOINT (-73.99112 40.75751)\n\n\n48894\nTrendy duplex in the very heart of Hell's Kitchen\nManhattan\nHell's Kitchen\n40.76404\n-73.98933\nPrivate room\n90\n7\n0\n0.00\n1\n23\nPOINT (-73.98933 40.76404)\n\n\n\n\n48847 rows × 13 columns\n\n\n\n\n\nCode\n# party dataset clean\nprt_df['duration'] = (prt_df['Closed Date'].view('int64') // 10**9 - prt_df['Created Date'].view('int64') // 10**9)/60\nprt_df['hour'] = prt_df['Closed Date'].dt.hour\nprt_df['dow'] = prt_df['Created Date'].dt.weekday\nprt_df['date'] = prt_df['Created Date'].dt.date\nprt_gdf = gpd.GeoDataFrame(prt_df, geometry=[Point(xy) for xy in zip(prt_df.Longitude, prt_df.Latitude)])\nprt_gdf.set_crs(epsg=4326, inplace=True)\nprt_gdf = prt_gdf[(prt_gdf['duration'] &gt; 0) & (prt_gdf['duration'] &lt; 480)]\nprt_gdf.head(5)\n\n\n\n\n\n\n\n\n\nCreated Date\nClosed Date\nLocation Type\nIncident Zip\nCity\nBorough\nLatitude\nLongitude\nduration\nhour\ndow\ndate\ngeometry\n\n\n\n\n0\n2015-12-31 00:01:15\n2015-12-31 03:48:04\nStore/Commercial\n10034.0\nNEW YORK\nMANHATTAN\n40.866183\n-73.918930\n226.816667\n3.0\n3\n2015-12-31\nPOINT (-73.91893 40.86618)\n\n\n1\n2015-12-31 00:02:48\n2015-12-31 04:36:13\nStore/Commercial\n10040.0\nNEW YORK\nMANHATTAN\n40.859324\n-73.931237\n273.416667\n4.0\n3\n2015-12-31\nPOINT (-73.93124 40.85932)\n\n\n2\n2015-12-31 00:03:25\n2015-12-31 00:40:15\nResidential Building/House\n10026.0\nNEW YORK\nMANHATTAN\n40.799415\n-73.953371\n36.833333\n0.0\n3\n2015-12-31\nPOINT (-73.95337 40.79942)\n\n\n3\n2015-12-31 00:03:26\n2015-12-31 01:53:38\nResidential Building/House\n11231.0\nBROOKLYN\nBROOKLYN\n40.678285\n-73.994668\n110.200000\n1.0\n3\n2015-12-31\nPOINT (-73.99467 40.67829)\n\n\n4\n2015-12-31 00:05:10\n2015-12-31 03:49:10\nResidential Building/House\n10033.0\nNEW YORK\nMANHATTAN\n40.850304\n-73.938516\n224.000000\n3.0\n3\n2015-12-31\nPOINT (-73.93852 40.85030)\n\n\n\n\n\n\n\n\n\nCode\nuber_df['hour'] = uber_df['Date/Time'].dt.hour\nuber_df['dow'] = uber_df['Date/Time'].dt.weekday\nuber_df['date'] = uber_df['Date/Time'].dt.date\nuber_gdf = gpd.GeoDataFrame(uber_df, geometry=[Point(xy) for xy in zip(uber_df.Lon, uber_df.Lat)])\nuber_gdf.set_crs(epsg=4326, inplace=True)\nuber_gdf.head(5)\n\n\n\n\n\n\n\n\n\nDate/Time\nLat\nLon\nBase\nhour\ndow\ndate\ngeometry\n\n\n\n\n0\n2014-06-01 00:00:00\n40.7293\n-73.9920\nB02512\n0\n6\n2014-06-01\nPOINT (-73.99200 40.72930)\n\n\n1\n2014-06-01 00:01:00\n40.7131\n-74.0097\nB02512\n0\n6\n2014-06-01\nPOINT (-74.00970 40.71310)\n\n\n2\n2014-06-01 00:04:00\n40.3461\n-74.6610\nB02512\n0\n6\n2014-06-01\nPOINT (-74.66100 40.34610)\n\n\n3\n2014-06-01 00:04:00\n40.7555\n-73.9833\nB02512\n0\n6\n2014-06-01\nPOINT (-73.98330 40.75550)\n\n\n4\n2014-06-01 00:07:00\n40.6880\n-74.1831\nB02512\n0\n6\n2014-06-01\nPOINT (-74.18310 40.68800)\n\n\n\n\n\n\n\n\n\nCode\nbar_gdf = gpd.GeoDataFrame(bar_df, geometry=[Point(xy) for xy in zip(bar_df.Longitude, bar_df.Latitude)])\nbar_gdf.set_crs(epsg=4326, inplace=True)\nbar_gdf = bar_gdf[bar_gdf['num_calls']&lt;250]\nbar_gdf.head(5)\n\n\n\n\n\n\n\n\n\nLocation Type\nIncident Zip\nCity\nBorough\nLatitude\nLongitude\nnum_calls\ngeometry\n\n\n\n\n0\nClub/Bar/Restaurant\n10308.0\nSTATEN ISLAND\nSTATEN ISLAND\n40.544096\n-74.141155\n40\nPOINT (-74.14115 40.54410)\n\n\n1\nClub/Bar/Restaurant\n10012.0\nNEW YORK\nMANHATTAN\n40.729793\n-73.998842\n18\nPOINT (-73.99884 40.72979)\n\n\n2\nClub/Bar/Restaurant\n10308.0\nSTATEN ISLAND\nSTATEN ISLAND\n40.544209\n-74.141040\n21\nPOINT (-74.14104 40.54421)\n\n\n3\nClub/Bar/Restaurant\n10034.0\nNEW YORK\nMANHATTAN\n40.866376\n-73.928258\n160\nPOINT (-73.92826 40.86638)\n\n\n4\nClub/Bar/Restaurant\n11220.0\nBROOKLYN\nBROOKLYN\n40.635207\n-74.020285\n17\nPOINT (-74.02028 40.63521)"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-should-i-live-airbnb-analysis-visualization",
    "href": "posts/NY/550 Final.html#where-should-i-live-airbnb-analysis-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where should I live? – Airbnb Analysis & Visualization",
    "text": "Where should I live? – Airbnb Analysis & Visualization\n\n\nCode\n# group by bonough\nnbhd_price = air_df.groupby('neighbourhood_group')['price'].mean()\nnbhd_price  = nbhd_price .reset_index()\nbdry_price = bdry_gdf.merge(nbhd_price,left_on='boro_name', right_on='neighbourhood_group', how='inner')\n\n\n\n\nCode\n# group by neighborhood\nair_cen_gdf =air_gdf.sjoin(nbhd_gdf, how='inner')\ncensus_price = air_cen_gdf.groupby('ntacode')['price'].mean()\ncensus_price = census_price.reset_index()\ncensus_review = air_cen_gdf.groupby('ntacode')['number_of_reviews'].mean()\ncensus_review = census_review.reset_index()\ncensus_rm = air_cen_gdf.groupby('ntacode')['reviews_per_month'].mean()\ncensus_rm = census_rm.reset_index()\ncensus_pr = census_price.merge(census_review,on='ntacode')\ncensus_prm = census_pr.merge(census_rm,on = 'ntacode')\ncensus_prm = nbhd_gdf.merge(census_prm[['price','number_of_reviews','reviews_per_month','ntacode']],on='ntacode')\n\n\nTo better understand the airbnb situation in New York, I firstly visualized the statistical distribution of Airbnb data. We were able to find a distribution of prices that, with the exception of some of the higher-priced listings, was close to a normal distribution for most of the homes, with a concentration in the $20-$500 a night range. As for the number of reviews, We were able to find that the vast majority of listings received lower reviews. When it comes to room type, the ‘entire room’ and ‘private room’ took the major part. What’s more, most home located in Manhattan and Brooklyn, which can be explained by the fact that Brooklyn and Manhattan have most of New York’s places to hang out.\n\n\nCode\n# airbnb situation in New York\n_,axss = plt.subplots(2,2,figsize = [20,8])\nsns.histplot(air_df['price'], bins=80, kde=False, color='#2a9d8f',ax = axss[0,0])\nsns.histplot(air_df['number_of_reviews'], bins=100, kde=False, color='#2a9d8f',ax = axss[0,1])\nsns.countplot(x='room_type', data=air_df,ax = axss[1,0])\nsns.countplot(x='neighbourhood_group', data=air_df,ax = axss[1,1])\nplt.show()\n\n\n\n\n\nTo better understand Airbnb’s geographic distribution patterns on New York, we used map visualizations for further analysis. The spatial distribution of locations shows that the density of listings gradually decreases in all directions, centered on Manhattan, while prices show an accumulation of higher prices at the center of density. Looking at average home prices in the greater region, Manhattan and Brooklyn have the first and second highest.\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5), facecolor=\"#e5e7eb\")\n\n# Plot\nair_gdf.plot(\n    ax=ax,\n    column=\"price\",\n    edgecolor=\"black\",\n    linewidth=0.1,\n    legend=True,\n    legend_kwds=dict(loc=\"lower right\", fontsize=10),\n    cmap=\"Reds\",\n    markersize=2,\n    scheme=\"Quantiles\",\n    k=5,\n)\n\nax.set_title(\"Airbnb Price Location in NY\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n\nCode\nm = bdry_price.explore(column=\"price\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd when we look at the distribution of prices and reviews for Audemars Piguet at a smaller neighborhood scale, we are able to see that Audemars Piguet’s prices are also gradually decreasing in all directions, centered on Upper Manhattan. As for the reviews of the listings, we were able to find that the number of reviews in the higher priced areas is relatively low. The lower priced areas have a higher number of reviews as well as a higher average number of reviews per month. This can be explained by the fact that higher priced homes have a relatively smaller audience and less affordable people.\n\n\nCode\nm = census_prm.explore(column=\"number_of_reviews\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\nm = census_prm.explore(column=\"price\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nOverall, the price of a listing and more comprehensive information is overall not available at the same time. Those who are not price-sensitive have more options located in Mankato as well as Northwest Brooklyn. For a more cost-effective and comprehensive option, consider listings in the Bronx and Queens!"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-can-i-have-fun-entertainment-analysis-visualization",
    "href": "posts/NY/550 Final.html#where-can-i-have-fun-entertainment-analysis-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where Can I have fun? – Entertainment Analysis & Visualization",
    "text": "Where Can I have fun? – Entertainment Analysis & Visualization\nIn addition to accommodation options, we also paid equal attention to what ‘post-fun’ places there are to choose from in New York outside of everyday play. So why not go to a bar? From the point of view of the distribution of bars in New York, most of the bars are still concentrated in Manhattan, where the most prosperous business activities and nightlife in New York. However, when we look at complaints, we find that the average number of complaints received by bars in areas other than the Bronx is about the same, with Queens having the highest average, which is probably due to the fact that Queens itself is a large neighborhood. And as noisy areas are accompanied by disturbances at night, areas near bars with high complaints should be avoided as much as possible when choosing an Airbnb.\n\n\nCode\nbar_call = bar_gdf.groupby('Borough')['num_calls'].mean()\nbar_call = bar_call.reset_index()\nbar_call['Borough'] = bar_call['Borough'].str.lower()\nbar_count = bar_gdf.groupby('Borough').count()\nbar_count = bar_count.reset_index()\nbar_count = bar_count[['Borough','City']]\nbar_count['Borough'] = bar_count['Borough'].str.lower()\nbar_count.columns = ['Borough', 'Count']\nbar_cc = bar_call.merge(bar_count,on='Borough')\nbdry_gdf['boro_name'] = bdry_gdf['boro_name'].str.lower()\nbar_bd = bdry_gdf.merge(bar_cc,left_on='boro_name', right_on='Borough', how='inner')\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(8, 5), facecolor=\"#e5e7eb\")\n\n# Plot\nbar_gdf.plot(\n    ax=ax,\n    column=\"num_calls\",\n    edgecolor=\"black\",\n    linewidth=0.1,\n    legend=True,\n    legend_kwds=dict(loc=\"lower right\", fontsize=10),\n    cmap=\"Reds\",\n    markersize=2,\n    scheme=\"Quantiles\",\n    k=5,\n)\n\nax.set_title(\"Bar Complaints in NY\")\nax.set_axis_off()\nax.set_aspect(\"equal\")\n\n\n\n\n\n\n\nCode\n_,axss = plt.subplots(2,figsize = [20,10])\nsns.barplot(x = 'boro_name', y ='num_calls',data = bar_bd,ax = axss[1])\nsns.barplot(x = 'boro_name', y ='Count',data = bar_bd,ax = axss[0])\n\n\n&lt;Axes: xlabel='boro_name', ylabel='Count'&gt;\n\n\n\n\n\n\n\nCode\nbar_cen_gdf =bar_gdf.sjoin(nbhd_gdf, how='inner')\nbar_call = bar_cen_gdf.groupby('ntacode')['num_calls'].mean()\nbar_call = bar_call.reset_index()\nbar_call_sum = bar_cen_gdf.groupby('ntacode').count()\nbar_call_sum = bar_call_sum.reset_index()\nbar_call_sum = bar_call_sum[['ntacode','City']]\ncen_bar = nbhd_gdf.merge(bar_call,on='ntacode')\ncen_bar = cen_bar.merge(bar_call_sum,on='ntacode')\n\n\n\n\nCode\nbar_cc = bar_call.merge(bar_call_sum,on='ntacode')\nbar_cc.head(4)\n\n\n\n\n\n\n\n\n\nntacode\nnum_calls\nCity\n\n\n\n\n0\nBK09\n12.000000\n4\n\n\n1\nBK17\n37.714286\n7\n\n\n2\nBK19\n58.800000\n5\n\n\n3\nBK21\n14.000000\n1\n\n\n\n\n\n\n\n\n\nCode\nm = cen_bar.explore(column=\"num_calls\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\nm = cen_bar.explore(column=\"City\", scheme=\"FisherJenks\")\nm\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAnd when we looked at where and when parties were held in New York, we were able to see that the most parties were held in residential buildings, and the vast majority of parties ended between nighttime hours and 5 a.m. the next day, which means that most of the venues where parties took place probably weren’t a good choice for an Airbnb location!\n\n\nCode\n_,axss = plt.subplots(2,2,figsize = [20,10])\nsns.countplot(x = 'dow',data = prt_gdf,ax=axss[0,0])\nsns.countplot(x = 'hour',data = prt_gdf,ax=axss[0,1])\nsns.histplot(prt_gdf['duration'],bins=80, kde=False, color='#2a9d8f',ax = axss[1,0])\nsns.countplot(x = 'Location Type',data = prt_gdf,ax=axss[1,1])\n\n\n&lt;Axes: xlabel='Location Type', ylabel='count'&gt;"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-can-i-find-a-bar-clustering-visualization",
    "href": "posts/NY/550 Final.html#where-can-i-find-a-bar-clustering-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where Can I Find a Bar? – Clustering Visualization",
    "text": "Where Can I Find a Bar? – Clustering Visualization\nBased on cluster analysis, we are able to provide travelers with Airbnb location options that meet their needs for different needs. For example, if our traveler is a person who does not require a lot of accommodation but wants to go to a bar in the evening and wants to take a taxi home quickly, we analyzed the clustering of bar and Uber related metrics and found that areas with label 3 and 4 are very suitable for the location where he/she is going to visit.\n\n\nCode\npbua_nb_gdf.groupby(\"travel_bar\", as_index=False)[['num_uber','num_bar','num_party','duration']].mean().sort_values(by=\"travel_bar\")\n\n\n\n\n\n\n\n\n\ntravel_bar\nnum_uber\nnum_bar\nnum_party\nduration\n\n\n\n\n0\n0\n809.397436\n9.487179\n780.089744\n142.621888\n\n\n1\n1\n100.377049\n6.524590\n733.622951\n205.199978\n\n\n2\n2\n763.187500\n27.875000\n3645.875000\n149.463024\n\n\n3\n3\n22529.000000\n69.500000\n2057.250000\n119.591318\n\n\n4\n4\n5410.800000\n106.000000\n2492.000000\n147.806346\n\n\n\n\n\n\n\n\n\nCode\n# setup the figure\nf, ax = plt.subplots(figsize=(10, 8))\n\n# plot, coloring by label column\n# specify categorical data and add legend\npbua_nb_gdf.plot(\n    column=\"travel_bar\",\n    cmap=\"Dark2\",\n    categorical=True,\n    legend=True,\n    edgecolor=\"k\",\n    lw=0.5,\n    ax=ax,\n)\n\n\nax.set_axis_off()\nplt.axis(\"equal\");"
  },
  {
    "objectID": "posts/NY/550 Final.html#where-can-i-find-a-quite-airbnb-clustering-visualization",
    "href": "posts/NY/550 Final.html#where-can-i-find-a-quite-airbnb-clustering-visualization",
    "title": "How to Find My Bar in New York?",
    "section": "Where Can I Find A Quite Airbnb? – Clustering Visualization",
    "text": "Where Can I Find A Quite Airbnb? – Clustering Visualization\nLet’s take another example. Emily wants to spend a nice weekend in New York City, but she wants to avoid too many bars in the neighborhood because they are loud and potentially dangerous. On the other hand, Emily doesn’t have a big budget, so she doesn’t want to spend too much money on Airbnb. From the cluster analysis of ‘Airbnb-Bar’, we can find that the area represented by cluster 2 meets Emily’s needs. Overall, from the map, the intersection of Bronx Grove and Queens would be a great residential option for Emily.\n\n\nCode\npbua_nb_gdf.groupby(\"bar_air\", as_index=False)[['num_party','num_calls','num_bar','price','number_of_reviews','reviews_per_month']].mean().sort_values(by=\"bar_air\")\n\n\n\n\n\n\n\n\n\nbar_air\nnum_party\nnum_calls\nnum_bar\nprice\nnumber_of_reviews\nreviews_per_month\n\n\n\n\n0\n0\n3594.937500\n39.004224\n29.125000\n112.422663\n24.804384\n1.061521\n\n\n1\n1\n933.514286\n34.081496\n5.942857\n91.453277\n38.928529\n2.204802\n\n\n2\n2\n2465.166667\n33.000383\n107.333333\n180.279458\n23.902023\n1.028439\n\n\n3\n3\n994.100000\n27.607487\n24.800000\n209.540961\n18.689871\n0.836661\n\n\n4\n4\n686.850575\n32.515925\n6.643678\n95.245483\n19.849280\n1.219340\n\n\n\n\n\n\n\n\n\nCode\n# setup the figure\nf, ax = plt.subplots(figsize=(10, 8))\n\n# plot, coloring by label column\n# specify categorical data and add legend\npbua_nb_gdf.plot(\n    column=\"bar_air\",\n    cmap=\"Dark2\",\n    categorical=True,\n    legend=True,\n    edgecolor=\"k\",\n    lw=0.5,\n    ax=ax,\n)\n\n\nax.set_axis_off()\nplt.axis(\"equal\");"
  },
  {
    "objectID": "posts/NY/550 Final.html#interactive-airbnb-location-selection-tools",
    "href": "posts/NY/550 Final.html#interactive-airbnb-location-selection-tools",
    "title": "How to Find My Bar in New York?",
    "section": "Interactive Airbnb Location Selection Tools",
    "text": "Interactive Airbnb Location Selection Tools\nAnd similarly, we provide an interactive map of the specific locations of Airbnb’s in the community, where visitors can see the specific prices and number and frequency of reviews of Airbnb’s in their preferred neighborhood, which can further help them make the right choice for them.\n\n\nCode\npn.extension(\"tabulator\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nair = air_gdf.sjoin(pbua_nb_gdf,how='inner')\n\n\n\n\nCode\nnt_names = list(pbua_nb_gdf['ntaname'].unique())\n\nneighborhoodSelect = pn.widgets.Select(\n    value=\"St. Albans\", options=nt_names, name=\"Neighborhood\"\n)\n\nneighborhoodSelect\n\n\n\n\nCode\ndef filter_by_neighborhood(data, neighborhood_name):\n    sel = data[\"ntaname\"] == neighborhood_name\n    return data.loc[sel]\n\ndef airbnb_data(data, neighborhood_name):\n    sel = nbhd_gdf[\"ntaname\"] == neighborhood_name\n    hood_geo = nbhd_gdf.loc[sel]\n\n    m = hood_geo.explore(\n        style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n        name=\"Neighborhood boundary\",\n        tiles=xyzservices.providers.CartoDB.Voyager,\n    )\n\n    data.explore(\n        m=m,  # Add to the existing map!\n        marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n        marker_type=\"circle_marker\",  # or 'marker' or 'circle'\n        name=\"Tickets\",\n    )\n    return m\n\ndef create_dashboard_1(neighborhood_name):\n    tickets = filter_by_neighborhood(air, neighborhood_name)\n    m = airbnb_data(tickets, neighborhood_name)\n    return pn.pane.plot.Folium(m, height=600)\n\n\n\n\nCode\nticket_dashboard_1 = pn.Column(\n    pn.Column(\"## Airbnb in Your Neighborhood\", neighborhoodSelect),\n    # Add a height spacer\n    pn.Spacer(height=45),\n    # Bottom: the main chart, bind widgets to the function\n    pn.bind(create_dashboard_1, neighborhood_name=neighborhoodSelect),\n)\n\nticket_dashboard_1"
  },
  {
    "objectID": "posts/NY/550 Final.html#general-feature-of-airbnb-in-one-neighborhood",
    "href": "posts/NY/550 Final.html#general-feature-of-airbnb-in-one-neighborhood",
    "title": "How to Find My Bar in New York?",
    "section": "General Feature of Airbnb in One Neighborhood",
    "text": "General Feature of Airbnb in One Neighborhood\nBased on the above analysis, we have been able to provide different tourists with the range of Airbnb’s they need for their choice of community. But again, the generalization about Airbnb’s within such a community is something we would like to describe to visitors. Therefore, we have selected the names of Airbnb within the community range (as they contain some attractive features of the listings) for word cloud analysis to get the common features of Airbnb within a community to help the tourists in further screening.\n\n\nCode\ndef fnstring(name):\n    fi = air.loc[air['ntaname'] == name]\n    str_list = fi['name'].astype(str).tolist()\n    combined_string = ' '.join(str_list)\n    return combined_string\n\n\n\n\nCode\ndef wcloud(name):\n    wc = WordCloud(\n    background_color=\"black\", max_words=100, width=1000, height=500, colormap=\"tab20c\"\n)\n    text = fnstring(name)\n    img = wc.generate(text)\n    fig, ax = plt.subplots()\n    ax.imshow(img, interpolation=\"bilinear\")\n    ax.set_axis_off()\n    plt.show();\n\n\n\n\nCode\ntemp = pn.Column(\n    pn.Column(\"## Airbnb in Your Neighborhood\", neighborhoodSelect),\n    # Add a height spacer\n    pn.Spacer(height=45),\n    # Bottom: the main chart, bind widgets to the function\n    pn.bind(wcloud, name=neighborhoodSelect),\n)\ntemp"
  },
  {
    "objectID": "posts/design/index.html",
    "href": "posts/design/index.html",
    "title": "Landscape Architecture & Urban Design Work",
    "section": "",
    "text": "If want to see the complete version of my portfolio, please check the link and see the full working sample.\nSince the outbreak of COVID-19, the social distance kept between each other seems to become the new norm of life. In the mid-summer of 2020, I went out to a park and lay down on the open sloped lawn. I felt that I was no longer a spec of sand in isolation. At that moment, I realized that the environment that I had taken for granted had, in a subtle way, provided so many windows to connect with the world, and there were more subjects in the world sharing this beauty together.\n\nAs a medium, the landscape provides the possibility of sharing stories in a temporal dimension. the landscape in this process is like a window that builds up a temporal connection, reflecting the transparent superimposed relationship of different historical processes. My hometown’s 3000 years historical memory has been lost in the washout of commercialization. The urban renewal should not simply function as a fresh new sheet, but a palimpsest, an overlay of new and old footprint, so that the future can have the opportunity to dialogue with the past through the gap between the layers. Therefore, in the design of the Palace Ruins Park, through the secondary transformation of the sound generated by the users’ activities at present, I built a temporal connection with the sound generated by the ritual procedures that took place in the past palace, and therefore reveal the transparency of the site in the temporal dimension.\n\n\nLikewise, landscape as a medium offers the opportunity of sharing space and function for different users. Unfortunately, during urbanization, the social stratification of the landscape is happening inexorably. In the present day, all participants in urban activities have unconsciously become recipients of that information, leading to the spontaneity of the people involved in public activities. In the project on urban agriculture, I learned that rural community has lost their farming land during urbanization and gentrification. More and more vulnerable groups lost their right to live freely and had to follow the direction of the social hierarchical order implied by the established space. The rediscovery of the existing urban space should give a new meaning to the public space that was considered as low value. By combining urban agriculture and transportation trails, I created re-employment for lost-land farmers and offered the possibility of a space for all people to engage in activities that eliminate class differences.\n\nMy academic experience in landscape has provided me a chance to reflect on the drawbacks of my home city’s development. I always remember the delightful memory of weekend hiking and the pure environment when I was a kid. Meanwhile, I will never forget the children’s running and climbing in the temporary installation that I constructed, which reminds me what contribution I can make to society. In the future, my goal is to go back to my home city and create public space without class differences and time limits."
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html",
    "href": "posts/TOD/MUSA508_Ass2_new.html",
    "title": "TOD Development – Washington DC for example",
    "section": "",
    "text": "Washington DC has a mature rail transit system, which provides the city with highly possibility of TOD development. Therefore, we take Washington DC as our analysis site to explore the current situation of region surrounding station and if DC has potential for TOD development in the future.\n\n\nTo make further analysis, we select some of the variables, including population, median rent, median household income and etc, and use the ACS-5 data in 2009 and 2019 as our raw data source. Besides basic variable, we create new varialbes like percentage of Bachelors or more and percentage of white to have more comprehensive analysis.\n\n\nCode\nacs_variable_list.2019 &lt;- load_variables(2019, #year\n                                         \"acs5\", #five year ACS estimates\n                                         cache = TRUE)\n\nacs_variable_list.2009 &lt;- load_variables(2009, #year\n                                         \"acs5\", #five year ACS estimates\n                                         cache = TRUE)\n\n\n\n\nCode\ntracts19 &lt;-  \n  get_acs(geography = \"tract\",\n          variables = c(\"B25026_001E\",\"B02001_002E\",\n                        \"B15001_050E\",\"B15001_009E\",\n                        \"B19013_001E\", \"B25058_001E\",\n                        \"B06012_002E\"), \n          year=2019, state=11,\n          geometry=TRUE) %&gt;% \n  st_transform('ESRI:102728')\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\nCode\ntracts19 &lt;- \n  tracts19 %&gt;%\n  dplyr::select( -NAME, -moe) %&gt;%\n  spread(key = variable, value = estimate) %&gt;%\n  rename(TotalPop = B25026_001, \n         Whites = B02001_002,\n         FemaleBachelors = B15001_050, \n         MaleBachelors = B15001_009,\n         MedHHInc = B19013_001, \n         MedRent = B25058_001,\n         TotalPoverty = B06012_002)\n\ntracts19 &lt;- \n  tracts19 %&gt;%\n  mutate(pctWhite = ifelse(TotalPop &gt; 0, Whites / TotalPop, 0),\n         pctBachelors = ifelse(TotalPop &gt; 0, ((FemaleBachelors + MaleBachelors) / TotalPop), 0),\n         pctPoverty = ifelse(TotalPop &gt; 0, TotalPoverty / TotalPop, 0),\n         year = \"2019\") %&gt;%\n  dplyr::select(-Whites,-FemaleBachelors,-MaleBachelors,-TotalPoverty)\n\n\n\n\nCode\ntracts09 &lt;- \n  get_acs(geography = \"tract\", \n          variables = c(\"B25026_001E\",\"B02001_002E\",\n                        \"B15001_050E\",\"B15001_009E\",\n                        \"B19013_001E\",\"B25058_001E\",\n                        \"B06012_002E\"), \n          year=2009, state=11, \n          geometry=TRUE, output=\"wide\") %&gt;%\n  st_transform('ESRI:102728') %&gt;%\n  rename(TotalPop = B25026_001E, \n         Whites = B02001_002E,\n         FemaleBachelors = B15001_050E, \n         MaleBachelors = B15001_009E,\n         MedHHInc = B19013_001E, \n         MedRent = B25058_001E,\n         TotalPoverty = B06012_002E) %&gt;%\n  dplyr::select(-NAME, -starts_with(\"B\")) %&gt;%\n  mutate(pctWhite = ifelse(TotalPop &gt; 0, Whites / TotalPop,0),\n         pctBachelors = ifelse(TotalPop &gt; 0, ((FemaleBachelors + MaleBachelors) / TotalPop),0),\n         pctPoverty = ifelse(TotalPop &gt; 0, TotalPoverty / TotalPop, 0),\n         year = \"2009\") %&gt;%\n  dplyr::select(-Whites, -FemaleBachelors, -MaleBachelors, -TotalPoverty) \n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\nCode\nallTracts &lt;- rbind(tracts19,tracts09)\n\n\nTo understand if the station has indication to the public safety, we also collect crime dataset from DC gov website to join the existing social and economic dataset.\n\n\nCode\ncrime19 &lt;- st_read(\"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?outFields=*&where=1%3D1&f=geojson\") %&gt;%\n  dplyr::select(OFFENSE, CENSUS_TRACT, OBJECTID) %&gt;%\n  st_transform('ESRI:102728')  \n\n\nReading layer `OGRGeoJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?outFields=*&where=1%3D1&f=geojson' \n  using driver `GeoJSON'\nSimple feature collection with 33969 features and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.11414 ymin: 38.81939 xmax: -76.91002 ymax: 38.9937\nGeodetic CRS:  WGS 84\n\n\nCode\ncrime09 &lt;- st_read(\"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/33/query?outFields=*&where=1%3D1&f=geojson\") %&gt;%\n  dplyr::select(OFFENSE, CENSUS_TRACT, OBJECTID) %&gt;%\n  st_transform('ESRI:102728') \n\n\nReading layer `OGRGeoJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/33/query?outFields=*&where=1%3D1&f=geojson' \n  using driver `GeoJSON'\nSimple feature collection with 31354 features and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.11276 ymin: 38.81348 xmax: -76.91002 ymax: 38.99422\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\ncrime_counts_by_tract_19 &lt;- allTracts %&gt;%\n  st_intersection(crime19) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(crime_counts=n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(year=\"2019\")\n\ncrime_counts_by_tract_09 &lt;- allTracts %&gt;%\n  st_intersection(crime09) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(crime_counts=n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(year=\"2009\")\n\ncrime_counts_all &lt;- rbind(crime_counts_by_tract_09, crime_counts_by_tract_19)\nallTracts &lt;- left_join(allTracts, crime_counts_all, by=c(\"GEOID\"=\"GEOID\", \"year\"=\"year\"))\n\n\n\n\n\nDue to Washington DC has mature and widespread metro system, we select DC’s metro station data, which is also from DC gov website, as our transportation element in the TOD analysis.\n\n\nCode\ndc_station &lt;- st_read(\"https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/52/query?where=1%3D1&outFields=*&outSR=4326&f=json\") %&gt;%\n  dplyr::select(NAME,LINE) %&gt;%\n  st_transform('ESRI:102728')  \n\n\nReading layer `ESRIJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/52/query?where=1%3D1&outFields=*&outSR=4326&f=json' \n  using driver `ESRIJSON'\nSimple feature collection with 40 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.085 ymin: 38.84567 xmax: -76.93526 ymax: 38.97609\nGeodetic CRS:  WGS 84\n\n\nCode\ndc_line &lt;- st_read('https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/106/query?outFields=*&where=1%3D1&f=geojson') %&gt;%\n  dplyr::select(NAME,GIS_ID)%&gt;%\n  st_transform('ESRI:102728')\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/106/query?outFields=*&where=1%3D1&f=geojson' \n  using driver `GeoJSON'\nSimple feature collection with 6 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -77.08577 ymin: 38.83827 xmax: -76.91328 ymax: 38.97979\nGeodetic CRS:  WGS 84\n\n\nFrom the map of station and line distribution, we can find that the stations mainly locate in the north part of DC, diffusing from the center. The lines build a continous connection from the north-west to south-east, while having some connections from north to the central part of DC.\n\n\nCode\nggplot() +\n  geom_sf(data = st_union(tracts19)) +\n  geom_sf(data = dc_station,\n          aes(color = LINE),\n          show.legend = 'Point',size=2) +\n  geom_sf(data = dc_line,\n          aes(color = GIS_ID),\n          show.legend = 'Line',size=1)\n\n\n\n\n\nCode\n  labs(title = 'Station stops',\n       subtitle = 'Washington DC',\n       caption = 'Figure 1,1')+\n  scale_color_manual(values = c(\"blue\",\"green\",\"orange\",\"red\",\"#C0C0C0\",\"yellow\"),name=\"Metro Line\")\n\n\nNULL"
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html#social-and-economic-dataset",
    "href": "posts/TOD/MUSA508_Ass2_new.html#social-and-economic-dataset",
    "title": "TOD Development – Washington DC for example",
    "section": "",
    "text": "To make further analysis, we select some of the variables, including population, median rent, median household income and etc, and use the ACS-5 data in 2009 and 2019 as our raw data source. Besides basic variable, we create new varialbes like percentage of Bachelors or more and percentage of white to have more comprehensive analysis.\n\n\nCode\nacs_variable_list.2019 &lt;- load_variables(2019, #year\n                                         \"acs5\", #five year ACS estimates\n                                         cache = TRUE)\n\nacs_variable_list.2009 &lt;- load_variables(2009, #year\n                                         \"acs5\", #five year ACS estimates\n                                         cache = TRUE)\n\n\n\n\nCode\ntracts19 &lt;-  \n  get_acs(geography = \"tract\",\n          variables = c(\"B25026_001E\",\"B02001_002E\",\n                        \"B15001_050E\",\"B15001_009E\",\n                        \"B19013_001E\", \"B25058_001E\",\n                        \"B06012_002E\"), \n          year=2019, state=11,\n          geometry=TRUE) %&gt;% \n  st_transform('ESRI:102728')\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\nCode\ntracts19 &lt;- \n  tracts19 %&gt;%\n  dplyr::select( -NAME, -moe) %&gt;%\n  spread(key = variable, value = estimate) %&gt;%\n  rename(TotalPop = B25026_001, \n         Whites = B02001_002,\n         FemaleBachelors = B15001_050, \n         MaleBachelors = B15001_009,\n         MedHHInc = B19013_001, \n         MedRent = B25058_001,\n         TotalPoverty = B06012_002)\n\ntracts19 &lt;- \n  tracts19 %&gt;%\n  mutate(pctWhite = ifelse(TotalPop &gt; 0, Whites / TotalPop, 0),\n         pctBachelors = ifelse(TotalPop &gt; 0, ((FemaleBachelors + MaleBachelors) / TotalPop), 0),\n         pctPoverty = ifelse(TotalPop &gt; 0, TotalPoverty / TotalPop, 0),\n         year = \"2019\") %&gt;%\n  dplyr::select(-Whites,-FemaleBachelors,-MaleBachelors,-TotalPoverty)\n\n\n\n\nCode\ntracts09 &lt;- \n  get_acs(geography = \"tract\", \n          variables = c(\"B25026_001E\",\"B02001_002E\",\n                        \"B15001_050E\",\"B15001_009E\",\n                        \"B19013_001E\",\"B25058_001E\",\n                        \"B06012_002E\"), \n          year=2009, state=11, \n          geometry=TRUE, output=\"wide\") %&gt;%\n  st_transform('ESRI:102728') %&gt;%\n  rename(TotalPop = B25026_001E, \n         Whites = B02001_002E,\n         FemaleBachelors = B15001_050E, \n         MaleBachelors = B15001_009E,\n         MedHHInc = B19013_001E, \n         MedRent = B25058_001E,\n         TotalPoverty = B06012_002E) %&gt;%\n  dplyr::select(-NAME, -starts_with(\"B\")) %&gt;%\n  mutate(pctWhite = ifelse(TotalPop &gt; 0, Whites / TotalPop,0),\n         pctBachelors = ifelse(TotalPop &gt; 0, ((FemaleBachelors + MaleBachelors) / TotalPop),0),\n         pctPoverty = ifelse(TotalPop &gt; 0, TotalPoverty / TotalPop, 0),\n         year = \"2009\") %&gt;%\n  dplyr::select(-Whites, -FemaleBachelors, -MaleBachelors, -TotalPoverty) \n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n\nCode\nallTracts &lt;- rbind(tracts19,tracts09)\n\n\nTo understand if the station has indication to the public safety, we also collect crime dataset from DC gov website to join the existing social and economic dataset.\n\n\nCode\ncrime19 &lt;- st_read(\"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?outFields=*&where=1%3D1&f=geojson\") %&gt;%\n  dplyr::select(OFFENSE, CENSUS_TRACT, OBJECTID) %&gt;%\n  st_transform('ESRI:102728')  \n\n\nReading layer `OGRGeoJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/1/query?outFields=*&where=1%3D1&f=geojson' \n  using driver `GeoJSON'\nSimple feature collection with 33969 features and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.11414 ymin: 38.81939 xmax: -76.91002 ymax: 38.9937\nGeodetic CRS:  WGS 84\n\n\nCode\ncrime09 &lt;- st_read(\"https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/33/query?outFields=*&where=1%3D1&f=geojson\") %&gt;%\n  dplyr::select(OFFENSE, CENSUS_TRACT, OBJECTID) %&gt;%\n  st_transform('ESRI:102728') \n\n\nReading layer `OGRGeoJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/FEEDS/MPD/MapServer/33/query?outFields=*&where=1%3D1&f=geojson' \n  using driver `GeoJSON'\nSimple feature collection with 31354 features and 23 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.11276 ymin: 38.81348 xmax: -76.91002 ymax: 38.99422\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\ncrime_counts_by_tract_19 &lt;- allTracts %&gt;%\n  st_intersection(crime19) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(crime_counts=n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(year=\"2019\")\n\ncrime_counts_by_tract_09 &lt;- allTracts %&gt;%\n  st_intersection(crime09) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(crime_counts=n()) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(year=\"2009\")\n\ncrime_counts_all &lt;- rbind(crime_counts_by_tract_09, crime_counts_by_tract_19)\nallTracts &lt;- left_join(allTracts, crime_counts_all, by=c(\"GEOID\"=\"GEOID\", \"year\"=\"year\"))"
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html#station-dataset",
    "href": "posts/TOD/MUSA508_Ass2_new.html#station-dataset",
    "title": "TOD Development – Washington DC for example",
    "section": "",
    "text": "Due to Washington DC has mature and widespread metro system, we select DC’s metro station data, which is also from DC gov website, as our transportation element in the TOD analysis.\n\n\nCode\ndc_station &lt;- st_read(\"https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/52/query?where=1%3D1&outFields=*&outSR=4326&f=json\") %&gt;%\n  dplyr::select(NAME,LINE) %&gt;%\n  st_transform('ESRI:102728')  \n\n\nReading layer `ESRIJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/52/query?where=1%3D1&outFields=*&outSR=4326&f=json' \n  using driver `ESRIJSON'\nSimple feature collection with 40 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -77.085 ymin: 38.84567 xmax: -76.93526 ymax: 38.97609\nGeodetic CRS:  WGS 84\n\n\nCode\ndc_line &lt;- st_read('https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/106/query?outFields=*&where=1%3D1&f=geojson') %&gt;%\n  dplyr::select(NAME,GIS_ID)%&gt;%\n  st_transform('ESRI:102728')\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://maps2.dcgis.dc.gov/dcgis/rest/services/DCGIS_DATA/Transportation_Rail_Bus_WebMercator/MapServer/106/query?outFields=*&where=1%3D1&f=geojson' \n  using driver `GeoJSON'\nSimple feature collection with 6 features and 10 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -77.08577 ymin: 38.83827 xmax: -76.91328 ymax: 38.97979\nGeodetic CRS:  WGS 84\n\n\nFrom the map of station and line distribution, we can find that the stations mainly locate in the north part of DC, diffusing from the center. The lines build a continous connection from the north-west to south-east, while having some connections from north to the central part of DC.\n\n\nCode\nggplot() +\n  geom_sf(data = st_union(tracts19)) +\n  geom_sf(data = dc_station,\n          aes(color = LINE),\n          show.legend = 'Point',size=2) +\n  geom_sf(data = dc_line,\n          aes(color = GIS_ID),\n          show.legend = 'Line',size=1)\n\n\n\n\n\nCode\n  labs(title = 'Station stops',\n       subtitle = 'Washington DC',\n       caption = 'Figure 1,1')+\n  scale_color_manual(values = c(\"blue\",\"green\",\"orange\",\"red\",\"#C0C0C0\",\"yellow\"),name=\"Metro Line\")\n\n\nNULL"
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html#tod-dataset-create",
    "href": "posts/TOD/MUSA508_Ass2_new.html#tod-dataset-create",
    "title": "TOD Development – Washington DC for example",
    "section": "TOD dataset create",
    "text": "TOD dataset create\nBased on the selected defining method, we set TOD label for ACS-5 dataset. Considering the inflation, we make inflation adjustment on the median rent and median household income in the dataset for better comparison.\n\n\nCode\nallTracts.group &lt;- \n  rbind(\n    st_centroid(allTracts)[buffer,] %&gt;%\n      st_drop_geometry() %&gt;%\n      left_join(allTracts) %&gt;%\n      st_sf() %&gt;%\n      mutate(TOD = \"TOD\"),\n    st_centroid(allTracts)[buffer, op = st_disjoint] %&gt;%\n      st_drop_geometry() %&gt;%\n      left_join(allTracts) %&gt;%\n      st_sf() %&gt;%\n      mutate(TOD = \"Non-TOD\")) %&gt;%\n  mutate(MedRent.inf = ifelse(year == \"2009\", MedRent * 1.19, MedRent))%&gt;%\n  mutate(MedHHInc.inf = ifelse(year == \"2009\", MedHHInc *1.19, MedHHInc)) \n\n\n\n\nCode\ncentroid_all &lt;- st_centroid(allTracts.group, of_largest_polygon = TRUE)\n\n\nMeanwhile, we filter the part with ‘TOD’ label in dataset for further deeper analysis in the TOD region.\n\n\nCode\nTod_region &lt;- allTracts.group %&gt;%\n  select(TOD) %&gt;%\n  filter(TOD ==\"TOD\") %&gt;%\n  st_union() %&gt;%\n  st_sf()"
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html#social-and-economic-analysis-based-on-tod",
    "href": "posts/TOD/MUSA508_Ass2_new.html#social-and-economic-analysis-based-on-tod",
    "title": "TOD Development – Washington DC for example",
    "section": "Social and economic analysis based on TOD",
    "text": "Social and economic analysis based on TOD\nBefore discussing the impacts of railway station to surrounding tracts, we figure out the difference of TOD region between 2009 and 2019. We can find that the most tracts in 2019 kept the same as 2009, except few tracts was excluded due to tracts’ re-split. Overall, the TOD region kept the same from 2009 to 2019.\n\n\nCode\nggplot(allTracts.group)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill = TOD))+\n  scale_fill_manual(values = c(\"grey\", \"yellow\"))+\n  labs(title = \"TOD and Non-TOD Census Tracts in Washington DC\",\n       caption = \"Figure 2.1\")+\n  facet_wrap(~year)+\n  mapTheme()\n\n\n\n\n\nFrom the map of population distribution, we can find that DC has more population in 2019 in general. Besides the north-west and south part, the north-east part had trend to become another populations agglomeration. However,there’s not obvious difference in the population change in the TOD area.\n\n\nCode\n#Total Population within the TOD & Non-TOD tracts from 2009 to 2019\nggplot(allTracts.group)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill=q5(TotalPop)))+\n  geom_sf(data = st_union(Tod_region), color = \"red\",fill=\"transparent\", size = 200)+\n  scale_fill_manual(values = palette5,\n                    labels= qBr(allTracts.group, \"TotalPop\"),\n                    name = \"Population\\n(Quintile Breaks)\")+\n    labs(title = \"Total Population by census tracts, 2009-2019\",\n         subtitle = \"Washington DC\",\n       caption = \"Figure 2.2\")+\n  facet_wrap(~year)+\n  mapTheme()\n\n\n\n\n\nWhen it comes to the median household income, we can easily find that the north-west part have a average higher income level, which kept the same situation from 2009 to 2019. Focusing on the TOD region, we can find that in the center part of TOD region had an apparent median household income increase comapred to the surrounding areas.\n\n\nCode\n#Median household income within the TOD & Non-TOD tracts from 2009 to 2019\nggplot(allTracts.group)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill=q5(MedHHInc.inf)))+\n  geom_sf(data = st_union(Tod_region), color = \"red\",fill=\"transparent\", size = 9)+\n  scale_fill_manual(values = palette5,\n                    labels= qBr(allTracts.group, \"MedHHInc.inf\"),\n                    name = \"Median Household Income($)\\n(Quintile Breaks)\\n(Inflation adjusted to 2019)\")+\n    labs(title = \"Median Household Income by census tracts, 2009-2019\",\n         subtitle = \"Washington DC\",\n       caption = \"Figure 2.3\")+\n  facet_wrap(~year)+\n  mapTheme()\n\n\n\n\n\nCompared to change in median household income, the high-education group (the group have the bachelors or higher degree) distribution had an interesting change in the decade. Compared to 2009, the high-education group gradually moved to the central part of DC from north-west part. Despite not much strong evidence showing the influence of TOD, the central part, which locates many transfer station, still had potential influence of gathering of people.\n\n\nCode\n#Percentage of bachelors within the TOD & Non-TOD tracts from 2009 to 2019\nggplot(allTracts.group)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill=q5(pctBachelors)))+\n  geom_sf(data = st_union(Tod_region), color = \"red\",fill=\"transparent\", size = 9)+\n  scale_fill_manual(values = palette5,\n                    labels= newqBr(allTracts.group, \"pctBachelors\"),\n                    name = \"Percentage of Bachelors\\n(Quintile Breaks)\")+\n    labs(title = \"Percentage of Bachelors by census tracts, 2009-2019\",\n         subtitle = \"Washington DC\",\n       caption = \"Figure 2.4\")+\n  facet_wrap(~year)+\n  mapTheme()\n\n\n\n\n\nThe change in median rent price has a similar trend like change in median household income. In general, whole DC experienced a rent increase in the decade, with north part owning a higher median rent price. Also, the region in the influence of TOD tend to have a higehr price in 2019 comapred to 2009.\n\n\nCode\n#Median Rent within the TOD & Non-TOD tracts from 2009 to 2019\nggplot(allTracts.group)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill=q5(MedRent.inf)))+\n  geom_sf(data = st_union(Tod_region), color = \"red\",fill=\"transparent\", size = 9)+\n  scale_fill_manual(values = palette5,\n                    labels= newqBr(allTracts.group, \"MedRent.inf\"),\n                    name = \"Median Rent($)\\n(Quintile Breaks)\\n(Inflation adjusted to 2019)\")+\n    labs(title = \"Median Rent by census tracts, 2009-2019\",\n         subtitle = \"Washington DC\",\n       caption = \"Figure 2.5\")+\n  facet_wrap(~year)+\n  mapTheme()\n\n\n\n\n\nConcerning public safety, we can find that crime counts had a slight increase in the decade, concentrating in the central and east part of DC. As the trasfer station located in the central part before, we can assume that the TOD could also increase the crime possibility in the radius area.\n\n\nCode\n#Crime counts within the TOD & Non-TOD tracts from 2009 to 2019\nggplot(allTracts.group)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill=q5(crime_counts)))+\n  geom_sf(data = st_union(Tod_region), color = \"red\",fill=\"transparent\", size = 9)+\n  scale_fill_manual(values = palette5,\n                    labels= newqBr(allTracts.group, \"crime_counts\"),\n                    name = \"Crime Counts\\n(Quintile Breaks)\")+\n    labs(title = \"Crime Counts by census tracts, 2009-2019\",\n         subtitle = \"Washington DC\",\n       caption = \"Figure 2.6\")+\n  facet_wrap(~year)+\n  mapTheme()"
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html#station-centroid-visualization",
    "href": "posts/TOD/MUSA508_Ass2_new.html#station-centroid-visualization",
    "title": "TOD Development – Washington DC for example",
    "section": "station-centroid visualization",
    "text": "station-centroid visualization\nBesides the analysis focusing on the tracts influenced by TOD, we also want to figure out the capability of different stations in gathering people and influencing social economic situation. Therefore, we select two indicators, including population and median rent price, to analyze the situation.\n\nPopulation analysis\nTo get the influence based on station, we use spatial join to get the tracts that in each station, and then sum up the population based on the station’s name. From the map, we can find that the station in the central part of DC gathered more people than the surrounding areas. The phenomenon could result from the gathering influence of transfer station in this region. What’s more, the central part also had more increase in total population than surrounding areas.\n\n\nCode\nbuffer_new&lt;- st_buffer(dc_station,2640)%&gt;%\n  dplyr::select(NAME)\n\npop_area &lt;- st_join(buffer_new,allTracts%&gt;%select(TotalPop,year))\n\npop_area_sum &lt;- pop_area%&gt;%\n  group_by(NAME,year)%&gt;%\n  summarise(pop = sum(TotalPop))%&gt;%\n  st_drop_geometry()\n\nstation_new &lt;- left_join(dc_station,pop_area_sum,by='NAME')\n\n\n\n\nCode\n# population\nggplot() +\n  geom_sf(data = allTracts.group,fill='grey40') + \n  geom_sf(data = Tod_region, color = \"white\",fill=\"transparent\", size = 1)+\n    geom_sf(data = station_new,\n          pch = 21,\n          aes(size=pop),\n          fill=alpha('red',0.7),\n          col = 'grey20') +\n  labs(title = 'Population Graduated Symbol Maps in 2009 and 2019',\n       caption = 'Figure 3.1')+\n  facet_wrap(~year) +\n  scale_size(range = c(1,5))\n\n\n\n\n\n\n\nMedian Rent analysis\nHaving the similar data process step with population analysis one, we can find that the whole TOD region had an obvious median rent increase. When we look at the increase trend, we can find that the center to south-east TOD part have a higher median rent price increase compared to the north-west part, which may result from the the area’s 2009 median rent price were on the high side for DC.\n\n\nCode\nrent_area &lt;- st_join(buffer_new,allTracts%&gt;%select(MedRent,year))\nrent_area[is.na(rent_area)] &lt;- 0\n\nrent_area_sum &lt;- rent_area%&gt;%\n  group_by(NAME,year)%&gt;%\n  summarise(rent = mean(MedRent))%&gt;%\n  st_drop_geometry()\n\nstation_new2 &lt;- left_join(dc_station,rent_area_sum,by='NAME')\n\n\n\n\nCode\n# rent\nggplot() +\n  geom_sf(data = allTracts.group,fill='grey40') + \n  geom_sf(data = Tod_region, color = \"white\",fill=\"transparent\", size = 1)+\n    geom_sf(data = station_new2,\n          pch = 21,\n          aes(size=rent),\n          fill=alpha('red',0.7),\n          col = 'grey20') +\n    labs(title = ' Median Rent Graduated Symbol Maps in 2009 and 2019',\n       caption = 'Figure 3.2')+\n  facet_wrap(~year) +\n  scale_size(range = c(1,5))"
  },
  {
    "objectID": "posts/TOD/MUSA508_Ass2_new.html#the-analysis-of-the-distance-to-tod",
    "href": "posts/TOD/MUSA508_Ass2_new.html#the-analysis-of-the-distance-to-tod",
    "title": "TOD Development – Washington DC for example",
    "section": "The analysis of the distance to TOD",
    "text": "The analysis of the distance to TOD\nAfter the analysis for TOD radius areas, we try to make an analysis that if the distance from station have some continuous influence to social economic indicator. We make multi buffer based on the station, and then use the center point of tracts to represent the data in each tracts. Based on the points location relationship to the buffer rings, we label the distances information in each tracts dataset and make further analysis.\n\n\nCode\nstation_ring &lt;- multipleRingBuffer(st_union(dc_station), 2640*9, 2640)\n\nallTracts.rings &lt;-\n  st_join(st_centroid(dplyr::select(allTracts.group, GEOID, year)),\n          station_ring) %&gt;%\n  st_drop_geometry() %&gt;%\n  left_join(dplyr::select(allTracts.group, GEOID, MedRent, year), \n            by=c(\"GEOID\"=\"GEOID\", \"year\"=\"year\")) %&gt;%\n  st_sf() %&gt;%\n  mutate(distance = distance / 5280)\n\n\n\n\nCode\nggplot() +\n    geom_sf(data=station_ring,aes(color=distance)) +\n  scale_color_gradient()+\n    geom_sf(data=dc_station, size=1) +\n    geom_sf(data=st_union(allTracts.rings), fill=NA, color=\"red\",size=2) +\n    labs(title=\"Station Stops: Half Mile Buffers\",\n         subtitle = \"Washington DC\",\n         caption = \"Figure 3.3\") +\n    mapTheme()\n\n\n\n\n\n\n\nCode\nggplot(allTracts.rings)+\n  geom_sf(data = st_union(tracts19))+\n  geom_sf(aes(fill = as.factor(distance)))+\n  geom_sf(data = Tod_region, color= \"#de660c\",fill=\"transparent\", size =9)+\n  scale_fill_brewer(palette = \"YlGnBu\",\n                      name = \"Distance to Subway Stations\")+\n  #distiller -&gt; 用来做continuous palette, brewer-&gt;discrete \n    labs(title = \"Distance to Subway Stations by census tracts\",\n         subtitle = \"Washington DC\",\n       caption = \"Figure 3.4\")+\n  mapTheme()\n\n\n\n\n\nFrom the line chart of average median rent in different distances to station, we can find that the rent had a general increase from 2009 to 2019, which also show the same trend that the rent price gradually decrease when far from station at the beginning, and then increase to the peak when the distance comes to 2 to 2.5 miles. Due to the tract data which distance to TOD is 3.5 miles is Null and filled with 0 in data cleaning, we can still draw the conclusion from existing 2009 data that the rent will decrease again after 2.5 miles.\n\n\nCode\nallTracts.rings[is.na(allTracts.rings)] &lt;- 0\nsummary &lt;- allTracts.rings%&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(year,distance)%&gt;%\n  summarise(averagerent = mean(MedRent))\n\n\n\n\nCode\nggplot(data=summary,aes(x=distance, y=averagerent, group=year, color=year)) +\n    geom_line() +\n    ggtitle(\"Average rent in differnt distance to station\") +\n    ylab(\"Average rent\")"
  },
  {
    "objectID": "posts/bikeshare/project.html",
    "href": "posts/bikeshare/project.html",
    "title": "Shared Bike Usage Prediciton",
    "section": "",
    "text": "Bikeshare programs is becoming an integral part of urban transportation system. Indego Bike is one of the earliest bicycle sharing system in Philadelphia. Bicycling has become more prevalent and important in Philadelphia because of the city’s smaller center city size and more bike-friendly streets. However, Bikesharing is also affected by how to balance the number of bikes at each station, which can be framed as ‘re-balancing’. This issue arises from the spatial and temporal variations in user demand. If a station has a large number of bikes in use making parking impossible or if there are no bikes available at a certain time due to the time of day this can lead to inefficiencies in sharing bikes. Therefore, the goal of this report is to rapidly predict the demand for bikes across various stations in Philadelphia. We will perform analysis on the historical data on bike usage at different space, time and weather conditions. The forecast allows the Indego Bike to proactively address imbalances, strategically re-positioning bikes in anticipation of peak usage time or special events.\n\n\nCode\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(lubridate)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(viridis)\nlibrary(riem)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(RSocrata)\nlibrary(dplyr)\nlibrary(spdep)\nlibrary(caret)\nlibrary(ckanr)\nlibrary(FNN)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(ggcorrplot) # plot correlation plot\nlibrary(corrr)      # another way to plot correlation plot\nlibrary(kableExtra)\nlibrary(jtools)     # for regression model plots\nlibrary(ggstance) # to support jtools plots\nlibrary(ggpubr)    # plotting R^2 value on ggplot point scatter\nlibrary(broom.mixed) # needed for effects plots\nlibrary(vtable)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(purrr)\n\nsource(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r\")\ndevtools::install_github('thomasp85/gganimate')\n\nplotTheme &lt;- theme(\n  plot.title =element_text(size=12),\n  plot.subtitle = element_text(size=8),\n  plot.caption = element_text(size = 6),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title.y = element_text(size = 10),\n  # Set the entire chart region to blank\n  panel.background=element_blank(),\n  plot.background=element_blank(),\n  #panel.border=element_rect(colour=\"#F0F0F0\"),\n  # Format the grid\n  panel.grid.major=element_line(colour=\"#D0D0D0\",size=.2),\n  axis.ticks=element_blank())\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nCode\nmapTheme &lt;- theme(plot.title =element_text(size=12),\n                  plot.subtitle = element_text(size=8),\n                  plot.caption = element_text(size = 6),\n                  axis.line=element_blank(),\n                  axis.text.x=element_blank(),\n                  axis.text.y=element_blank(),\n                  axis.ticks=element_blank(),\n                  axis.title.x=element_blank(),\n                  axis.title.y=element_blank(),\n                  panel.background=element_blank(),\n                  panel.border=element_blank(),\n                  panel.grid.major=element_line(colour = 'transparent'),\n                  panel.grid.minor=element_blank(),\n                  legend.direction = \"vertical\", \n                  legend.position = \"right\",\n                  plot.margin = margin(1, 1, 1, 1, 'cm'),\n                  legend.key.height = unit(1, \"cm\"), legend.key.width = unit(0.2, \"cm\"))\n\npalette5 &lt;- c(\"#eff3ff\",\"#bdd7e7\",\"#6baed6\",\"#3182bd\",\"#08519c\")\npalette4 &lt;- c(\"#D2FBD4\",\"#92BCAB\",\"#527D82\",\"#123F5A\")\npalette2 &lt;- c(\"#6baed6\",\"#08519c\")"
  },
  {
    "objectID": "posts/bikeshare/project.html#indego-bike-use-data",
    "href": "posts/bikeshare/project.html#indego-bike-use-data",
    "title": "Shared Bike Usage Prediciton",
    "section": "Indego Bike Use Data",
    "text": "Indego Bike Use Data\nThe data of Indego shared bike usage is from the open data in Indego Website. the data include the first quarter usage information in 2023. And the dataset includes variables like time, station, geometry of start and end station of each trip.\n\n\nCode\ndat_1 &lt;- read.csv('/Users/mr.smile/Desktop/UPENN/FALL23/MUSA508/musa_5080_2023-main/bikeshare/data/indego-trips-2023-q1.csv')\ndat_ag &lt;- dat_1\ndat_ag &lt;- na.omit(dat_ag)\n\n\n\n\nCode\ndat_ag_new &lt;- dat_ag\ndat_ag_new &lt;- dat_ag_new %&gt;%\n  mutate(interval60 = floor_date(mdy_hm(start_time), unit = \"hour\"),\n         interval15 = floor_date(mdy_hm(start_time), unit = \"15 mins\"),\n         week = week(interval60),\n         dotw = wday(interval60, label=TRUE))%&gt;%\n  filter(week &lt;= 5)\n\n\n\n\nCode\nphillyCensus &lt;- \n  get_acs(geography = \"tract\", \n          variables = c(\"B01003_001\", \"B19013_001\", \n                        \"B02001_002\", \"B08013_001\",\n                        \"B08012_001\", \"B08301_001\", \n                        \"B08301_010\", \"B01002_001\"), \n          year = 2021, \n          state = \"PA\", \n          geometry = TRUE, \n          county='Philadelphia',\n          output = \"wide\") %&gt;%\n  rename(Total_Pop =  B01003_001E,\n         Med_Inc = B19013_001E,\n         Med_Age = B01002_001E,\n         White_Pop = B02001_002E,\n         Travel_Time = B08013_001E,\n         Num_Commuters = B08012_001E,\n         Means_of_Transport = B08301_001E,\n         Total_Public_Trans = B08301_010E) %&gt;%\n  select(Total_Pop, Med_Inc, White_Pop, Travel_Time,\n         Means_of_Transport, Total_Public_Trans,\n         Med_Age,\n         GEOID, geometry) %&gt;%\n  mutate(Percent_White = White_Pop / Total_Pop,\n         Mean_Commute_Time = Travel_Time / Total_Public_Trans,\n         Percent_Taking_Public_Trans = Total_Public_Trans / Means_of_Transport)"
  },
  {
    "objectID": "posts/bikeshare/project.html#transit-stop-data",
    "href": "posts/bikeshare/project.html#transit-stop-data",
    "title": "Shared Bike Usage Prediciton",
    "section": "transit stop data",
    "text": "transit stop data\nTransit stop data comes from dvrpc. Concerning the concept of last 5 minutes walking distance, it’s reasonable that the shared bike serves as the connection between the destination and transit station. The distance to the closest stop could effect the using frequency of shared bike station. Therefore, we use the bus stop and rail stop data in Philadelphia as the raw data, and then use KNN algorithm to calculate the distance of the shared bike station to the nearest transit stop.\n\n\nCode\nstop &lt;- st_read('https://arcgis.dvrpc.org/portal/rest/services/Transportation/SEPTA_TransitStops/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson')%&gt;%\n  dplyr::select(objectid,lon,lat,mode,geometry) %&gt;%\n  st_transform('ESRI:102728')\n\nstop_bus &lt;- stop %&gt;% filter(mode == 'Bus') \nstop_tro &lt;- stop %&gt;% filter(mode == 'Trolley')\nstop_hs &lt;- stop %&gt;% filter(mode == 'Highspeed')\n\n\n\n\nCode\nphillyTracts &lt;- \n  phillyCensus %&gt;%\n  as.data.frame() %&gt;%\n  distinct(GEOID, .keep_all = TRUE) %&gt;%\n  select(GEOID, geometry) %&gt;% \n  st_sf\n\n\n\n\nCode\ndat_census &lt;- st_join(dat_ag_new %&gt;% \n          filter(is.na(start_lon) == FALSE &\n                   is.na(start_lat) == FALSE &\n                   is.na(end_lat) == FALSE &\n                   is.na(end_lon) == FALSE) %&gt;%\n          st_as_sf(., coords = c(\"start_lon\", \"start_lat\"), crs = 4326),\n        phillyTracts %&gt;%\n          st_transform(crs=4326),\n        join=st_intersects,\n              left = TRUE) %&gt;%\n  rename(start_Tract = GEOID) %&gt;%\n  mutate(start_lon = unlist(map(geometry, 1)),\n         start_lat = unlist(map(geometry, 2)))%&gt;%\n  as.data.frame() %&gt;%\n  select(-geometry)%&gt;%\n  st_as_sf(., coords = c(\"end_lon\", \"end_lat\"), crs = 4326) %&gt;%\n  st_join(., phillyTracts %&gt;%\n            st_transform(crs=4326),\n          join=st_intersects,\n          left = TRUE) %&gt;%\n  rename(end_Tract = GEOID)  %&gt;%\n  mutate(to_lon = unlist(map(geometry, 1)),\n         to_lat = unlist(map(geometry, 2)))%&gt;%\n  as.data.frame() %&gt;%\n  select(-geometry)"
  },
  {
    "objectID": "posts/bikeshare/project.html#weather-data",
    "href": "posts/bikeshare/project.html#weather-data",
    "title": "Shared Bike Usage Prediciton",
    "section": "weather data",
    "text": "weather data\nThe weather data comes from the API provided by package ‘riem’. Due to the using of bike is highly influenced by the weather condition, like rainy or windy situation will obstacle the use of bike. Therefore, it’s necessary to take weather into consideration. I selected the weather data from the time range of the shared bike I select and merge them as a combined dataset.\n\n\nCode\nweather.Panel &lt;- \n  riem_measures(station = \"ORD\", date_start = \"2023-01-01\", date_end = \"2023-02-04\") %&gt;%\n  dplyr::select(valid, tmpf, p01i, sknt)%&gt;%\n  replace(is.na(.), 0) %&gt;%\n    mutate(interval60 = ymd_h(substr(valid,1,13))) %&gt;%\n    mutate(week = week(interval60),\n           dotw = wday(interval60, label=TRUE)) %&gt;%\n    group_by(interval60) %&gt;%\n    summarize(Temperature = max(tmpf),\n              Precipitation = sum(p01i),\n              Wind_Speed = max(sknt)) %&gt;%\n    mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))\n\nglimpse(weather.Panel)"
  },
  {
    "objectID": "posts/bikeshare/project.html#data-visualization",
    "href": "posts/bikeshare/project.html#data-visualization",
    "title": "Shared Bike Usage Prediciton",
    "section": "Data Visualization",
    "text": "Data Visualization\nFrom the plot of weather from 1/1/2023 to 2/4/2023, we can find that the degree of percipitation is relatively low and stable. When we focus on the wind speed and temperature, we can find that the wind speed have slightly increase when it comes to the end of January, while the temperature was gradually turning down.\n\n\nCode\ngrid.arrange(\n  ggplot(weather.Panel, aes(interval60,Precipitation)) + geom_line() + \n  labs(title=\"Percipitation\", x=\"Hour\", y=\"Perecipitation\") + plotTheme,\n  ggplot(weather.Panel, aes(interval60,Wind_Speed)) + geom_line() + \n    labs(title=\"Wind Speed\", x=\"Hour\", y=\"Wind Speed\") + plotTheme,\n  ggplot(weather.Panel, aes(interval60,Temperature)) + geom_line() + \n    labs(title=\"Temperature\", x=\"Hour\", y=\"Temperature\") + plotTheme,\n  top=\"Weather Data - Philadelphia ORD, 2023\")\n\n\n\n\n\nAlso, the usage of shared bike turns out an apparent difference in using frequency. we can find that in AM rush and PM rush period, the Mean Number of Hourly Trips is getting higher than other time of the day, which results from the potential more use for commute.\n\n\nCode\ndat_census %&gt;%\n        mutate(time_of_day = case_when(hour(interval60) &lt; 7 | hour(interval60) &gt; 18 ~ \"Overnight\",\n                                 hour(interval60) &gt;= 7 & hour(interval60) &lt; 10 ~ \"AM Rush\",\n                                 hour(interval60) &gt;= 10 & hour(interval60) &lt; 15 ~ \"Mid-Day\",\n                                 hour(interval60) &gt;= 15 & hour(interval60) &lt;= 18 ~ \"PM Rush\"))%&gt;%\n         group_by(interval60, start_station, time_of_day) %&gt;%\n         tally()%&gt;%\n  group_by(start_station, time_of_day)%&gt;%\n  summarize(mean_trips = mean(n))%&gt;%\n  ggplot()+\n  geom_histogram(aes(mean_trips), binwidth = 1)+\n  labs(title=\"Mean Number of Hourly Trips Per Station. Philadelphia, January, 2023\",\n       x=\"Number of trips\", \n       y=\"Frequency\")+\n  facet_wrap(~time_of_day)+\n  plotTheme\n\n\n\n\n\nCode\ndat_census$start_time &lt;- strptime(dat_census$start_time, format = \"%m/%d/%Y %H:%M\")\ndat_census$end_time &lt;- strptime(dat_census$end_time, format = \"%m/%d/%Y %H:%M\")\n\n\n\n\nCode\nweek2 &lt;-\n  filter(dat_census , week == 2 & dotw == \"Mon\")\n\nweek2.panel &lt;-\n  expand.grid(\n    interval15 = unique(week2$interval15),\n    Origin.Tract = unique(dat_census$start_Tract))\n\nbike.animation.data &lt;-\n  mutate(week2, Trip_Counter = 1) %&gt;%\n    right_join(week2.panel) %&gt;% \n    group_by(interval15, Origin.Tract) %&gt;%\n    summarize(Trip_Count = sum(Trip_Counter, na.rm=T)) %&gt;% \n    ungroup() %&gt;% \n    left_join(phillyTracts, by=c(\"Origin.Tract\" = \"GEOID\")) %&gt;%\n    st_sf() %&gt;%\n    mutate(Trips = case_when(Trip_Count == 0 ~ \"0 trips\",\n                             Trip_Count &gt; 0 & Trip_Count &lt;= 3 ~ \"1-3 trips\",\n                             Trip_Count &gt; 3 & Trip_Count &lt;= 6 ~ \"4-6 trips\",\n                             Trip_Count &gt; 6 & Trip_Count &lt;= 10 ~ \"7-10 trips\",\n                             Trip_Count &gt; 10 ~ \"11+ trips\")) %&gt;%\n    mutate(Trips  = fct_relevel(Trips, \"0 trips\",\"1-3 trips\",\"4-6 trips\",\n                                       \"7-10 trips\",\"10+ trips\"))\n\nbikeshare_animation &lt;-\n  ggplot() +\n    geom_sf(data = bike.animation.data, aes(fill = Trips)) +\n    scale_fill_manual(values = palette5) +\n    labs(title = \"Bikeshare For One Day in January 2023\",\n         subtitle = \"15 minute intervals: {current_frame}\") +\n    transition_manual(interval15)  + \n  mapTheme\n\n\nThe distribution of usage time of shared bike illustrate the short-distance feature of shared bike using in Philadelphia. From the Bike share trips, we can clearly see that the time of most trips is done within 15 minutes. The situation implies the potential situation that people could have better tolerance on the weather condition of using a bike.\n\n\nCode\nggplot(dat_census %&gt;%\n         group_by(interval15, start_station) %&gt;%\n         tally())+\n  geom_histogram(aes(n), binwidth = 1)+\n  labs(title=\"Bike share trips per hr by station. Philly January, 2023\",\n       x=\"Trip Counts\", \n       y=\"Number of Stations\")+\n  plotTheme\n\n\n\n\n\nThe time and count line chart in each day of week shows the different patterns for weekday and weekend. We can find that the Weekdays show the similar regularity that the use of shared bike will quick increase in the late morning and late afternoon due to potential commute. However, the use of bike in weekend increases gradually to the peak of the day in the noon and decreased later. The pattern indicates the potential distinguish in the model use in the weekend and weekday if considering re-distribution the bike in one day.\n\n\nCode\nggplot(dat_census %&gt;% mutate(hour = hour(start_time)))+\n     geom_freqpoly(aes(hour, color = dotw), binwidth = 1)+\n  labs(title=\"Bike share trips in Philly, by day of the week, 2023\",\n       x=\"Hour\", \n       y=\"Trip Counts\")+\n     plotTheme\n\n\n\n\n\nFrom the usage frequency in spatial aspect,in general, the weekday have more bike usage than that in weekend. Also, we can find that no matter time for a day and whether weekend, we can clearly see the more usage in the central city area. The situation reveals that the central city is an essential place for shared bike use as a key destination. What’s more, the university city also reveals the high frequency of shared bike usage, especially in the weekday, which may attribute for the student daily activities. What’s more, the north and south part near the central city reveals a relvatively stable using situation compared to other places.\n\n\nCode\nggplot()+\n  geom_sf(data = phillyTracts %&gt;%\n          st_transform(crs=4326))+\n  geom_point(data = dat_census %&gt;% \n            mutate(hour = hour(start_time),\n                weekend = ifelse(dotw %in% c(\"Sun\", \"Sat\"), \"Weekend\", \"Weekday\"),\n                time_of_day = case_when(hour(interval60) &lt; 7 | hour(interval60) &gt; 18 ~ \"Overnight\",\n                                 hour(interval60) &gt;= 7 & hour(interval60) &lt; 10 ~ \"AM Rush\",\n                                 hour(interval60) &gt;= 10 & hour(interval60) &lt; 15 ~ \"Mid-Day\",\n                                 hour(interval60) &gt;= 15 & hour(interval60) &lt;= 18 ~ \"PM Rush\"))%&gt;%\n              group_by(start_station, start_lat, start_lon, weekend, time_of_day) %&gt;%\n              tally(),\n            aes(x=start_lon, y = start_lat, color = n), \n            fill = \"transparent\", alpha = 0.4, size = 0.3)+\n  scale_colour_viridis(direction = -1,\n  discrete = FALSE, option = \"D\")+\n  ylim(min(dat_census$start_lat), max(dat_census$start_lat))+\n  xlim(min(dat_census$start_lon), max(dat_census$start_lon))+\n  facet_grid(weekend ~ time_of_day)+\n  labs(title=\"Bike share trips per hr by station. Philly, 2023\")+\n  mapTheme\n\n\n\n\n\nFrom the top use of different station and time, we can find that the PM rush time have major demand. And the station 3208,3296,3038 have top 3 demand for shared bike use. The situation indicates that in the further activity and re-balance move, the afternoon need more attention and effort to make a balance. What’s more, these stations also need further focus on the re-balancing.\n\n\nCode\nto_plot &lt;- dat_census\nto_plot &lt;- to_plot%&gt;% \n            mutate(hour = hour(start_time),\n                weekend = ifelse(dotw %in% c(\"Sun\", \"Sat\"), \"Weekend\", \"Weekday\"),\n                time_of_day = case_when(hour(interval60) &lt; 7 | hour(interval60) &gt; 18 ~ \"Overnight\",\n                                 hour(interval60) &gt;= 7 & hour(interval60) &lt; 10 ~ \"AM Rush\",\n                                 hour(interval60) &gt;= 10 & hour(interval60) &lt; 15 ~ \"Mid-Day\",\n                                 hour(interval60) &gt;= 15 & hour(interval60) &lt;= 18 ~ \"PM Rush\")) %&gt;%  \n  group_by(start_station,weekend, start_lat, start_lon, time_of_day) %&gt;%\n  tally() %&gt;% \n  arrange(-n) %&gt;% \n  head(30) \n\nto_plot$ID &lt;- seq_along(to_plot$start_station)\n\nto_plot %&gt;% \n  arrange(-n) %&gt;%\n  head(30) %&gt;% \n  ggplot(aes(x = reorder(ID, -n), n, fill = time_of_day, color = weekend)) +\n  scale_fill_manual(values = palette4, name=\"Time of Day\") + \n  guides(color=\"none\") + \n  geom_bar(stat = \"identity\", position=\"stack\") +\n  scale_color_manual(values = c(\"transparent\", \"black\")) +\n  labs(title=\"Top 30 Occurences of Rides By Time\")+\n  ylab(\"Number of Rides\") +\n  xlab(\"\")"
  },
  {
    "objectID": "posts/bikeshare/project.html#data-merge",
    "href": "posts/bikeshare/project.html#data-merge",
    "title": "Shared Bike Usage Prediciton",
    "section": "Data Merge",
    "text": "Data Merge\nIn the following steps, we created a study panel where each instance in the panel is a unique combination of space and time. Also, We need to add some more information to this panel. This includes counting the number of rides at this station at this particular hour, adding weather information, bringing in census data, and calculating time and day of week, calcluate the nearest distance from bus,trolley stop to the station.\n\n\nCode\nlength(unique(dat_census$interval60)) * length(unique(dat_census$start_station))\n\n\nstudy.panel &lt;- \n  expand.grid(interval60=unique(dat_census$interval60), \n              start_station = unique(dat_census$start_station)) %&gt;%\n  left_join(., dat_census %&gt;%\n              select(start_station, start_Tract, start_lon, start_lat)%&gt;%\n              distinct() %&gt;%\n              group_by(start_station) %&gt;%\n              slice(1))\n\nnrow(study.panel)      \n\n\n\n\nCode\nride.panel &lt;- \n  dat_census %&gt;%\n  mutate(Trip_Counter = 1) %&gt;%\n  right_join(study.panel) %&gt;% \n  group_by(interval60, start_station, start_Tract, start_lon, start_lat) %&gt;%\n  summarize(Trip_Count = sum(Trip_Counter, na.rm=T)) %&gt;%\n  left_join(weather.Panel) %&gt;%\n  ungroup() %&gt;%\n  filter(is.na(start_station) == FALSE) %&gt;%\n  mutate(week = week(interval60),\n         dotw = wday(interval60, label = TRUE)) %&gt;%\n  filter(is.na(start_Tract) == FALSE)\n\n\n\n\nCode\nride.panel &lt;- \n  left_join(ride.panel, phillyCensus %&gt;%\n              as.data.frame() %&gt;%\n              select(-geometry), by = c(\"start_Tract\" = \"GEOID\"))\n\n\n\n\nCode\nride.panel &lt;- ride.panel %&gt;%\n  mutate('s_lat' = start_lat,\n         's_lon' = start_lon) %&gt;%\n  st_as_sf(coords = c(\"start_lat\", \"start_lon\"), crs = 'EPSG:4326') %&gt;%\n  st_transform('ESRI:102728')\n\nride.panel &lt;- ride.panel%&gt;%\n  mutate(\n      bus_nn1 = nn_function(st_coordinates(ride.panel), \n                              st_coordinates(stop_bus), k = 1),\n      tro_nn1 = nn_function(st_coordinates(ride.panel), \n                              st_coordinates(stop_tro), k = 1), \n      hs_nn1 = nn_function(st_coordinates(ride.panel), \n                              st_coordinates(stop_hs), k = 1)) \n\n\nTo make better suggestion for the predicting model, we created time lag features for better predictions. In the context of predicting the number of trips (like rides or journeys) in a given time frame, it’s often observed that the number of trips in a specific hour is closely related to the number of trips in adjacent hours. This is because factors influencing the number of trips, such as commuter patterns, daily routines, or even weather conditions, tend to have continuity over short time periods.\n\n\nCode\nride.panel &lt;- \n  ride.panel %&gt;% \n  arrange(start_station, interval60) %&gt;% \n  mutate(lagHour = dplyr::lag(Trip_Count,1),\n         lag2Hours = dplyr::lag(Trip_Count,2),\n         lag3Hours = dplyr::lag(Trip_Count,3),\n         lag4Hours = dplyr::lag(Trip_Count,4),\n         lag12Hours = dplyr::lag(Trip_Count,12),\n         lag1day = dplyr::lag(Trip_Count,24),\n         holiday = ifelse(yday(interval60) == 148,1,0)) %&gt;%\n   mutate(day = yday(interval60)) %&gt;%\n   mutate(holidayLag = case_when(dplyr::lag(holiday, 1) == 1 ~ \"PlusOneDay\",\n                                 dplyr::lag(holiday, 2) == 1 ~ \"PlustTwoDays\",\n                                 dplyr::lag(holiday, 3) == 1 ~ \"PlustThreeDays\",\n                                 dplyr::lead(holiday, 1) == 1 ~ \"MinusOneDay\",\n                                 dplyr::lead(holiday, 2) == 1 ~ \"MinusTwoDays\",\n                                 dplyr::lead(holiday, 3) == 1 ~ \"MinusThreeDays\"),\n         holidayLag = ifelse(is.na(holidayLag) == TRUE, 0, holidayLag))\n\n\nFrom the correlation of lag time to the trip count, we can find that the 1 hour, 2 hours and 1 day lag show a relatively high correlation. Based on the discovery, we can have better dependent variable selection in the further data modeling.\n\n\nCode\nas.data.frame(ride.panel) %&gt;%\n    group_by(interval60) %&gt;% \n    summarise_at(vars(starts_with(\"lag\"), \"Trip_Count\"), mean, na.rm = TRUE) %&gt;%\n    gather(Variable, Value, -interval60, -Trip_Count) %&gt;%\n    mutate(Variable = factor(Variable, levels=c(\"lagHour\",\"lag2Hours\",\"lag3Hours\",\"lag4Hours\",\n                                                \"lag12Hours\",\"lag1day\")))%&gt;%\n    group_by(Variable) %&gt;%  \n    summarize(correlation = round(cor(Value, Trip_Count),2))\n\n\n# A tibble: 6 × 2\n  Variable   correlation\n  &lt;fct&gt;            &lt;dbl&gt;\n1 lagHour           0.85\n2 lag2Hours         0.64\n3 lag3Hours         0.42\n4 lag4Hours         0.23\n5 lag12Hours       -0.48\n6 lag1day           0.76"
  },
  {
    "objectID": "posts/bikeshare/project.html#cross-validation",
    "href": "posts/bikeshare/project.html#cross-validation",
    "title": "Shared Bike Usage Prediciton",
    "section": "Cross Validation",
    "text": "Cross Validation\nIn conclusion, we executed a series of 100 cross-validation trials applying Model 4 to all five weeks of data. The resultant Mean Absolute Error (MAE) of 0.43 underscores that, while this model represents our most effective approach to date, there remains a notable margin of error in its predictions. This suggests an opportunity for further refinement. Additionally, implementing cross-validation against various socio-economic indicators could provide valuable insights. Particularly, it may reveal whether the demand at bike stations within specific neighborhoods is systematically underestimated or overestimated by the current model\n\n\nCode\nfitControl &lt;- trainControl(method = \"cv\", number = 100)\n\nreg.cv &lt;- train(Trip_Count ~  start_station +  hour(interval60) + dotw + Temperature + Wind_Speed +\n                   lagHour + lag2Hours+ lag1day, data=ride.panel, method = \"lm\", trControl = fitControl, na.action = na.pass)\n\nreg.cv$resample %&gt;% \n  summarise(MAE = mean(reg.cv$resample[,3]),\n            sd(reg.cv$resample[,3])\n) %&gt;%\n  kbl(col.name=c('Mean Absolute Error','Standard Deviation of MAE')) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\n\n\nMean Absolute Error\nStandard Deviation of MAE\n\n\n\n\n0.4309091\n0.0134426"
  },
  {
    "objectID": "posts/house/housing_risk.html",
    "href": "posts/house/housing_risk.html",
    "title": "Housing Subsidy",
    "section": "",
    "text": "The Department of Housing and Community Development (HCD) in Emil City is launching a home repair tax credit program, and they want to reach out to those who are most likely to take this credit at a low cost. When reaching out randomly, only 11% of the homeowners take this credit and the unsuccessful reach out also wastes a large amount of money.\nIn order to make the most use of the housing subsidy and create a satisfying benefit, I will build a logistic regression model to predict under given features whether a homeowner will take the credit or not.\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\noptions(scipen=10000000)\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(caret)\nlibrary(knitr) \nlibrary(pscl)\nlibrary(plotROC)\nlibrary(pROC)\nlibrary(lubridate)\nlibrary(ggcorrplot) \nlibrary(vcd)\nlibrary(grid)\n\n\n\n\nCode\npalette5 &lt;- c(\"#264653\",\"#2a9d8f\",\"#e9c46a\",'#f4a261',\"#e76f51\")\npalette4 &lt;- c(\"#264653\",\"#2a9d8f\",\"#e9c46a\",\"#e76f51\")\npalette2 &lt;- c(\"#264653\",\"#2a9d8f\")\n\nchurn &lt;- read.csv('/Users/mr.smile/Desktop/UPENN/FALL23/MUSA508/musa_5080_2023-main/Housing Subsidy/churnBounce.csv')\n\nhouse_sub &lt;- read.csv('/Users/mr.smile/Desktop/UPENN/FALL23/MUSA508/musa_5080_2023-main/Housing Subsidy/housingSubsidy.csv')"
  },
  {
    "objectID": "posts/house/housing_risk.html#feature-engineering",
    "href": "posts/house/housing_risk.html#feature-engineering",
    "title": "Housing Subsidy",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nFrom the initial data visualization, we could hardly figure out the significantly difference between two different groups. Therefore, it’s essential to engineering the feature to improve further model performance. As for the education, jobs and pdays, which both have many categories, I re-categorize these feature into fewer categories. What’s more, I standardize the cpi and cci indicator to improve the interpretablity.\n\n\nCode\nhouse_sub$cons.price.idx_s &lt;- scale(house_sub$cons.price.idx)\nhouse_sub$cons.conf.idx_s &lt;- scale(house_sub$cons.conf.idx)\nhouse_sub$poutcome_new &lt;- ifelse(house_sub$poutcome == \"success\", 1, 0)\nhouse_sub &lt;- \n  house_sub %&gt;% \n  mutate(education,\n         education_new = case_when(education == \"basic.4y\" ~ \"medium\",\n                               education == \"basic.6y\" ~ \"medium\",\n                               education == \"basic.9y\" ~ \"high\",\n                               education == \"high.school\" ~ \"high\", \n                               education == \"professional.course\" ~ \"high\",\n                               education == \"university.degree\" ~ \"very high\",\n                               education == \"illiterate\"  ~ \"low\", \n                               education == \"unknown\" ~ \"low\"))%&gt;%\n  mutate(job,\n         job_new = case_when(job == \"retired\" ~ \"low income\",\n                         job == \"unemployed\" ~ \"low income\", \n                         job ==\"unknown\" ~ \"low income\", \n                         job == \"student\" ~ \"low income\",\n                         job == \"housemaid\" ~ \"medium income\",\n                         job == \"blue-collar\" ~ \"medium income\",\n                         job == \"services\" ~ \"medium income\",\n                         job == \"technician\" ~ \"high income\",\n                         job ==  \"management\" ~ \"high income\",\n                         job == \"admin.\" ~ \"high income\",\n                         job == \"entrepreneur\" ~ \"very high income\", \n                         job == \"self-employed\" ~ \"medium income\")) %&gt;%\n  mutate(pdays,\n         pdays_new = case_when(pdays == \"0\"  ~ \"0-6\",\n                           pdays == \"1\" ~ \"0-6\", \n                           pdays == \"2\" ~ \"0-6\", \n                           pdays == \"3\" ~ \"0-6\", \n                           pdays == \"4\" ~ \"0-6\", \n                           pdays == \"5\" ~ \"0-6\", \n                           pdays == \"6\" ~ \"0-6\", \n                           pdays == \"7\" ~ \"7-15\",\n                           pdays == \"9\"  ~ \"7-15\",\n                           pdays == \"10\"  ~ \"7-15\", \n                           pdays == \"11\"  ~ \"7-15\", \n                           pdays == \"12\"  ~ \"7-15\", \n                           pdays == \"13\"  ~ \"7-15\", \n                           pdays == \"14\"  ~ \"7-15\", \n                           pdays == \"15\"  ~ \"7-15\", \n                           pdays == \"17\" ~ \"16-21\", \n                           pdays == \"18\" ~ \"16-21\",\n                           pdays == \"19\" ~ \"16-21\", \n                           pdays == \"21\"~ \"16-21\", \n                           pdays == \"16\" ~ \"16-21\",\n                           pdays == \"999\" ~ \"unknown\"))"
  },
  {
    "objectID": "posts/house/housing_risk.html#training-and-test-split",
    "href": "posts/house/housing_risk.html#training-and-test-split",
    "title": "Housing Subsidy",
    "section": "Training and Test split",
    "text": "Training and Test split\nAfter feature engineering, I split the dataset into training and test one with 65/35 for further data modeling. To compare the performance of new feature, I test the performance of both model using the raw features in the dataset and model with variables after feature engineering and selection.\n\n\nCode\ntrainIndex &lt;- createDataPartition(house_sub$y, p = .65,\n                                  list = FALSE,\n                                  times = 1)\nhouseTrain &lt;- house_sub[ trainIndex,]\nhouseTest  &lt;- house_sub[-trainIndex,]\n\n\n\n\nCode\nset.seed(3426)\ntempModel &lt;- glm(y_numeric ~ .,\n                  data=houseTrain %&gt;% \n                    dplyr::select(-X,- y,-cons.price.idx_s,-cons.conf.idx_s,\n                                  -poutcome_new,-education_new,-job_new,-pdays_new),\n                  family=\"binomial\" (link=\"logit\"))\n\ntestProbs_raw &lt;- data.frame(Outcome = as.factor(houseTest$y_numeric),\n                        Probs = predict(tempModel, houseTest, type= \"response\"))\nsummary(tempModel)\n\n\n\nCall:\nglm(formula = y_numeric ~ ., family = binomial(link = \"logit\"), \n    data = houseTrain %&gt;% dplyr::select(-X, -y, -cons.price.idx_s, \n        -cons.conf.idx_s, -poutcome_new, -education_new, -job_new, \n        -pdays_new))\n\nCoefficients:\n                                 Estimate   Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  -193.7565741  127.7984723  -1.516 0.129491    \nage                             0.0142384    0.0086150   1.653 0.098383 .  \njobblue-collar                 -0.3467856    0.2861796  -1.212 0.225598    \njobentrepreneur                -0.5024294    0.4766389  -1.054 0.291833    \njobhousemaid                    0.0665795    0.4920445   0.135 0.892365    \njobmanagement                  -0.7155597    0.3195953  -2.239 0.025159 *  \njobretired                     -0.2185553    0.3899311  -0.560 0.575140    \njobself-employed               -0.5748698    0.4292805  -1.339 0.180523    \njobservices                    -0.1561030    0.3018183  -0.517 0.605011    \njobstudent                     -0.2972152    0.4586449  -0.648 0.516966    \njobtechnician                  -0.0035192    0.2416566  -0.015 0.988381    \njobunemployed                   0.0113993    0.4777049   0.024 0.980962    \njobunknown                     -0.5130227    0.9233155  -0.556 0.578463    \nmaritalmarried                  0.2925676    0.2692011   1.087 0.277125    \nmaritalsingle                   0.3443418    0.3029167   1.137 0.255641    \nmaritalunknown                -13.5484129  471.5800915  -0.029 0.977080    \neducationbasic.6y               0.6243513    0.4421978   1.412 0.157971    \neducationbasic.9y               0.5959060    0.3789407   1.573 0.115821    \neducationhigh.school            0.3131519    0.3624309   0.864 0.387570    \neducationilliterate           -13.9561322 1455.3976605  -0.010 0.992349    \neducationprofessional.course    0.7154565    0.3860064   1.853 0.063813 .  \neducationuniversity.degree      0.5123543    0.3606339   1.421 0.155403    \neducationunknown                0.4708085    0.4488698   1.049 0.294235    \ntaxLienunknown                 -0.2659035    0.2390765  -1.112 0.266048    \ntaxLienyes                    -12.1771728 1455.3976575  -0.008 0.993324    \nmortgageunknown                -0.0855127    0.5266982  -0.162 0.871025    \nmortgageyes                    -0.1756021    0.1491228  -1.178 0.238969    \ntaxbill_in_phlyes              -0.1300561    0.1944329  -0.669 0.503559    \ncontacttelephone               -1.2536614    0.2968190  -4.224 0.000024 ***\nmonthaug                       -0.2163491    0.4481657  -0.483 0.629278    \nmonthdec                        0.0859451    0.7057282   0.122 0.903072    \nmonthjul                       -0.3073090    0.3860889  -0.796 0.426059    \nmonthjun                        0.1079818    0.4605025   0.234 0.814607    \nmonthmar                        1.9766138    0.5845507   3.381 0.000721 ***\nmonthmay                       -0.3445120    0.3152756  -1.093 0.274511    \nmonthnov                       -0.4261588    0.4417362  -0.965 0.334677    \nmonthoct                        0.0663095    0.5653704   0.117 0.906634    \nmonthsep                       -0.0829597    0.6405100  -0.130 0.896945    \nday_of_weekmon                 -0.2995344    0.2303156  -1.301 0.193416    \nday_of_weekthu                 -0.1912149    0.2302379  -0.831 0.406250    \nday_of_weektue                 -0.0281146    0.2313850  -0.122 0.903290    \nday_of_weekwed                  0.1470685    0.2313566   0.636 0.524986    \ncampaign                       -0.0416736    0.0399321  -1.044 0.296665    \npdays                          -0.0005937    0.0008399  -0.707 0.479624    \nprevious                        0.1455766    0.2025662   0.719 0.472349    \npoutcomenonexistent             0.8170531    0.3436879   2.377 0.017439 *  \npoutcomesuccess                 0.9577807    0.8320618   1.151 0.249694    \nunemploy_rate                  -0.9715137    0.4823099  -2.014 0.043979 *  \ncons.price.idx                  1.8120114    0.8407990   2.155 0.031153 *  \ncons.conf.idx                   0.0716044    0.0291579   2.456 0.014059 *  \ninflation_rate                 -0.1175052    0.4595617  -0.256 0.798190    \nspent_on_repairs                0.0048006    0.0104447   0.460 0.645785    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1853.7  on 2678  degrees of freedom\nResidual deviance: 1391.3  on 2627  degrees of freedom\nAIC: 1495.3\n\nNumber of Fisher Scoring iterations: 14\n\n\n\n\nCode\nset.seed(3426)\nnewModel &lt;- glm(y_numeric ~ .,\n                  data=houseTrain %&gt;% \n                    dplyr::select(-X,-y,-poutcome,-education,-job,-pdays,\n                                  -age,-day_of_week,-mortgage,\n                                -taxbill_in_phl,-taxLien,-cons.price.idx,-cons.conf.idx\n                                  ),\n                  family=\"binomial\" (link=\"logit\"))\n\ntestProbs &lt;- data.frame(Outcome = as.factor(houseTest$y_numeric),\n                        Probs = predict(newModel, houseTest, type= \"response\"))\nsummary(newModel)\n\n\n\nCall:\nglm(formula = y_numeric ~ ., family = binomial(link = \"logit\"), \n    data = houseTrain %&gt;% dplyr::select(-X, -y, -poutcome, -education, \n        -job, -pdays, -age, -day_of_week, -mortgage, -taxbill_in_phl, \n        -taxLien, -cons.price.idx, -cons.conf.idx))\n\nCoefficients:\n                            Estimate   Std. Error z value  Pr(&gt;|z|)    \n(Intercept)             -10.67857738  51.47345814  -0.207  0.835652    \nmaritalmarried            0.20317012   0.26074138   0.779  0.435861    \nmaritalsingle             0.16384054   0.27576121   0.594  0.552419    \nmaritalunknown          -13.36499701 489.46686959  -0.027  0.978216    \ncontacttelephone         -1.18402612   0.29322289  -4.038 0.0000539 ***\nmonthaug                 -0.11214386   0.43900564  -0.255  0.798376    \nmonthdec                 -0.01079686   0.68405837  -0.016  0.987407    \nmonthjul                 -0.16884392   0.38091645  -0.443  0.657580    \nmonthjun                  0.23586597   0.44898467   0.525  0.599352    \nmonthmar                  1.98919097   0.57015394   3.489  0.000485 ***\nmonthmay                 -0.29875119   0.31041524  -0.962  0.335836    \nmonthnov                 -0.41254124   0.43469535  -0.949  0.342603    \nmonthoct                  0.08919060   0.56004384   0.159  0.873467    \nmonthsep                 -0.10168068   0.63457639  -0.160  0.872697    \ncampaign                 -0.04376002   0.03947379  -1.109  0.267610    \nprevious                 -0.23618124   0.12772635  -1.849  0.064441 .  \nunemploy_rate            -0.94197901   0.47550590  -1.981  0.047591 *  \ninflation_rate            0.00838913   0.44677763   0.019  0.985019    \nspent_on_repairs          0.00187749   0.01018300   0.184  0.853720    \ncons.price.idx_s          0.99845258   0.47561524   2.099  0.035792 *  \ncons.conf.idx_s           0.29797649   0.13027638   2.287  0.022180 *  \npoutcome_new              0.66448756   0.84248458   0.789  0.430274    \neducation_newlow          0.00004037   0.33224923   0.000  0.999903    \neducation_newmedium      -0.19618783   0.24309957  -0.807  0.419651    \neducation_newvery high   -0.04539249   0.17050390  -0.266  0.790066    \njob_newlow income        -0.16724034   0.24201361  -0.691  0.489542    \njob_newmedium income     -0.24734626   0.18146896  -1.363  0.172875    \njob_newvery high income  -0.40101631   0.46048223  -0.871  0.383830    \npdays_new16-21           -2.23883739   1.29870242  -1.724  0.084725 .  \npdays_new7-15            -0.15475105   0.62871179  -0.246  0.805574    \npdays_newunknown         -0.86195394   0.89420360  -0.964  0.335079    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1853.7  on 2678  degrees of freedom\nResidual deviance: 1416.4  on 2648  degrees of freedom\nAIC: 1478.4\n\nNumber of Fisher Scoring iterations: 14"
  },
  {
    "objectID": "posts/house/housing_risk.html#model-evaluation",
    "href": "posts/house/housing_risk.html#model-evaluation",
    "title": "Housing Subsidy",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nDistribution of Predicted Probabilities\nWhen we look at the density plot of the predicted probabilities by observed outcome by different models, we can find that the distribution of these two model has little difference, while the model with new feature have more concentration and higher density in the prediction for not entering the program.\n\n\nCode\nggplot(testProbs, aes(x = Probs, fill = as.factor(Outcome))) + \n  geom_density() +\n  facet_grid(Outcome ~ .) +\n  scale_fill_manual(values = palette2) +\n  labs(x = \"Enter\", y = \"Density of probabilities\",\n       title = \"Distribution of predicted probabilities by observed outcome(new)\") +\n  theme(strip.text.x = element_text(size = 18),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nCode\nggplot(testProbs_raw, aes(x = Probs, fill = as.factor(Outcome))) + \n  geom_density() +\n  facet_grid(Outcome ~ .) +\n  scale_fill_manual(values = palette2) +\n  labs(x = \"Enter\", y = \"Density of probabilities\",\n       title = \"Distribution of predicted probabilities by observed outcome(raw)\") +\n  theme(strip.text.x = element_text(size = 18),\n        legend.position = \"none\")\n\n\n\n\n\n\n\nThreshold setting and analysis\nBased on the density, I set the probability’s threshold of 0.14 for determine whether the objective will enter the program. From the aspect of sensitivity, which represent the ability to correctly identify positive instances, the new model has better performance than the model with raw feature.\n\n\nCode\ntestProbs &lt;- \n  testProbs %&gt;%\n  mutate(predOutcome  = as.factor(ifelse(testProbs$Probs &gt; 0.5 , 1, 0)))\n\ncaret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, \n                       positive = \"1\")\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1256  123\n         1   27   34\n                                              \n               Accuracy : 0.8958              \n                 95% CI : (0.8789, 0.9111)    \n    No Information Rate : 0.891               \n    P-Value [Acc &gt; NIR] : 0.294               \n                                              \n                  Kappa : 0.2672              \n                                              \n Mcnemar's Test P-Value : 0.000000000000008716\n                                              \n            Sensitivity : 0.21656             \n            Specificity : 0.97896             \n         Pos Pred Value : 0.55738             \n         Neg Pred Value : 0.91080             \n             Prevalence : 0.10903             \n         Detection Rate : 0.02361             \n   Detection Prevalence : 0.04236             \n      Balanced Accuracy : 0.59776             \n                                              \n       'Positive' Class : 1                   \n                                              \n\n\n\n\nCode\ntestProbs_raw &lt;- \n  testProbs_raw %&gt;%\n  mutate(predOutcome  = as.factor(ifelse(testProbs_raw$Probs &gt; 0.5 , 1, 0)))\n\ncaret::confusionMatrix(testProbs_raw$predOutcome, testProbs_raw$Outcome, \n                       positive = \"1\")\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1239  113\n         1   44   44\n                                          \n               Accuracy : 0.891           \n                 95% CI : (0.8737, 0.9066)\n    No Information Rate : 0.891           \n    P-Value [Acc &gt; NIR] : 0.5212          \n                                          \n                  Kappa : 0.3047          \n                                          \n Mcnemar's Test P-Value : 0.00000005731   \n                                          \n            Sensitivity : 0.28025         \n            Specificity : 0.96571         \n         Pos Pred Value : 0.50000         \n         Neg Pred Value : 0.91642         \n             Prevalence : 0.10903         \n         Detection Rate : 0.03056         \n   Detection Prevalence : 0.06111         \n      Balanced Accuracy : 0.62298         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n\nROC Curve & AUC score\nTo see the performance of two models more directly, I check the ROC(Receiver Operating Characteristic) curve, which visualize the performance of binary classification model. The curve that is “above” the y=x line shows the good performance of model. What’s more, I use the AUC (area under curve) score to measure the behavior, with higher score represent better ability to distinguish between the features. And the model with new feature having higher AUC score indicates the better interpretability for whether entering the program.\n\n\nCode\nggplot(testProbs, aes(d = as.numeric(Outcome), m = Probs)) +\n  geom_roc(n.cuts = 50, labels = FALSE, colour = \"#FE9900\") +\n  style_roc(theme = theme_grey) +\n  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +\n  labs(title = \"ROC Curve - clickModel\")\n\n\n\n\n\n\n\nCode\nauc(testProbs$Outcome, testProbs$Probs)\n\n\nArea under the curve: 0.7687\n\n\nCode\nauc(testProbs_raw$Outcome, testProbs_raw$Probs)\n\n\nArea under the curve: 0.7517"
  },
  {
    "objectID": "posts/house/housing_risk.html#cross-validation-fitness",
    "href": "posts/house/housing_risk.html#cross-validation-fitness",
    "title": "Housing Subsidy",
    "section": "Cross Validation & Fitness",
    "text": "Cross Validation & Fitness\nTo assess the performance and generalization ability of two models and identify the issue of over-fitting due to randomness of train and test data splitting, I use cross validation to compare and select the final model. Comparing the results of two models, we can find that the the new model in general has better performance due to higher ROC score and sensitivity.\n\n\nCode\nctrl &lt;- trainControl(method = \"cv\", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)\n\ncvFit &lt;- train(y ~ .,\n                  data=house_sub %&gt;% \n                    dplyr::select(-X,-y_numeric,-poutcome,-education,-job,-pdays,\n                                  -age,-day_of_week,-mortgage,\n                                -taxbill_in_phl,-taxLien,-cons.price.idx,-cons.conf.idx), \n                method=\"glm\", family=\"binomial\",\n                metric=\"ROC\", trControl = ctrl)\n\ncvFit\n\n\nGeneralized Linear Model \n\n4119 samples\n  14 predictor\n   2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (100 fold) \nSummary of sample sizes: 4078, 4078, 4077, 4077, 4079, 4079, ... \nResampling results:\n\n  ROC        Sens       Spec \n  0.7729003  0.9825676  0.209\n\n\n\n\nCode\ncvFit_raw &lt;- train(y ~ .,\n                  data=house_sub %&gt;% \n                    dplyr::select(-X,- y_numeric,-cons.price.idx_s,-cons.conf.idx_s,\n                                  -poutcome_new,-education_new,-job_new,-pdays_new), \n                method=\"glm\", family=\"binomial\",\n                metric=\"ROC\", trControl = ctrl)\n\ncvFit_raw\n\n\nGeneralized Linear Model \n\n4119 samples\n  19 predictor\n   2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (100 fold) \nSummary of sample sizes: 4078, 4078, 4077, 4078, 4077, 4078, ... \nResampling results:\n\n  ROC       Sens       Spec  \n  0.762658  0.9811562  0.2185\n\n\nTo check the goodness of fitting metrics more clearly, I visualize the result of 100 times cross validation to see the generalization and stability of the model when confronting different data. From the histogram of ROC score, we can find that the new model has distribution more like normal distribution compared the the raw model, and the situation shows that the new model is relatively more stable across different subsets of the data. What’s more, when we focus on the distribution of sensitivity, the new model has more density on the higher sensitivity score, which shows that the new model has better ability to correctly identify positive instances.\n\n\nCode\ndplyr::select(cvFit$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(cvFit$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 1)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics(new)\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")\n\n\n\n\n\n\n\nCode\ndplyr::select(cvFit_raw$resample, -Resample) %&gt;%\n  gather(metric, value) %&gt;%\n  left_join(gather(cvFit$results[2:4], metric, mean)) %&gt;%\n  ggplot(aes(value)) + \n    geom_histogram(bins=35, fill = \"#2a9d8f\") +\n    facet_wrap(~metric) +\n    geom_vline(aes(xintercept = mean), colour = \"#e76f51\", linetype = 3, size = 1.5) +\n    scale_x_continuous(limits = c(0, 1)) +\n    labs(x=\"Goodness of Fit\", y=\"Count\", title=\"CV Goodness of Fit Metrics(old)\",\n         subtitle = \"Across-fold mean reprented as dotted lines\")"
  },
  {
    "objectID": "posts/house/housing_risk.html#situation-definition",
    "href": "posts/house/housing_risk.html#situation-definition",
    "title": "Housing Subsidy",
    "section": "Situation Definition",
    "text": "Situation Definition\nTo use the model to help calculate the benefits under the prediction of model, we need to set the benefits of different combinations of predicted and actual situation first. Our approach will be to use the confusion matrix from testProbs. Below the cost/benefit for each outcome in our confusion matrix is calculated, like so:\nTrue Negative: $0 True Positive: (-$2850 - $5000 + $10000 + $56000)*0.25*Count - $2850*0.75Count False Negative: -$2850Count False Positive: $0"
  },
  {
    "objectID": "posts/house/housing_risk.html#cost-benefit-analysis",
    "href": "posts/house/housing_risk.html#cost-benefit-analysis",
    "title": "Housing Subsidy",
    "section": "Cost-Benefit Analysis",
    "text": "Cost-Benefit Analysis\nFrom the result of cost/benefit table, we can find that the main benefit comes from True-positive situation, which represent we correctly predict that the homeowner will enter the program. And the main cost comes from False-positive, where we would put into money for marketing source and the homeowners will not enter.\n\n\nCode\ncost_benefit_table &lt;-\n   testProbs %&gt;%\n      count(predOutcome, Outcome) %&gt;%\n      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),\n                True_Positive = sum(n[predOutcome==1 & Outcome==1]),\n                False_Negative = sum(n[predOutcome==0 & Outcome==1]),\n                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %&gt;%\n       gather(Variable, Count) %&gt;%\n       mutate(Revenue =\n               ifelse(Variable == \"True_Negative\",0,\n               ifelse(Variable == \"True_Positive\",((-2850-5000+10000+56000)*(Count*0.25)+(-2850*Count*0.75)),\n               ifelse(Variable == \"False_Negative\", 0,\n               ifelse(Variable == \"False_Positive\", -2850 * Count, 0))))) %&gt;%\n    bind_cols(data.frame(Description = c(\n              \"We correctly predicted no enter\",\n              \"We correctly predicted enter\",\n              \"We predicted no enter and homeowner want to enter\",\n              \"We predicted homeowner will enter and homeowner did not enter\")))\n\nkable(cost_benefit_table,\n       caption = \"Cost/Benefit Table\") %&gt;% kable_styling()\n\n\n\nCost/Benefit Table\n\n\nVariable\nCount\nRevenue\nDescription\n\n\n\n\nTrue_Negative\n1256\n0\nWe correctly predicted no enter\n\n\nTrue_Positive\n34\n421600\nWe correctly predicted enter\n\n\nFalse_Negative\n123\n0\nWe predicted no enter and homeowner want to enter\n\n\nFalse_Positive\n27\n-76950\nWe predicted homeowner will enter and homeowner did not enter\n\n\n\n\n\n\n\n\n\nCode\niterateThresholds &lt;- function(data) {\n  x = .01\n  all_prediction &lt;- data.frame()\n  while (x &lt;= 1) {\n  \n  this_prediction &lt;-\n      testProbs %&gt;%\n      mutate(predOutcome = ifelse(Probs &gt; x, 1, 0)) %&gt;%\n      count(predOutcome, Outcome) %&gt;%\n      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),\n                True_Positive = sum(n[predOutcome==1 & Outcome==1]),\n                False_Negative = sum(n[predOutcome==0 & Outcome==1]),\n                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %&gt;%\n     gather(Variable, Count) %&gt;%\n     mutate(Revenue =\n               ifelse(Variable == \"True_Negative\", 0,\n               ifelse(Variable == \"True_Positive\",((-2850-5000+10000+56000)*(Count*0.25)+(-2850*Count*0.75)),\n               ifelse(Variable == \"False_Negative\", 0,\n               ifelse(Variable == \"False_Positive\", -2850 * Count, 0)))),\n            Threshold = x)\n  \n  all_prediction &lt;- rbind(all_prediction, this_prediction)\n  x &lt;- x + .01\n  }\nreturn(all_prediction)\n}\n\n\nBased on the cost/benefit analysis, we want to increase the true-positive rate and decrease the False-Positive rate to increase the total benefit for the model.Therefore, we need to find the different threshold’s influence on this two indicators. We can find that with the increase of threshold, the True-positive and False Positive will both decrease.\n\n\nCode\nwhichThreshold &lt;- iterateThresholds(test_Probs1)\n\nwhichThreshold %&gt;%\n  ggplot(.,aes(Threshold, Count, colour = Variable)) +\n  geom_point() +\n  scale_colour_manual(values = palette4) +    \n  labs(title = \"Confusion Metric Outcome by Threshold\",\n       y = \"Count\") +\n  guides(colour=guide_legend(title = \"Legend\")) \n\n\n\n\n\nHowever, it’s hard to get the maximium of benefits easily. So we plot the line of benefits with the increasing threshold. We can find the benefit will reach the max when the threshold is set as 0.14 properly.\n\n\nCode\nwhichThreshold &lt;- iterateThresholds(testProbs2)\n\nwhichThreshold_revenue &lt;- \nwhichThreshold %&gt;% \n    group_by(Threshold) %&gt;% \n    summarize(Revenue = sum(Revenue))\n\n  ggplot(whichThreshold_revenue)+\n  geom_line(aes(x = Threshold, y = Revenue))+\n  geom_vline(xintercept =  pull(arrange(whichThreshold_revenue, -Revenue)[1,1]))+\n    labs(title = \"Model Revenues By Threshold For Test Sample\",\n         subtitle = \"Vertical Line Denotes Optimal Threshold\")\n\n\n\n\n\nHowever, if we use the counts of credits, which represents the number of homeowners can get the credit to enter the program, to measure the performance of different threshold, we can find that the count of credit will continously increase with the increase of threshold until the threshold comes to 0.8.\n\n\nCode\nwhichThreshold_credit &lt;- \nwhichThreshold %&gt;% \n  mutate(credit =  ifelse(Variable == \"True_Positive\", (Count * 0.25),\n                             ifelse(Variable == \"False_Negative\", Count, 0))) %&gt;%\n  group_by(Threshold) %&gt;% \n  summarize(Credit = sum(credit))\n\n  ggplot(whichThreshold_credit)+\n  geom_line(aes(x = Threshold, y = Credit, colour = \"#FE9900\"))+\n  geom_vline(xintercept =  pull(arrange(whichThreshold_credit, -Credit)[1,1]))+\n    labs(title = \"Total Count of Credits By Threshold For Test Sample\",\n         subtitle = \"Vertical Line Denotes Optimal Threshold\")+\n  theme(legend.position = \"None\")\n\n\n\n\n\nif we compare the revenue and credit number of the optimal benefits’ threshold and 0.5 threshold, we can find that despite the benefit will reach max with 0.22 threshold for the model, the model with serve less credit compared to 0.5 threshold.\n\n\nCode\nthreshold_table &lt;- merge(whichThreshold_revenue, whichThreshold_credit, by = \"Threshold\")\n\nfinal_table &lt;- threshold_table %&gt;%\n                  slice(22, 50) \n\nkable(final_table, caption = \"Total Revenue and Total Count of Credits for Optimal Threshold and 0.5 Threshold\") %&gt;% \n  kable_styling()\n\n\n\nTotal Revenue and Total Count of Credits for Optimal Threshold and 0.5 Threshold\n\n\nThreshold\nRevenue\nCredit\n\n\n\n\n0.22\n609800\n101.5\n\n\n0.50\n344650\n131.5"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html",
    "title": "House Price Prediction, Philadelphia",
    "section": "",
    "text": "In light of recent concerns about the accuracy of Zillow’s housing market predictions, our team has undertaken an effort to enhance house price prediction in Philadelphia. Our primary goal is to contribute to more advanced decision-making for the City of Philadelphia by leveraging machine learning techniques. To achieve improved accuracy and generalizability, we have taken a comprehensive approach. We’ve incorporated various indicators across different fields to ensure that our predictions align more closely with real-world conditions. By doing so, we aim to provide a more accurate representation of Philadelphia’s housing market, which can be valuable for both current decision-making and future planning. This report serves as a detailed account of our approach and methods. It emphasizes the significance of understanding the local context and the seamless integration of this knowledge with pre-existing data. Our overarching objective is to provide a richer context and a more precise understanding of Philadelphia’s housing landscape, fostering informed decision-making for the present and future.\n\n\nCode\ndata &lt;- st_read(\"https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Midterm/data/2023/studentData.geojson\")\n\nvariables &lt;- c('objectid',\n               'census_tract',\n               'parcel_shape',\n               'building_code',\n               'quality_grade',\n               'central_air',\n               'depth',\n               'exempt_land',\n               'exterior_condition',\n               'fireplaces',\n               'frontage',\n               'fuel',\n               'garage_spaces',\n               'garage_type',\n               'general_construction',\n               'interior_condition',\n               'number_of_bathrooms',\n               'number_of_bedrooms',\n               'number_of_rooms',\n               'number_stories',\n               'off_street_open',\n               'sale_price',\n               'year_built',\n               'toPredict')\n\nf_df &lt;- data[, variables]"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#census-data",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#census-data",
    "title": "House Price Prediction, Philadelphia",
    "section": "Census Data",
    "text": "Census Data\nThe first source used was from the U.S Census Bureau - specifically the American Community Survey (ACS) dataset - we used data from the 2021 five year ACS survey, which is the most recent ACS five year survey data available.\n\n\nCode\ntracts21 &lt;-  \n  get_acs(geography = \"tract\",\n          variables = c(\"B01001_001E\", # ACS total Pop estimate\n              'B02001_002E', # Estimate!!Total:!!White alone\n              'B02001_003E', # Estimate!!Total:!!Black or African American alone\n              'B02001_005E', # Estimate!!Total:!!Asian alone\n              'B19013A_001E',# Median household income White alone\n              'B19013B_001E',# Median household income Black alone\n              'B19013D_001E',# Median household income Asian alone\n              'B19013_001E', # Median household income \n              'B01002_001E', # Median Age by Sex \n              \"B25002_001E\", # Estimate of total housing units\n              \"B25002_003E\", # Number of vacant housing units\n              \"B06009_006E\", # Total graduate or professional degree\n              \"B15001_050E\",\n              \"B15001_009E\",\n              \"B25058_001E\",\n              \"B06012_002E\",\n              'B25105_001E', # Median monthly housing costs \n              'B25037_001E', # Median year structure built --!!Total\n              'B10058_002E', # Total:!!In labor force\n              'B10058_007E', # Not in labor force\n              'B08141_017E', # Public transportation:!!No vehicle available\n              'B08141_018E', # Public transportation:!!1 vehicle available \n              'B09002_002E', # OWN CHILDREN UNDER 18 YEARS married family\n              'B09002_015E', # Female householder\n              'B14001_004E'), # Enrolled in kindergarten \n          year=2021, state=42,\n          county=101, geometry=TRUE, output = \"wide\") %&gt;% \n  st_transform('ESRI:102728') %&gt;%\n\n  rename(TotalPop = B01001_001E, \n         White = B02001_002E,\n         Black = B02001_003E,\n         Asian = B02001_005E,\n         MedHHInc_White = B19013A_001E,\n         MedHHInc_Black = B19013B_001E,\n         MedHHInc_Asian = B19013D_001E,\n         MedHHInc = B19013_001E,\n         Median_Age_bysex = B01002_001E,\n         Total_Housing_Units = B25002_001E,\n         Vacant_Housing_Units = B25002_003E,\n         Total_Graduate_Prof_Degree = B06009_006E,\n         FemaleBachelors = B15001_050E, \n         MaleBachelors = B15001_009E,\n         MedRent = B25058_001E,\n         TotalPoverty = B06012_002E,\n         Med_mon_hscst = B25105_001E,\n         Med_built_year = B25037_001E,\n         Lab_force = B10058_002E,\n         no_lav_force = B10058_007E,\n         no_pub_trans = B08141_017E,\n         one_pub_trans = B08141_018E,\n         own_child = B09002_002E,\n         own_child_fe = B09002_015E,\n         enroll_kinder = B14001_004E)\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n\nCode\ntracts21 &lt;- tracts21[, !grepl(\"M$\", names(tracts21))]\n\nmean_in_w &lt;- mean(tracts21$MedHHInc_White,na.rm = TRUE)\ntracts21$MedHHInc_White[is.na(tracts21$MedHHInc_White)] &lt;- mean_in_w\n\nmean_in_b &lt;- mean(tracts21$MedHHInc_Black,na.rm = TRUE)\ntracts21$MedHHInc_Black[is.na(tracts21$MedHHInc_Black)] &lt;- mean_in_b\n\nmean_in_a &lt;- mean(tracts21$MedHHInc_Asian,na.rm = TRUE)\ntracts21$MedHHInc_Asian[is.na(tracts21$MedHHInc_Asian)] &lt;- mean_in_a\n\nmean_in &lt;- mean(tracts21$MedHHInc,na.rm = TRUE)\ntracts21$MedHHInc[is.na(tracts21$MedHHInc)] &lt;- mean_in\n\nmed_age &lt;- mean(tracts21$Median_Age_bysex,na.rm = TRUE)\ntracts21$Median_Age_bysex[is.na(tracts21$Median_Age_bysex)] &lt;- med_age\n\nmed_rent &lt;- mean(tracts21$MedRent,na.rm = TRUE)\ntracts21$MedRent[is.na(tracts21$MedRent)] &lt;- med_rent\n\nmean_medmonhh &lt;- mean(tracts21$Med_mon_hscst,na.rm = TRUE)\ntracts21$Med_mon_hscst[is.na(tracts21$Med_mon_hscst)] &lt;- mean_medmonhh\n\nmed_b_yr &lt;- median(tracts21$Med_built_year,na.rm = TRUE)\ntracts21$Med_built_year[is.na(tracts21$Med_built_year)] &lt;- med_b_yr\n\n\n\n\nCode\ndf_pro &lt;- df_pro %&gt;%\n  st_transform('ESRI:102728')\ntracts21 &lt;- tracts21 %&gt;%\n  st_transform('ESRI:102728')\n\nhh_tract &lt;- st_join(df_pro,tracts21, join = st_within)\n\nhh_tract &lt;- hh_tract %&gt;%\n  st_transform('ESRI:102728')\n\nhh_tract &lt;- subset(hh_tract,select = -c(central_air,fuel,garage_type,Asian,MedHHInc_White,MedHHInc_Black,MedHHInc_Asian,FemaleBachelors,MaleBachelors,Med_built_year,one_pub_trans,no_lav_force))"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#get-data-from-open-data-philly",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#get-data-from-open-data-philly",
    "title": "House Price Prediction, Philadelphia",
    "section": "Get Data from Open Data Philly",
    "text": "Get Data from Open Data Philly\nOpenData Philly is an open source website that provides a catalog of free data, officially sponsored by the City of Philadelphia. The data provided by the city and other organizations allows us to collect a wide variety of information we can utilize to categorize, describe, and develop a profile for key geographic characteristics which might impact price. And we use the following dataset as our potential variable:\n\nCommercial Corridors – dataset record the location of commercial corridors in Philadelphia.\nSchool – dataset the record the location of school.\nCity Facilities – dataset record the different type of ameneties.\nNeighborhhod-Philadelphia – planning districts will be used as a proxy for neighborhoods.\nCrime – dataset records crime location in Philadelphia with detailed type.\n\n\n\nCode\nboundary &lt;- st_read(\"https://opendata.arcgis.com/datasets/405ec3da942d4e20869d4e1449a2be48_0.geojson\") %&gt;%\n  st_transform('ESRI:102728')\n\nCommercial_Corridors &lt;- st_read(\"https://opendata.arcgis.com/datasets/f43e5f92d34e41249e7a11f269792d11_0.geojson\") %&gt;%\n  st_transform('ESRI:102728')\n\ncrime &lt;- read.csv('https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%272021-01-01%27%20AND%20dispatch_date_time%20%3C%20%272022-01-01%27')\nCrime &lt;-\n  crime %&gt;%\n    filter(text_general_code == \"Aggravated Assault Firearm\",\n           lat &gt; -1) %&gt;%\n    dplyr::select(lat, lng) %&gt;%\n    na.omit() %&gt;%\n    st_as_sf(coords = c(\"lng\", \"lat\"), crs = \"EPSG:4326\") %&gt;%\n    st_transform('ESRI:102728') %&gt;%\n    distinct()\n\nSchool &lt;- st_read(\"https://opendata.arcgis.com/datasets/d46a7e59e2c246c891fbee778759717e_0.geojson\") %&gt;%\n  st_transform('ESRI:102728')\n\nCityFacilities &lt;- st_read(\"https://opendata.arcgis.com/datasets/b3c133c3b15d4c96bcd4d5cc09f19f4e_0.geojson\") %&gt;%\n  st_transform('ESRI:102728')\n\nnhoods &lt;- \n  st_read('https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson') %&gt;%\n  st_transform('ESRI:102728')%&gt;%\n  dplyr::select(name)\n\n\n\n\nCode\nhh_tract &lt;- st_join(hh_tract, nhoods, join = st_within)\n\n\n\nSchool: Private\nHomes located in proximity to reputable educational institutions, particularly private schools, often enjoy increased desirability and can fetch higher prices in the real estate market. As a result, we assess the presence of private schools within a 0.25-mile radius of each house. This distance allows for convenient commuting for students and parents while providing access to a variety of educational activities and resources.\n\n\nCode\nSchool &lt;- School %&gt;%\n  dplyr::select(OBJECTID,GRADE_LEVEL,TYPE_SPECIFIC)\n\nSchool_Private &lt;- School %&gt;%\n    filter(TYPE_SPECIFIC == \"PRIVATE\") %&gt;%\n    distinct() %&gt;%\n    dplyr::select(geometry)\n\n\n\n\nCode\nhh_tract$school_1320 &lt;- hh_tract %&gt;% \n    st_buffer(1320) %&gt;% \n    aggregate(mutate(Crime, counter = 1),., sum) %&gt;%\n    pull(counter)\nhh_tract$school_1320[is.na(hh_tract$school_1320)] &lt;- 0\n\n\n\n\nDistance to Commercial Corridors\nThe proximity to commercial corridors can be a significant factor in attracting more residents to move in. This proximity offers residents the convenience of having grocery stores, restaurants, shops, and other amenities nearby. Additionally, it enhances walkability and fosters more economic activity, contributing to a livelier neighborhood. Therefore, we calculate the distance between each house and the nearest commercial corridor. However, there might also be potential risks of noise, traffic, and crimes, which may influence people’s decision-making process.\n\n\nCode\nnearest_dists &lt;- st_distance(hh_tract, Commercial_Corridors)\nclosest_distances &lt;- apply(nearest_dists, 1, min)\nhh_tract &lt;- hh_tract %&gt;%\n  mutate(dis_cc = closest_distances) \n\n\n\n\nCrime: Aggravated Assault within 0.25/0.125 miles\nCrime is often a crucial neighborhood indicator. Homebuyers frequently prefer properties situated in neighborhoods with lower crime rates, as it ensures a safer and more desirable living environment. By calculating crime counts within different buffer distances of 0.25 miles and 0.125 miles from each house, we aim to account for the potential influence of varying crime ranges on home values. Then, we utilize a k-nearest neighbor approach to provide an additional perspective on the relationship between crime and home values, which calculates the distance between each house and the nearest crime incidents, considering multiple nearest neighbors (k) ranging from 1 to 5. This helps us understand how different levels of crime proximity might affect home values.\n\n\nCode\nhh_tract$crimes.Buffer_660 &lt;- hh_tract %&gt;% \n    st_buffer(660) %&gt;% \n    aggregate(mutate(Crime, counter = 1),., sum) %&gt;%\n    pull(counter)\n\nhh_tract$crimes.Buffer_1320 &lt;- hh_tract %&gt;% \n    st_buffer(1320) %&gt;% \n    aggregate(mutate(Crime, counter = 1),., sum) %&gt;%\n    pull(counter)\n\nhh_tract$crimes.Buffer_660[is.na(hh_tract$crimes.Buffer_660)] &lt;- 0 \nhh_tract$crimes.Buffer_1320[is.na(hh_tract$crimes.Buffer_1320)] &lt;- 0\n\n\n\n\nCode\n## Nearest Neighbor Feature\nhh_tract &lt;-\n  hh_tract %&gt;% \n    mutate(\n      crime_nn1 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(Crime), k = 1),\n      \n      crime_nn2 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(Crime), k = 2), \n      \n      crime_nn3 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(Crime), k = 3), \n      \n      crime_nn4 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(Crime), k = 4), \n      \n      crime_nn5 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(Crime), k = 5)) \n\n\n\n\nCity Facilities: Park and recreation, Library, Museum and zoo\nCity facilities such as parks and recreation areas, libraries, museums, and zoos are essential amenities that can significantly influence the attractiveness and livability of a neighborhood. Homebuyers often value proximity to these facilities, as they contribute to a higher quality of life and a sense of community. So we further calculate the proximity of each house to these selected facilities with a buffer of 0.25 miles, which is a reasonable walking distance for residents to access these amenities. Then, we use knn approach again to assess how proximity to these city facilities may affect home values.\n\n\nCode\nCityFacilities &lt;- CityFacilities %&gt;%\ndplyr::filter(ASSET_GROUP1 == \"A11\" | ASSET_GROUP1 == \"A18\" | ASSET_GROUP1 == \"A8\") %&gt;%\n  select(geometry)%&gt;%\n  distinct()\n\nhh_tract$facilities.Buffer &lt;- hh_tract %&gt;% \n    st_buffer(1320) %&gt;% \n    aggregate(mutate(CityFacilities, counter = 1),., sum) %&gt;%\n    pull(counter)\n\nhh_tract$facilities.Buffer[is.na(hh_tract$facilities.Buffer)] &lt;- 0\n\n\n\n\nCode\n## Nearest Neighbor Feature\nhh_tract &lt;-\n  hh_tract %&gt;% \n    mutate(\n      facilities_nn1 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(CityFacilities), k = 1),\n      \n      facilities_nn2 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(CityFacilities), k = 2), \n      \n      facilities_nn3 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(CityFacilities), k = 3), \n      \n      facilities_nn4 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(CityFacilities), k = 4), \n      \n      facilities_nn5 = nn_function(st_coordinates(hh_tract), \n                              st_coordinates(CityFacilities), k = 5)) \n\n\n##Home Price Scatterplots The four graphs below represent four key dependent variables that broadly cover each aspect of a neighborhood in order to determine house prices. Interior condition represent directly the quality of house, relating highly to the house cost. As forthe number of crime in 0.25 miles surrounding house, representing the degree of security. And the number of ameneties like park represents the accessibility to the public service. Last but not least, the number of female who owned child in the tract where house locates indicates the potential demand for school in the area.\n\n\nCode\n## Home Features cor\nst_drop_geometry(hh_tract) %&gt;% \n  dplyr::select(sale_price, own_child_fe, crimes.Buffer_660, facilities.Buffer,interior_condition) %&gt;%\n  filter(sale_price &lt;= 1000000) %&gt;%\n  gather(Variable, Value, -sale_price) %&gt;% \n   ggplot(aes(Value, sale_price)) +\n     geom_point(size = .5) + geom_smooth(method = \"lm\", se=F, colour = \"#FA7800\") +\n     facet_wrap(~Variable, ncol = 2, scales = \"free\") +\n     labs(title = \"4 varialbes' correlation to sale price\") +\n     plotTheme()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n##Correlation Test from Correlation Matrix A correlation matrix compares factors against one another to see how related they are at determining the others value. Each box presented can indicate whether there is a correlation (directly or inverse) or little relationship between them. We are able to see strong correlation between many factors. From the correlation matrix, we can find that the number of school, the number of crime in 400 meter range, the number of female owned children in the tract, the total poverty situation where the house located have a positive correlation to the house price; Median household income, median rent, median monthly house cost have a negative correlation to the house price. These high correlation performance will function as an important basis for the variable selection in the model for predicting house price.\n\n\nCode\nhh_tract_pre &lt;- hh_tract%&gt;%\n  dplyr::select(-c(objectid,census_tract,GEOID,Total_Housing_Units))\n\nnumericVars &lt;- \n  select_if(st_drop_geometry(hh_tract_pre), is.numeric) %&gt;% na.omit()\n\nggcorrplot(\n  round(cor(numericVars), 1), \n  p.mat = cor_pmat(numericVars),\n  colors = c(\"#25CB10\", \"white\", \"#FA7800\"),\n  type=\"lower\",\n  insig = \"blank\") +  \n    labs(title = \"Correlation across numeric variables\")"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#statistical-table-for-variable-selected",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#statistical-table-for-variable-selected",
    "title": "House Price Prediction, Philadelphia",
    "section": "Statistical table for variable selected",
    "text": "Statistical table for variable selected\nThe model used for predicting house price contains variable including varibles in the table below. In detail, we use internal characteristics (interior_condition,number of bathrooms, number of bedrooms), public services (number of school in 0.5 miles, number of amenities in 0.5 miles, distance to commercial corridor) and social economic spatial structure (number of crime in 0.25 miles, poverty situation, public transit accessibility, number of female own children).\n\n\nCode\nhh_model &lt;- subset(hh_tract,select = c('sale_price','interior_condition','number_of_bathrooms','number_of_bedrooms','school_1320','crimes.Buffer_660','own_child_fe','TotalPoverty','facilities.Buffer','dis_cc','no_pub_trans'))\n\nsumtable(hh_model,\n         summ=c('min(x)',\n                'mean(x)',\n                'median(x)',\n                'sd(x)',\n                'pctile(x)[25]', \n                'pctile(x)[75]',\n                'max(x)'))\n\n\n\nSummary Statistics\n\n\nVariable\nMin\nMean\nMedian\nSd\nPctile[25]\nPctile[75]\nMax\n\n\n\n\nsale_price\n0\n267494\n225000\n234256\n135000\n325000\n4000000\n\n\ninterior_condition\n0\n3.7\n4\n0.83\n4\n4\n7\n\n\nnumber_of_bathrooms\n0\n1.1\n1\n0.66\n1\n1\n5\n\n\nnumber_of_bedrooms\n0\n2.6\n3\n1.2\n2\n3\n10\n\n\nschool_1320\n0\n7.5\n5\n7.5\n2\n11\n49\n\n\ncrimes.Buffer_660\n0\n2\n1\n2.4\n0\n3\n20\n\n\nown_child_fe\n0\n406\n305\n363\n118\n604\n1876\n\n\nTotalPoverty\n0\n1046\n853\n787\n435\n1475\n4127\n\n\nfacilities.Buffer\n0\n4.7\n4\n5\n1\n7\n41\n\n\ndis_cc\n0\n672\n548\n639\n217\n957\n7386\n\n\nno_pub_trans\n0\n170\n123\n154\n58\n258\n761"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#map-of-home-prices",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#map-of-home-prices",
    "title": "House Price Prediction, Philadelphia",
    "section": "Map of Home Prices",
    "text": "Map of Home Prices\nThis map explores how home prices change in Philadelphia in 2021. We can see from the map that home prices are higher in the center city and outermost suburbs than in other areas.\n\n\nCode\nhh_tract &lt;- hh_tract %&gt;%\n  mutate(sale_class = cut(sale_price, breaks = c(0,20000, 100000, 200000, 250000,400000, max(hh_tract$sale_price, na.rm = TRUE))))\n\nggplot() +\n  geom_sf(data = nhoods, fill = \"grey80\") +\n  geom_sf(data = hh_tract, aes(colour = q5(sale_class)), \n          show.legend = \"point\", size = .75) +\n  scale_colour_manual(values = palette5,\n                   labels=c('0-$20k','$20k-$100k', '$100k-$200k', '$200k-$250k', '$250k-$400k'),\n                   name=\"Home Prices\\n($)\") +\n  labs(title=\"Sale Price, Philadelphia\") +\n  mapTheme()"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#interesting-maps",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#interesting-maps",
    "title": "House Price Prediction, Philadelphia",
    "section": "Interesting Maps",
    "text": "Interesting Maps\n\nHigh Education Level Population Distribution\nThe distribution of individuals with higher education closely mirrors the pattern of home prices. This demographic tends to reside predominantly in the city center or in the suburban areas on the outskirts of the city.\n\n\nCode\nggplot()+\n  geom_sf(data = nhoods, fill = \"grey80\") +\n  geom_sf(data=hh_tract,aes(colour = q5(Total_Graduate_Prof_Degree)), \n          show.legend = \"point\", size = .75) +\n  scale_colour_manual(values = palette5,\n                   labels=qBr(hh_tract,\"Total_Graduate_Prof_Degree\"),\n                   name=\"Population\") +\n  labs(title=\"Distribution of population with a high level of education, Philadelphia\") +\nmapTheme() \n\n\n\n\n\n\n\nFacilities Density\nRegarding facilities, a majority of them are concentrated in university city and the western part of the center city. This clustering is clearly visible and gradually extends outward toward the city’s periphery.\n\n\nCode\nggplot() +\n  geom_sf(data = nhoods, fill = \"grey80\") +\n  stat_density2d(data = data.frame(st_coordinates(CityFacilities)), \n                 aes(X, Y, fill = ..level.., alpha = ..level..),\n                 size = 0.01, bins = 40, geom = 'polygon') +\n  scale_fill_gradient(low = \"#25CB10\", high = \"#FA7800\", name = \"Density\") +\n  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +\n  labs(title = \"Density of City Facilities, Philadelphia\") +\n  mapTheme()\n\n\n\n\n\n\n\nPrivate School Density\nPrivate schools are concentrated in the center of the city and in the northwestern part of the city. The south-east corner and the south are relatively sparsely distributed.\n\n\nCode\nggplot() +\n  geom_sf(data = nhoods, fill = \"grey80\") +\n  stat_density2d(data = data.frame(st_coordinates(School_Private)), \n                 aes(X, Y, fill = ..level.., alpha = ..level..),\n                 size = 0.01, bins = 40, geom = 'polygon') +\n  scale_fill_gradient(low = \"#25CB10\", high = \"#FA7800\", name = \"Density\") +\n  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +\n  labs(title = \"Density of Private Schools, Philadelphia\") +\n  mapTheme()\n\n\n\n\n\n\n\nAssault Density\nRegarding the concentration of aggravated assaults, there are two distinct areas of concern. One is located to the west of University City, and the other is situated to the north of Center City. In these areas, both property sale prices and educational attainment levels among residents tend to be relatively low.\n\n\nCode\nggplot() +\n  geom_sf(data = nhoods, fill = \"grey80\") +\n  stat_density2d(data = data.frame(st_coordinates(Crime)), \n                 aes(X, Y, fill = ..level.., alpha = ..level..),\n                 size = 0.01, bins = 40, geom = 'polygon') +\n  scale_fill_gradient(low = \"#25CB10\", high = \"#FA7800\", name = \"Density\") +\n  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +\n  labs(title = \"Density of Aggravated Assaults, Philadelphia\") +\n  mapTheme()"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#table-of-results-training",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#table-of-results-training",
    "title": "House Price Prediction, Philadelphia",
    "section": "Table of Results (Training)",
    "text": "Table of Results (Training)\nWe have decided on a filtered list of key dependent variables to use to predict home values. Those factors being the following: interior condition, number of bathrooms, number of bedrooms, number of school in 0.5 miles, number of crime in 0.25 miles, number of female who own child in the tract, total poverty number, number of ameneties like parks in 0.5 miles, distance to commercial corridor, and the number of no public transportation access in the tract.\n\n\nCode\ninTrain &lt;- createDataPartition(\n              y = paste(hh_tract$name), \n              p = .60, list = FALSE)\np.training &lt;- hh_tract[inTrain,] \np.test &lt;- hh_tract[-inTrain,]  \n\nindpdt_var &lt;- c('sale_price','interior_condition','number_of_bathrooms','number_of_bedrooms','school_1320','crimes.Buffer_660','own_child_fe','TotalPoverty','facilities.Buffer','dis_cc','no_pub_trans')\n\nreg.training &lt;- \n  lm(sale_price ~ ., data = as.data.frame(p.training) %&gt;% \n                        dplyr::select(indpdt_var))\n\nsummary(reg.training)\n\n\n\nCall:\nlm(formula = sale_price ~ ., data = as.data.frame(p.training) %&gt;% \n    dplyr::select(indpdt_var))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-526125  -92091  -25235   50473 3610064 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         520752.789  10129.071  51.412  &lt; 2e-16 ***\ninterior_condition  -49177.410   2205.268 -22.300  &lt; 2e-16 ***\nnumber_of_bathrooms 108131.439   3519.572  30.723  &lt; 2e-16 ***\nnumber_of_bedrooms  -26261.869   1791.348 -14.660  &lt; 2e-16 ***\nschool_1320          -5962.815    464.716 -12.831  &lt; 2e-16 ***\ncrimes.Buffer_660    -6468.748   1374.538  -4.706 2.55e-06 ***\nown_child_fe           -92.871      8.326 -11.154  &lt; 2e-16 ***\nTotalPoverty           -32.183      3.851  -8.357  &lt; 2e-16 ***\nfacilities.Buffer     2118.017    341.230   6.207 5.55e-10 ***\ndis_cc                 -12.538      2.674  -4.688 2.78e-06 ***\nno_pub_trans            53.351     12.558   4.248 2.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 193200 on 13990 degrees of freedom\nMultiple R-squared:  0.3476,    Adjusted R-squared:  0.3471 \nF-statistic: 745.4 on 10 and 13990 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#table-of-goodness-of-fit",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#table-of-goodness-of-fit",
    "title": "House Price Prediction, Philadelphia",
    "section": "Table of Goodness of Fit",
    "text": "Table of Goodness of Fit\nn this phase of the process, we are currently training the model to predict housing prices. We have supplied the model with data points, each accompanied by its respective set of attributes linked to the sale price. The model learns the relationship between these attributes and sale prices, subsequently generating its own set of predicted values when presented with new data. The table presented below demonstrates the disparities between the model’s predictions and the actual values, quantified as their absolute errors. Given that we are dealing with substantial-value assets, such as housing, the differences between the model’s predictions and the actual values can be substantial. Additionally, it is equally crucial to assess these discrepancies in terms of percentage values to gauge the relative magnitude of the disparities.\n\n\nCode\np.test &lt;-\n  p.test %&gt;%\n  mutate(Regression = \"Baseline Regression\",\n         sale_price.Predict = predict(reg.training, p.test),\n         sale_price.Error = sale_price.Predict - sale_price,\n         sale_price.AbsError = abs(sale_price.Predict - sale_price),\n         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict)%&gt;%\n  filter(sale_price &lt; 5000000) \n\np.test %&gt;%\n   st_drop_geometry() %&gt;%\n  summarise(MAE = mean(sale_price.AbsError),\n            MAPE = abs(mean(sale_price.APE)*100)) %&gt;%\n  kbl(col.name=c('Mean Absolute Error','Mean Absolute Percentage Error')) %&gt;%\n  kable_classic()\n\n\n\n\n\nMean Absolute Error\nMean Absolute Percentage Error\n\n\n\n\n105953.8\n41.75261"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#cross-validation",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#cross-validation",
    "title": "House Price Prediction, Philadelphia",
    "section": "cross validation",
    "text": "cross validation\nUp to this point, our analysis has been primarily centered on a single training and test dataset. To evaluate the model’s capacity to generalize to new data, it’s essential to employ a repeated modeling approach involving multiple runs with varying training datasets. This methodology is known as K-Fold Cross Validation, and it entails partitioning the data into 100 distinct training datasets. Each training dataset omits a different set of home sales data points, which are subsequently utilized to assess the model’s error. The following scatter plot illustrates a histogram representing the Mean Absolute Error (MAE) across all 100 analyses. The MAE ranges from $80,000 to $130,000. The distribution of these MAEs closely approximates a normal distribution, with a prominent peak around $100,000. Despite a degree of inaccuracy, this observation suggests that the model exhibits generalizability and consistently yields results when confronted with various training data.\n\n\nCode\nfitControl &lt;- trainControl(method = \"cv\", number = 100)\nset.seed(825)\n\nreg.cv &lt;- \n  train(sale_price ~ ., data = st_drop_geometry(hh_tract) %&gt;% \n          dplyr::select(indpdt_var), \n        method = \"lm\", trControl = fitControl, na.action = na.pass)\n\nggplot(reg.cv$resample, aes(x=MAE)) + \n  geom_histogram(color='white',fill=\"orange\",bins=100)+\n  scale_x_continuous(limits=c(0,150000),breaks=c(25000,50000,75000,100000,125000,150000))+\n  scale_y_continuous(breaks=c(0,2,4,6,8,10))"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#plot-predicted-prices",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#plot-predicted-prices",
    "title": "House Price Prediction, Philadelphia",
    "section": "Plot Predicted Prices",
    "text": "Plot Predicted Prices\nThe graph presented below illustrates the disparity between our predictive model and the concept of “perfect predictions,” where 100% accuracy is attained. Across the majority of clusters, our model aligns closely with the line of perfection, indicating its overall accuracy. However, when dealing with a smaller number of high-value homes, especially those exceeding $2 million, the model exhibits a tendency to undervalue their worth. Another noteworthy limitation of the model is the presence of negative results. Multiple data points fall below the (0,0) point, indicating that the model predicts a negative value for certain homes, despite their actual sales prices being positive integers. Strikingly, the model appears to slightly overestimate the values of these homes, as evidenced by the model prediction line (in green) surpassing the line of perfection. This observation could serve as an indicator of properties that require significant investment in maintenance or are in such poor condition that the cost of repairs surpasses the purchase price, categorizing them as potential “fixer-upper” candidates.\n\n\nCode\np.test %&gt;%\n  dplyr::select(sale_price.Predict, sale_price) %&gt;%\n    ggplot(aes(sale_price, sale_price.Predict)) +\n  geom_point() +\n  stat_smooth(aes(sale_price, sale_price), \n             method = \"lm\", se = FALSE, size = 1, colour=\"#FA7800\") + \n  stat_smooth(aes(sale_price, sale_price.Predict), \n              method = \"lm\", se = FALSE, size = 1, colour=\"#25CB10\") +\n  labs(title=\"Predicted sale price as a function of observed price\",\n       subtitle=\"Orange line represents a perfect prediction; Green line represents prediction\") +\n  plotTheme()"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#map-of-residual-errors",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#map-of-residual-errors",
    "title": "House Price Prediction, Philadelphia",
    "section": "Map of Residual Errors",
    "text": "Map of Residual Errors\nThe map presented below provides a visual representation of the absolute margin of error associated with each sales price prediction. The majority of errors fall within the range of plus or minus $50,000. However, it’s important to note that there are distinct areas with more substantial errors, particularly in the northwest, where higher-value homes are located. This underscores the model’s consistent tendency to underestimate the values of these high-end properties.\n\n\nCode\np.test &lt;- p.test %&gt;% \n  mutate(spErrorClass = cut(sale_price.Error, breaks = c(min(p.test$sale_price.Error, na.rm=TRUE)-1, -100000, -50000, 50000, 100000, max(p.test$sale_price.Error, na.rm=TRUE))))\n\nggplot()+\n    geom_sf(data=nhoods,fill='grey80',color='transparent')+\n    geom_sf(data=p.test, size=0.5,aes(colour = spErrorClass))+\n    geom_sf(data=nhoods,fill='transparent',color='black')+\n    scale_color_manual(values = palette5,\n                    name = \"Sale Price Error Margins\",\n                    na.value = 'grey80',\n                    labels = c('Less than -100,000', '-100,000 to -50,000', '-50,000 to 50,000', '50,001 - 100,000','More than 100,000'))+\n  mapTheme()"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#spatial-lag-analysis",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#spatial-lag-analysis",
    "title": "House Price Prediction, Philadelphia",
    "section": "Spatial Lag Analysis",
    "text": "Spatial Lag Analysis\nAnalyzing the residuals map displayed above, we can observe a pattern of spatial clustering in the residuals. To further investigate this, we assess whether there’s a similarity between our predicted values and the predicted values of nearby homes. To do this, we employ a spatial lag calculation, which involves computing the spatial lag as the weighted sum of values from neighboring locations. The scatter plot below juxtaposes the sales price error for each home in our model against the spatial lag error for sales price, which is calculated based on the average error of the five nearest neighbors. The points in the scatter plot exhibit clustering, signifying that our model generally predicts similar errors for a home and its five nearest neighboring homes. The blue line on the plot represents the regression line, indicating this spatial similarity in error predictions.\n\n\nCode\ncoords.test &lt;-  st_coordinates(p.test) \n\nneighborList.test &lt;- knn2nb(knearneigh(coords.test, 5))\n\nspatialWeights.test &lt;- nb2listw(neighborList.test, style=\"W\")\n \np.test %&gt;% \n  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %&gt;%\n  ggplot()+\n  geom_point(aes(x =lagPriceError, y =sale_price.Error)) +\n  stat_smooth(aes(lagPriceError, sale_price.Error), \n             method = \"lm\", se = FALSE, size = 1, colour=\"blue\")+\n  xlim(-1000000,1000000)+\n  ylim(-1000000,1000000)+\n  plotTheme()"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#moran-i-test",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#moran-i-test",
    "title": "House Price Prediction, Philadelphia",
    "section": "Moran I test",
    "text": "Moran I test\nMoran’s I is a measurement in statistics that evaluates the tendency for data points with similar values to occur near one another. Similar to Nearest Neighbor, to statistically prove that houses spatially close to each other would have similar sale prices. This type of measurement helps us identify high and low value clusters (i.e different neighborhoods and their perceived value). In the graph below, you can observe a relatively narrow distribution in gray, which represents the counts of houses and their sale prices used in our regression model. The orange line corresponds to the Moran’s I value, which serves to assess whether there is a correlation between property value and location. The observed Moran’s I stands at approximately 0.4, a statistically significant value indicating a correlated spatial distribution of property values.\n\n\nCode\nmoranTest &lt;- moran.mc(p.test$sale_price.Error, \n                      spatialWeights.test, nsim = 999)\n\nggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +\n  geom_histogram(binwidth = 0.01) +\n  geom_vline(aes(xintercept = moranTest$statistic), colour = \"#FA7800\",size=1) +\n  scale_x_continuous(limits = c(-1, 1)) +\n  labs(title=\"Observed and permuted Moran's I\",\n       subtitle= \"Observed Moran's I in orange\",\n       x=\"Moran's I\",\n       y=\"Count\") +\n  plotTheme()"
  },
  {
    "objectID": "posts/houseprice/mid_initial_1010-copy-copy.html#predictions-by-neighborhood",
    "href": "posts/houseprice/mid_initial_1010-copy-copy.html#predictions-by-neighborhood",
    "title": "House Price Prediction, Philadelphia",
    "section": "Predictions by neighborhood",
    "text": "Predictions by neighborhood\n\n\nCode\np.test %&gt;%\nas.data.frame() %&gt;%\n  group_by(name) %&gt;%\n    summarize(meanPrediction = mean(sale_price.Predict),\n              meanPrice = mean(sale_price)) %&gt;%\n      kable() %&gt;% \n  kable_styling()\n\n\n\n\n\nname\nmeanPrediction\nmeanPrice\n\n\n\n\nACADEMY_GARDENS\n313183.0979\n295419.05\n\n\nALLEGHENY_WEST\n143262.5034\n85687.48\n\n\nANDORRA\n404836.8861\n402793.75\n\n\nASTON_WOODBRIDGE\n238714.9852\n287096.43\n\n\nBARTRAM_VILLAGE\n161179.3921\n95744.44\n\n\nBELLA_VISTA\n424206.4508\n555164.58\n\n\nBELMONT\n70926.0714\n138405.88\n\n\nBREWERYTOWN\n221028.3352\n228589.87\n\n\nBRIDESBURG\n312132.0606\n200516.32\n\n\nBURHOLME\n255146.4259\n260000.00\n\n\nBUSTLETON\n332599.6579\n378614.59\n\n\nBYBERRY\n545885.7394\n361250.00\n\n\nCALLOWHILL\n325057.0376\n287500.00\n\n\nCARROLL_PARK\n86130.0904\n119115.04\n\n\nCEDARBROOK\n380714.4444\n264435.45\n\n\nCEDAR_PARK\n296317.5155\n461459.71\n\n\nCHESTNUT_HILL\n425388.0109\n969235.33\n\n\nCLEARVIEW\n286770.5611\n157350.55\n\n\nCOBBS_CREEK\n183882.6411\n172067.12\n\n\nCRESCENTVILLE\n134613.7655\n204950.00\n\n\nDEARNLEY_PARK\n359261.3186\n347276.92\n\n\nDICKINSON_NARROWS\n375879.7875\n357342.44\n\n\nDUNLAP\n96908.1346\n150812.50\n\n\nEASTWICK\n299900.8238\n234138.38\n\n\nEAST_FALLS\n378336.5519\n415235.30\n\n\nEAST_KENSINGTON\n334676.6554\n356702.21\n\n\nEAST_OAK_LANE\n335013.8912\n263622.78\n\n\nEAST_PARKSIDE\n164238.4397\n101331.25\n\n\nEAST_PASSYUNK\n359417.6703\n369944.58\n\n\nEAST_POPLAR\n343215.7361\n167666.67\n\n\nELMWOOD\n179091.8888\n122608.90\n\n\nFAIRHILL\n127209.7276\n50076.19\n\n\nFAIRMOUNT\n423069.9158\n439904.62\n\n\nFELTONVILLE\n161175.8784\n120185.00\n\n\nFERN_ROCK\n249766.4411\n172133.90\n\n\nFISHTOWN\n389084.2584\n384590.11\n\n\nFITLER_SQUARE\n465778.4266\n939325.00\n\n\nFOX_CHASE\n303742.2368\n315408.45\n\n\nFRANCISVILLE\n464114.1133\n493279.24\n\n\nFRANKFORD\n152700.8349\n127409.52\n\n\nFRANKLINVILLE\n112733.8112\n74028.30\n\n\nGARDEN_COURT\n387127.4591\n461555.56\n\n\nGERMANTOWN_EAST\n208372.2991\n145838.43\n\n\nGERMANTOWN_MORTON\n194672.2474\n163134.39\n\n\nGERMANTOWN_PENN_KNOX\n295326.2061\n322985.71\n\n\nGERMANTOWN_SOUTHWEST\n210599.1570\n186343.46\n\n\nGERMANTOWN_WESTSIDE\n290993.2816\n171366.67\n\n\nGERMANTOWN_WEST_CENT\n328969.3698\n388359.21\n\n\nGERMANY_HILL\n471398.6431\n405656.67\n\n\nGIRARD_ESTATES\n329667.4836\n305383.72\n\n\nGLENWOOD\n138715.1587\n75405.88\n\n\nGRADUATE_HOSPITAL\n458035.6063\n583837.83\n\n\nGRAYS_FERRY\n205775.1777\n180690.16\n\n\nGREENWICH\n319151.8049\n261468.00\n\n\nHADDINGTON\n112776.5640\n115785.54\n\n\nHARROWGATE\n23448.4356\n87017.84\n\n\nHARTRANFT\n165910.5410\n105663.55\n\n\nHAVERFORD_NORTH\n229810.6242\n175756.56\n\n\nHAWTHORNE\n460867.9722\n592847.83\n\n\nHOLMESBURG\n179673.9978\n201102.94\n\n\nHUNTING_PARK\n93308.9048\n88302.09\n\n\nJUNIATA_PARK\n151308.2529\n154565.38\n\n\nKINGSESSING\n190626.0585\n136016.08\n\n\nLAWNDALE\n220614.3177\n191576.50\n\n\nLEXINGTON_PARK\n308038.9044\n307275.00\n\n\nLOGAN\n188461.6980\n142351.80\n\n\nLOGAN_SQUARE\n458097.3752\n526992.71\n\n\nLOWER_MOYAMENSING\n308267.2746\n230304.63\n\n\nLUDLOW\n483012.1207\n337166.67\n\n\nMANAYUNK\n390439.6649\n341272.40\n\n\nMANTUA\n152824.5282\n211866.67\n\n\nMAYFAIR\n227137.0179\n232220.42\n\n\nMCGUIRE\n362.7725\n65714.29\n\n\nMELROSE_PARK_GARDENS\n299697.5821\n177327.78\n\n\nMILLBROOK\n324326.7602\n282708.57\n\n\nMILL_CREEK\n130311.2099\n112055.30\n\n\nMODENA\n316047.7297\n295304.35\n\n\nMORRELL_PARK\n324461.4553\n294357.41\n\n\nMOUNT_AIRY_EAST\n315985.8335\n300325.22\n\n\nMOUNT_AIRY_WEST\n379370.4600\n492501.52\n\n\nNEWBOLD\n340099.1481\n303131.70\n\n\nNICETOWN\n242050.5672\n100662.50\n\n\nNORMANDY_VILLAGE\n272431.1201\n313958.33\n\n\nNORTHERN_LIBERTIES\n442951.4148\n635777.62\n\n\nNORTHWOOD\n207580.7881\n209995.89\n\n\nNORTH_CENTRAL\n182149.7097\n236589.42\n\n\nOGONTZ\n207642.8897\n146244.57\n\n\nOLD_KENSINGTON\n435858.7504\n447569.44\n\n\nOLNEY\n168499.0692\n144991.45\n\n\nOVERBROOK\n270669.8751\n206751.63\n\n\nOXFORD_CIRCLE\n233806.2729\n216263.19\n\n\nPACKER_PARK\n460367.1451\n546490.27\n\n\nPARKWOOD_MANOR\n318789.1952\n282820.25\n\n\nPASCHALL\n142326.2596\n100835.42\n\n\nPASSYUNK_SQUARE\n408431.2784\n428459.33\n\n\nPENNSPORT\n407954.5112\n370430.30\n\n\nPENNYPACK\n311989.9811\n306320.60\n\n\nPENNYPACK_WOODS\n312316.6692\n261250.00\n\n\nPENROSE\n261804.8861\n174590.91\n\n\nPOINT_BREEZE\n379799.7166\n326131.41\n\n\nPOWELTON\n257423.8048\n380000.00\n\n\nQUEEN_VILLAGE\n390044.9539\n569240.86\n\n\nRHAWNHURST\n280825.4615\n290088.18\n\n\nRICHMOND\n229070.2879\n182177.13\n\n\nRITTENHOUSE\n426305.8777\n797764.54\n\n\nRIVERFRONT\n374635.6916\n441249.75\n\n\nROXBOROUGH\n373662.5460\n320127.44\n\n\nROXBOROUGH_PARK\n411597.8106\n527944.44\n\n\nSHARSWOOD\n263195.6767\n134937.50\n\n\nSOCIETY_HILL\n417227.4487\n730063.49\n\n\nSOMERTON\n315262.2539\n342732.68\n\n\nSOUTHWEST_SCHUYLKILL\n183011.0433\n135575.48\n\n\nSPRING_GARDEN\n445306.9801\n443212.10\n\n\nSPRUCE_HILL\n378143.5800\n610750.00\n\n\nSTADIUM_DISTRICT\n333811.5238\n287570.59\n\n\nSTANTON\n58650.7560\n87913.81\n\n\nSTRAWBERRY_MANSION\n122640.1230\n86970.56\n\n\nSUMMERDALE\n185666.4741\n164812.56\n\n\nTACONY\n232639.7552\n183643.01\n\n\nTIOGA\n162364.3769\n96660.98\n\n\nTORRESDALE\n315534.0055\n290113.20\n\n\nUPPER_KENSINGTON\n11318.5677\n64203.95\n\n\nUPPER_ROXBOROUGH\n356465.5292\n364390.14\n\n\nWALNUT_HILL\n246846.7255\n286243.75\n\n\nWASHINGTON_SQUARE\n374948.3612\n688845.96\n\n\nWEST_KENSINGTON\n280537.5148\n208440.23\n\n\nWEST_OAK_LANE\n242813.1915\n198077.28\n\n\nWEST_PARKSIDE\n250118.7184\n163500.00\n\n\nWEST_PASSYUNK\n241586.9338\n198819.36\n\n\nWEST_POPLAR\n321317.5687\n407925.00\n\n\nWEST_POWELTON\n246067.3835\n253083.33\n\n\nWHITMAN\n332864.5096\n235917.81\n\n\nWINCHESTER_PARK\n343127.1764\n317564.29\n\n\nWISSAHICKON\n389331.6720\n360597.92\n\n\nWISSAHICKON_HILLS\n334588.7911\n368833.33\n\n\nWISSINOMING\n201347.5909\n166505.62\n\n\nWISTER\n175089.0203\n132357.69\n\n\nWYNNEFIELD\n242641.1852\n217276.00\n\n\nWYNNEFIELD_HEIGHTS\n314667.7244\n160300.00\n\n\nYORKTOWN\n254379.3574\n272875.00"
  }
]